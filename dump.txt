### FILES=191 date=2025-08-29_07:52:48 root=/opt/scalp
--- [1/191] ./pytest.ini ---
[pytest]
addopts = -q
--- [2/191] ./[REDACTED].py ---
import os
if os.getenv("SCALP_SKIP_BOOT","1") == "1":
    # default = skip; scripts will manage deps explicitly
    raise SystemExit
# sinon: si tu tiens à un mini bootstrap, fais-le ici mais léger--- [3/191] ./run-dash.sh ---
#!/bin/bash
cd "$(dirname "$0")"
echo "[*] Lancement du dashboard Streamlit..."
# tente python -m pip install -r si jamais le bootstrap n'a pas tourné
python -m pip install -r requirements.txt >/dev/null 2>&1 || true
streamlit run dash/app.py --server.port 8501 --server.headless true--- [4/191] ./dump.txt ---
### FILES=191 date=2025-08-29_07:52:48 root=/opt/scalp
--- [1/191] ./pytest.ini ---
[pytest]
addopts = -q
--- [2/191] ./[REDACTED].py ---
import os
if os.getenv("SCALP_SKIP_BOOT","1") == "1":
    # default = skip; scripts will manage deps explicitly
    raise SystemExit
# sinon: si tu tiens à un mini bootstrap, fais-le ici mais léger--- [3/191] ./run-dash.sh ---
#!/bin/bash
cd "$(dirname "$0")"
echo "[*] Lancement du dashboard Streamlit..."
# tente python -m pip install -r si jamais le bootstrap n'a pas tourné
python -m pip install -r requirements.txt >/dev/null 2>&1 || true
streamlit run dash/app.py --server.port 8501 --server.headless true--- [4/191] ./dump.txt ---
--- [5/191] ./entries_config.json ---
{
  "schema_version": "scalp-entries/1.0",
  "entry_sets": {
    "pullback_trend": {
      "context": {"min_buy_prob": 0.60, "min_adx": 0},
      "signals": {"rsi_cross": {"length": 7, "level": 50, "direction": "up"}},
      "risk": {"sl_atr_mult": 1.2, "tp_atr_mult": 1.8}
    },
    "breakout": {
      "context": {"min_buy_prob": 0.55, "min_adx": 20},
      "signals": {"close_above_high_n": 20},
      "risk": {"tp_atr_mult": 1.5}
    }
  },
  "common": {
    "vwap": {"session": "daily"},
    "bbands": {"length": 20, "stddev": 2.0},
    "timeout_bars": 25
  }
}--- [6/191] ./requirements-dev.txt ---
annulé
--- [7/191] ./workflows/ci.yml ---
name: CI — render + tests

on:
  push:
  pull_request:

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install deps (project)
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          # pytest au cas où il n’est pas listé dans requirements.txt
          pip install pytest

      - name: Ensure scripts are executable
        run: |
          chmod +x bin/*.sh || true

      - name: Render report (produit docs/index.html + docs/health.json)
        run: |
          bash bin/safe_render.sh

      - name: Run tests
        run: |
          pytest -q

      - name: Upload logs (diagnostic si échec)
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scalp-logs
          path: |
            logs/**
            docs/**--- [8/191] ./requirements.txt ---
# Core scientific stack – stay on numpy 1.26.x for ABI stability on 22.04
numpy==1.26.4
pandas==2.2.2
scipy==1.15.3
pyyaml>=6.0
tqdm>=4.66
optuna>=4.0

# Optional speedups that broke you; keep <1.4 to avoid ARRAY_API crash
bottleneck<1.4

# Others your project used
plotly>=6,<7
jinja2>=3.1
rich>=13--- [9/191] ./tools/health_check.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Health check rapide (dépendances + chemins + fichiers clés).
Usage: python tools/health_check.py
"""

from __future__ import annotations
import os, sys, importlib, json, shutil, time, yaml

OK = "OK"
KO = "KO"

REQUIRED_PKGS = [
    ("pandas",   "pd"),
    ("numpy",    "np"),
    ("pyyaml",   "yaml"),
    ("rich",     "rich"),
    ("plotly",   "plotly"),
    ("altair",   "altair"),
    ("pyarrow",  "pyarrow"),
]

CONFIG_PATH = "engine/config/config.yaml"

def check_imports():
    out = []
    for pkg, _alias in REQUIRED_PKGS:
        try:
            m = importlib.import_module(pkg)
            ver = getattr(m, "__version__", "?")
            out.append((pkg, OK, ver))
        except Exception as e:
            out.append((pkg, KO, str(e).split("\n", 1)[0]))
    return out

def load_yaml(path, missing_ok=False):
    if missing_ok and not os.path.isfile(path): return {}
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}

def fmt(pct):
    try:
        return f"{pct:.1f}%"
    except Exception:
        return "n/a"

def main():
    print("== SCALP • health_check ==")
    # 1) Imports
    print("\n[1/4] Dépendances Python")
    for pkg, status, info in check_imports():
        print(f" - {pkg:<10} : {status}  ({info})")

    # 2) Config + dossiers
    print("\n[2/4] Configuration & dossiers")
    cfg = load_yaml(CONFIG_PATH, missing_ok=True)
    if not cfg:
        print(f" - {CONFIG_PATH} : {KO} (introuvable)")
        data_dir = "/notebooks/scalp_data/data"
        reports_dir = "/notebooks/scalp_data/reports"
    else:
        print(f" - {CONFIG_PATH} : {OK}")
        rt = cfg.get("runtime", {})
        data_dir = rt.get("data_dir", "/notebooks/scalp_data/data")
        reports_dir = rt.get("reports_dir", "/notebooks/scalp_data/reports")
        print(f"   data_dir    = {data_dir}")
        print(f"   reports_dir = {reports_dir}")
        print(f"   risk_mode   = {rt.get('risk_mode', 'n/a')}")
        print(f"   termboard_enabled = {rt.get('termboard_enabled', False)}")

    for path in (data_dir, reports_dir):
        print(f" - dir {path} : {OK if os.path.isdir(path) else KO}")

    # 3) Fichiers clés
    print("\n[3/4] Fichiers clés")
    summ = os.path.join(reports_dir, "summary.json")
    nxt  = os.path.join(reports_dir, "strategies.yml.next")
    cur  = "engine/config/strategies.yml"
    for p in (summ, nxt, cur):
        if os.path.isfile(p):
            ts = time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime(os.path.getmtime(p)))
            print(f" - {p} : {OK} (modifié {ts} UTC)")
        else:
            print(f" - {p} : {KO}")

    # 4) Disque (espace libre)
    print("\n[4/4] Espace disque")
    try:
        total, used, free = shutil.disk_usage("/")
        used_pct = used / total * 100.0
        print(f" - free={free//(1024**3)} GiB • used={fmt(used_pct)}")
    except Exception:
        print(" - n/a")

    print("\nConseil: si plotly/altair manquent → vérifier `sitecustomize.py` et/ou relancer `bot.py` pour bootstrap.")

if __name__ == "__main__":
    main()--- [10/191] ./tools/publish_pages.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations
import json, os, subprocess, sys, time, shutil
from pathlib import Path

REPO_PATH  = Path(os.environ.get("REPO_PATH", "/notebooks/scalp")).resolve()
DOCS_DIR   = REPO_PATH / "docs"
DATA_DIR   = DOCS_DIR / "data"
REPORTS_DIR= REPO_PATH / "reports"
DOCS_DIR.mkdir(parents=True, exist_ok=True)
DATA_DIR.mkdir(parents=True, exist_ok=True)

os.environ.setdefault("GIT_TERMINAL_PROMPT", "0")

def [REDACTED]():
    """Charge /notebooks/.env si GIT_USER/TOKEN=[REDACTED] manquent."""
    need = any(not os.environ.get(k) for k in ("GIT_USER","GIT_TOKEN=[REDACTED]
    if not need:
        return
    for candidate in ("/notebooks/.env", "/notebooks/scalp/.env"):
        p = Path(candidate)
        if not p.exists():
            continue
        try:
            with p.open("r", encoding="utf-8") as f:
                for line in f:
                    s = line.strip()
                    if not s or s.startswith("#") or "=" not in s:
                        continue
                    k, v = s.split("=", 1)
                    k, v = k.strip(), v.strip()
                    if (v.startswith('"') and v.endswith('"')) or (v.startswith("'") and v.endswith("'")):
                        v = v[1:-1]
                    # ne pas écraser si déjà défini
                    os.environ.setdefault(k, v)
            print(f"[publish] .env chargé depuis {p}")
            break
        except Exception as e:
            print(f"[publish] warn: lecture {p}: {e}")

def sh(args, cwd: Path|None=None, check=True)->str:
    r = subprocess.run(args, cwd=str(cwd) if cwd else None,
                       stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                       text=True, check=check)
    return r.stdout.strip()

def git_config(user, email):
    try:
        sh(["git","config","user.name", user],  cwd=REPO_PATH)
        sh(["git","config","user.email",email], cwd=REPO_PATH)
    except Exception as e:
        print(f"[publish] warn git_config: {e}")

def [REDACTED](user, token, repo, branch):
    try:
        sh(["git","rev-parse","--is-inside-work-tree"], cwd=REPO_PATH, check=True)
    except Exception:
        sh(["git","init"], cwd=REPO_PATH, check=True)
    auth = f"https://{user}:{token}@github.com/{user}/{repo}.git"
    sh(["git","remote","remove","origin"], cwd=REPO_PATH, check=False)
    sh(["git","remote","add","origin", auth], cwd=REPO_PATH, check=True)
    print("[publish] origin=github (token) ✔")

def copy_reports():
    copied=[]
    for name in ("status.json","summary.json","last_errors.json","strategies.yml.next","strategies.yml"):
        src = REPORTS_DIR / name
        if src.exists():
            shutil.copy2(src, DATA_DIR / name)
            copied.append(name)
    if copied:
        print(f"[publish] copiés -> docs/data/: {copied}")
    else:
        print("[publish] aucun JSON à copier (ok au premier run).")

def write_health(status="ok"):
    try:
        commit = sh(["git","rev-parse","--short","HEAD"], cwd=REPO_PATH, check=False) or "unknown"
    except Exception:
        commit = "unknown"
    payload = {"generated_at": int(time.time()), "commit": commit, "status": status}
    (DOCS_DIR / "health.json").write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
    print("[publish] écrit docs/health.json")

def pages_url(user, repo) -> str:
    return f"https://{user}.github.io/{repo}/"

def git_add_commit_push(branch):
    sh(["git","add","docs"], cwd=REPO_PATH, check=False)
    sh(["git","commit","-m","chore(pages): publish dashboard"], cwd=REPO_PATH, check=False)
    sh(["git","push","origin", branch], cwd=REPO_PATH, check=True)
    print(f"[publish] push -> origin/{branch} ✔")

def main():
    [REDACTED]()

    user  = os.environ.get("GIT_USER","").strip()
    token = [REDACTED]"GIT_TOKEN=[REDACTED]
    repo  = os.environ.get("GIT_REPO","").strip()
    branch= os.environ.get("GIT_BRANCH","main").strip()
    email = os.environ.get("GIT_EMAIL", f"{user}@users.noreply.github.com").strip()

    print(f"[publish] repo={REPO_PATH}")
    if not (user and token and repo):
        print("[publish] ❌GIT_USER / GIT_TOKEN=[REDACTED] / GIT_REPO manquant(s) dans l'env.")
        return

    git_config(user, email)
    [REDACTED](user, token, repo, branch)
    copy_reports()
    write_health("ok")
    git_add_commit_push(branch)

    url = pages_url(user, repo)
    out = REPO_PATH / "docs" / "pages_url.txt"
    out.write_text(url, encoding="utf-8")
    print(f"[publish] Pages URL: {url} (écrit dans {out})")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"[publish] FATAL: {e}")
        sys.exit(1)--- [11/191] ./tools/ensure_deps.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations
import importlib.util, subprocess, sys

def _is_installed(modname: str) -> bool:
    return importlib.util.find_spec(modname) is not None

def ensure(pkg_to_mod: dict[str, str]) -> dict[str, bool]:
    """
    pkg_to_mod: mapping 'pip-package' -> 'python_module'
    Installe via pip les packages dont le module n'est pas importable.
    Retourne un dict {pip-package: True/False} selon install OK.
    """
    res = {}
    for pip_name, modname in pkg_to_mod.items():
        if _is_installed(modname):
            res[pip_name] = True
            continue
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "--quiet", "--no-input", pip_name])
            res[pip_name] = _is_installed(modname)
        except Exception:
            res[pip_name] = False
    return res--- [12/191] ./tools/[REDACTED].py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Écrit dans un fichier texte l’URL console Paperspace pour ouvrir dashboard.html.
"""

import os

REPORT_PATH = "/notebooks/scalp_data/reports/dashboard.html"
HINT_FILE = "/notebooks/scalp_data/reports/dashboard_url.txt"

def main():
    cwd = os.getcwd()
    nb_id = None
    parts = cwd.split("/")
    for p in parts:
        if len(p) >= 8 and p.isalnum():
            nb_id = p
    if not nb_id:
        nb_id = "<ton-id-manuel>"

    url = f"https://console.paperspace.com/nbooks/{nb_id}/files/scalp_data/reports/dashboard.html"

    os.makedirs(os.path.dirname(HINT_FILE), exist_ok=True)
    with open(HINT_FILE, "w", encoding="utf-8") as f:
        f.write(url + "\n")

    print(f"[HINT] URL écrite dans {HINT_FILE}")

if __name__ == "__main__":
    main()--- [13/191] ./tools/[REDACTED].py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Rendu statique en images (PNG + PDF) du dashboard backtest.
Sorties à la racine du repo:
  ./dashboard.png
  ./dashboard.pdf
"""

from __future__ import annotations
import os, sys, json, yaml, importlib, subprocess
from datetime import datetime

# --- chemins
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
REPORTS_DIR  = os.getenv("SCALP_REPORTS_DIR", "/notebooks/scalp_data/reports")
SUMMARY      = os.path.join(REPORTS_DIR, "summary.json")
STRAT_NEXT   = os.path.join(REPORTS_DIR, "strategies.yml.next")
STRAT_CURR   = os.path.join(PROJECT_ROOT, "engine", "config", "strategies.yml")

OUT_PNG = os.path.join(PROJECT_ROOT, "dashboard.png")
OUT_PDF = os.path.join(PROJECT_ROOT, "dashboard.pdf")

TOP_K = int(os.getenv("SCALP_DASH_TOPK", "20"))

NEEDED = ["matplotlib", "seaborn", "pandas", "pyyaml", "reportlab"]

def _log(msg: str):
    print(f"[render-img] {msg}")

def _ensure_libs():
    missing = []
    for pkg in NEEDED:
        try:
            importlib.import_module(pkg)
        except Exception:
            missing.append(pkg)
    if not missing:
        return
    _log(f"install pkgs via pip: {missing}")
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install"] + missing)
    except subprocess.CalledProcessError as e:
        _log(f"pip install failed (code {e.returncode})")

def _load_json(p):
    if not os.path.isfile(p): return {}
    with open(p, "r", encoding="utf-8") as f: return json.load(f)

def _load_yaml(p, missing_ok=True):
    if missing_ok and not os.path.isfile(p): return {}
    with open(p, "r", encoding="utf-8") as f: return yaml.safe_load(f) or {}

def _score(r):
    pf = float(r.get("pf", 0)); mdd = float(r.get("mdd", 1))
    sh = float(r.get("sharpe", 0)); wr = float(r.get("wr", 0))
    return pf*2.0 + sh*0.5 + wr*0.5 - mdd*1.5

def _build_tables(pd, rows_sorted):
    # TOP K -> DataFrame
    top = rows_sorted[:TOP_K]
    if not top:
        return pd.DataFrame(), pd.DataFrame()
    df_top = pd.DataFrame([{
        "RANK": i+1,
        "PAIR": r["pair"],
        "TF": r["tf"],
        "PF": round(float(r.get("pf",0)), 3),
        "MDD%": round(float(r.get("mdd",0))*100.0, 2),
        "TR": int(r.get("trades",0)),
        "WR%": round(float(r.get("wr",0))*100.0, 1),
        "Sharpe": round(float(r.get("sharpe",0)), 3),
        "Note": round(_score(r), 3),
    } for i, r in enumerate(top)])

    # Heatmap PF: pivot pair x TF
    df_h = pd.DataFrame([{"pair": r["pair"], "tf": r["tf"], "pf": r.get("pf",0)} for r in rows_sorted])
    if df_h.empty:
        return df_top, pd.DataFrame()
    order = ["1m","3m","5m","15m","30m","1h","4h","1d"]
    df_h["tf"] = pd.Categorical(df_h["tf"], categories=order, ordered=True)
    heat = df_h.pivot_table(index="pair", columns="tf", values="pf", aggfunc="max")
    return df_top, heat

def _draw_png(pd, plt, sns, df_top, heat, risk_mode: str):
    # Figure haute résolution
    plt.close("all")
    fig = plt.figure(figsize=(14, 10), dpi=150)
    fig.suptitle(f"SCALP — Dashboard backtest  (policy={risk_mode})  {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}",
                 fontsize=12, fontweight="bold", y=0.98)

    # Grille 2 rangées: TOP table (haut), Heatmap (bas)
    gs = fig.add_gridspec(2, 1, height_ratios=[1.2, 1.8], hspace=0.25)

    # --- TOP K table
    ax1 = fig.add_subplot(gs[0, 0])
    ax1.axis("off")
    if df_top.empty:
        ax1.text(0.5, 0.5, "Aucun résultat TOP.", ha="center", va="center", fontsize=12)
    else:
        tbl = ax1.table(cellText=df_top.values,
                        colLabels=df_top.columns.tolist(),
                        loc="center", cellLoc="center")
        tbl.scale(1, 1.3)
        for (row, col), cell in tbl.get_celld().items():
            if row == 0:
                cell.set_facecolor("#1f2937"); cell.set_text_props(color="white", weight="bold")
            else:
                cell.set_facecolor("#ffffff")
        ax1.set_title(f"TOP {min(TOP_K, len(df_top))}", loc="left", fontsize=11, pad=8)

    # --- Heatmap PF
    ax2 = fig.add_subplot(gs[1, 0])
    if heat.empty:
        ax2.axis("off")
        ax2.text(0.5, 0.5, "Pas de données pour la heatmap PF.", ha="center", va="center", fontsize=12)
    else:
        sns.heatmap(heat, ax=ax2, cmap="RdYlGn", annot=True, fmt=".2f",
                    cbar_kws={"label":"PF"}, linewidths=0.5, linecolor="#e5e7eb")
        ax2.set_xlabel("TF"); ax2.set_ylabel("PAIR")
        ax2.set_title("PF max par paire × TF", fontsize=11)

    fig.savefig(OUT_PNG, bbox_inches="tight")
    _log(f"PNG écrit → {OUT_PNG}")

def _draw_pdf(pd, OUT_PDF):
    # PDF = convertir l'image PNG en PDF (via reportlab) pour iPhone
    try:
        from reportlab.lib.pagesizes import letter, A4
        from reportlab.pdfgen import canvas
        from reportlab.lib.utils import ImageReader
        from PIL import Image
    except Exception:
        # fallback: rien
        _log("reportlab/PIL absents — PDF non généré (PNG OK).")
        return
    try:
        img = Image.open(OUT_PNG)
        w, h = img.size
        page_w, page_h = A4
        scale = min(page_w / w, page_h / h)
        pdf = canvas.Canvas(OUT_PDF, pagesize=A4)
        pdf.drawImage(ImageReader(img), x=0, y=0, width=w*scale, height=h*scale, preserveAspectRatio=True, anchor='sw')
        pdf.showPage(); pdf.save()
        _log(f"PDF écrit → {OUT_PDF}")
    except Exception as e:
        _log(f"PDF échec: {e}")

def generate():
    _ensure_libs()

    import pandas as pd
    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.pyplot as plt
    import seaborn as sns

    # données
    sm = _load_json(SUMMARY)
    rows = sm.get("rows", [])
    risk_mode = sm.get("risk_mode", "normal")

    rows_sorted = sorted(rows, key=[REDACTED] reverse=True)
    df_top, heat = _build_tables(pd, rows_sorted)

    # rendu
    _draw_png(pd, plt, sns, df_top, heat, risk_mode)
    _draw_pdf(pd, OUT_PDF)

if __name__ == "__main__":
    try:
        generate()
    except Exception as e:
        _log(f"erreur fatale: {e}")
        sys.exit(1)--- [14/191] ./tools/migrate-to-engine.py ---
# ops/migrate_to_engine.py
from __future__ import annotations
import re, shutil
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
ENGINE = ROOT / "engine"
ENGINE.mkdir(exist_ok=True)

# 1) Déplacer anciens packages s'ils existent
CANDIDATE_PKGS = ["scalper", "scalp"]  # anciens noms possibles de package interne
for name in CANDIDATE_PKGS:
    src = ROOT / name
    if src.exists() and src.is_dir():
        dst = ENGINE
        # on déplace le contenu interne dans engine/
        for p in src.iterdir():
            dest = dst / p.name
            if dest.exists():
                continue
            shutil.move(str(p), str(dest))
        # on laisse le répertoire racine (vide) à supprimer manuellement si besoin

# 2) Mettre à jour les imports dans tout le repo (hors TRASH et .git)
PATTERNS = [
    (re.compile(r"\bscalper\."), "engine."),
    (re.compile(r"\bscalp\."), "engine."),   # ancien package interne homonyme du repo
]
def fix_file(path: Path) -> None:
    try:
        txt = path.read_text(encoding="utf-8")
    except Exception:
        return
    orig = txt
    for rx, repl in PATTERNS:
        txt = rx.sub(repl, txt)
    if txt != orig:
        path.write_text(txt, encoding="utf-8")

for p in ROOT.rglob("*.py"):
    rel = p.relative_to(ROOT)
    if any(part.startswith("TRASH_") for part in rel.parts):
        continue
    if rel.parts and rel.parts[0] in (".git",):
        continue
    fix_file(p)

print("[✓] Migration terminée. Vérifie les imports et supprime l’ancien dossier vide si présent.")--- [15/191] ./tools/webhook.py ---
from flask import Flask, request, jsonify
import hmac, hashlib, os, subprocess, json, time

app = Flask(__name__)

def verify_signature(secret: [REDACTED] payload: bytes, sig256: str) -> bool:
    if not sig256 or not sig256.startswith("sha256="):
        return False
    digest = hmac.new(secret, payload, hashlib.sha256).hexdigest()
    return hmac.compare_digest(sig256, f"sha256={digest}")

def run_sync():
    log = subprocess.run(
        ["/opt/scalp/bin/git-sync.sh"],
        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, timeout=180
    )
    return log.returncode, log.stdout

@app.get("/ping")
def ping():
    return jsonify(ok=True, ts=int(time.time()))

@app.post("/gh")
def gh():
    # charge secret
    secret = [REDACTED]"WEBHOOK_SECRET=[REDACTED] or "").encode()
    if not secret:
        return jsonify(ok=False, error="No secret configured"), 500

    # vérifie signature
    sig = request.headers.get("X-Hub-Signature-256", "")
    payload = request.get_data()  # brut
    if not verify_signature(secret, payload, sig):
        return jsonify(ok=False, error="bad signature"), 401

    event = request.headers.get("X-GitHub-Event", "unknown")
    try:
        body = json.loads(payload or b"{}")
    except Exception:
        body = {}

    # on ne déclenche que sur push (par défaut), mais on accepte ping
    if event not in ("push", "ping"):
        return jsonify(ok=True, skipped=f"event {event}")

    code, out = run_sync()
    return jsonify(ok=(code == 0), code=code, log=out[-4000:])  # renvoie fin du log
--- [16/191] ./tools/render_guard.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations
import os, sys, hashlib, subprocess

PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
CONFIG_YAML  = os.path.join(PROJECT_ROOT, "engine", "config", "config.yaml")

def _load_yaml(path):
    import yaml
    if not os.path.isfile(path):
        return {}
    return yaml.safe_load(open(path,"r",encoding="utf-8")) or {}

def _hash(path: str) -> str:
    if not os.path.isfile(path):
        return ""
    with open(path, "rb") as f:
        import hashlib
        return hashlib.md5(f.read()).hexdigest()

def main():
    cfg = _load_yaml(CONFIG_YAML)
    rt = cfg.get("runtime", {}) if isinstance(cfg, dict) else {}
    reports_dir = rt.get("reports_dir", "/notebooks/scalp_data/reports")
    summary = os.path.join(reports_dir, "summary.json")
    guard = os.path.join(reports_dir, ".last_summary_hash")

    cur = _hash(summary)
    prev = ""
    if os.path.isfile(guard):
        prev = open(guard, "r", encoding="utf-8").read().strip()

    if cur and cur == prev:
        print("[render-guard] summary inchangé → skip render_report.py")
        return

    env = os.environ.copy()
    env["SCALP_REPORTS_DIR"] = reports_dir
    script = os.path.join(PROJECT_ROOT, "tools", "render_report.py")
    print("[render-guard] summary modifié → génération HTML…")
    subprocess.check_call([sys.executable, script], env=env, cwd=PROJECT_ROOT)

    if cur:
        with open(guard, "w", encoding="utf-8") as f:
            f.write(cur)

if __name__ == "__main__":
    main()--- [17/191] ./tools/trashify.py ---
#!/usr/bin/env python3
# tools/trashify.py
"""
Ménage du dépôt : déplace les éléments legacy/ inutiles vers .trash/<TIMESTAMP>/
- DRY RUN par défaut (aucune action tant que --apply n'est pas passé)
- Liste adaptée au dump le plus récent
- Prend soin de garder le dernier dump dans dumps/
- Sécurisé : ne touche jamais .git/ ni .trash/

Usage :
  python tools/trashify.py                 # aperçu (DRY RUN)
  python tools/trashify.py --apply         # exécute les déplacements
  python tools/trashify.py --restore PATH  # restaure depuis .trash/
  python tools/trashify.py --aggressive    # inclut fichiers/dev en plus (toujours DRY tant que --apply pas présent)
"""

from __future__ import annotations
import argparse
import os
import shutil
import time
from pathlib import Path
from typing import Iterable, List

REPO_ROOT = Path(__file__).resolve().parents[1]


# --- Cibles principales (safe) ------------------------------------------------
TRASH_DIRS_SAFE = [
    "scalper",                 # ancien moteur non utilisé par bot.py
    "tests",                   # dossiers de tests obsolètes
    "data",                    # les données doivent être hors dépôt
]

TRASH_GLOBS_COMMON = [
    "**/__pycache__",          # caches py
    "**/.ipynb_checkpoints",   # artefacts jupyter
]

TRASH_FILES_SAFE = [
    "engine/core/signal.py",   # doublon vs signals.py
    "dump.txt",                # vieux dump
    "requirements-dash.txt",   # on garde un seul requirements.txt
    "requirements-dev.txt",
]

# --- Cibles 'agressives' optionnelles ----------------------------------------
[REDACTED] = [
    "pytest.ini",
    ".pytest_cache",
]


# --- Helpers ------------------------------------------------------------------
def _existing(paths: Iterable[Path]) -> List[Path]:
    out: List[Path] = []
    for p in paths:
        try:
            if p.exists():
                out.append(p)
        except OSError:
            pass
    return out

def _is_protected(p: Path) -> bool:
    rp = p.resolve()
    # ne jamais toucher au repo root lui‑même, ni .git, ni .trash
    for forbid in [REPO_ROOT, REPO_ROOT / ".git", REPO_ROOT / ".trash"]:
        try:
            if rp == forbid.resolve() or str(rp).startswith(str(forbid.resolve())):
                return True
        except Exception:
            continue
    return False

def [REDACTED](aggressive: bool) -> List[Path]:
    items: List[Path] = []

    # dossiers exacts
    items += _existing([REPO_ROOT / d for d in TRASH_DIRS_SAFE])

    # fichiers exacts
    base_files = TRASH_FILES_SAFE + ([REDACTED] if aggressive else [])
    items += _existing([REPO_ROOT / f for f in base_files])

    # globs
    for pat in TRASH_GLOBS_COMMON:
        for p in REPO_ROOT.glob(pat):
            items.append(p)

    # filtre de sécurité
    safe = [p for p in items if not _is_protected(p)]
    # dédupliquer et trier (dossiers parents avant enfants)
    safe_sorted = sorted(set(safe), key=[REDACTED] x: (str(x).count(os.sep), str(x)))
    return safe_sorted

def _collect_old_dumps() -> List[Path]:
    """Dans dumps/, garder le fichier le plus récent et déplacer les autres."""
    d = REPO_ROOT / "dumps"
    if not d.exists() or not d.is_dir():
        return []
    files = sorted([p for p in d.glob("DUMP_*.txt") if p.is_file()],
                   key=[REDACTED] p: p.stat().st_mtime, reverse=True)
    if len(files) <= 1:
        return []
    # on garde files[0] (le plus récent), on déplace le reste
    return files[1:]

def _move_to_trash(paths: List[Path], apply: bool, label: str = "") -> None:
    if not paths:
        return
    dest_root = REPO_ROOT / ".trash" / time.strftime("%Y%m%d-%H%M%S")
    print(f"Destination: {dest_root} {label}".rstrip())
    for p in paths:
        rel = p.relative_to(REPO_ROOT)
        dest = dest_root / rel
        print(f"- {rel}  ->  .trash/{dest.relative_to(REPO_ROOT / '.trash')}")
        if apply:
            dest.parent.mkdir(parents=True, exist_ok=True)
            try:
                shutil.move(str(p), str(dest))
            except Exception as e:
                print(f"  [!] move failed: {e}")

def _restore_from_trash(src: Path) -> None:
    src = src.resolve()
    if (REPO_ROOT / ".trash") not in src.parents:
        raise SystemExit("Le chemin à restaurer doit provenir de .trash/")
    rel = src.relative_to(REPO_ROOT / ".trash")
    dest = REPO_ROOT / rel
    print(f"RESTORE  .trash/{rel} -> {rel}")
    dest.parent.mkdir(parents=True, exist_ok=True)
    shutil.move(str(src), str(dest))


# --- Main ---------------------------------------------------------------------
def main(argv: Iterable[str] | None = None) -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--apply", action="store_true", help="Exécuter réellement (sinon DRY RUN).")
    ap.add_argument("--restore", type=str, default="", help="Chemin à restaurer depuis .trash/")
    ap.add_argument("--aggressive", action="store_true", help="Inclut aussi fichiers/dev optionnels (pytest.ini, etc.).")
    args = ap.parse_args(list(argv) if argv is not None else None)

    if args.restore:
        _restore_from_trash(Path(args.restore))
        return 0

    print(f"[trashify] Repo : {REPO_ROOT}")
    basic = [REDACTED](args.aggressive)
    print(f"[trashify] Cibles de base détectées : {len(basic)}")
    _move_to_trash(basic, apply=args.apply)

    # dumps obsolètes (garde le plus récent)
    old_dumps = _collect_old_dumps()
    if old_dumps:
        print(f"[trashify] Dumps obsolètes : {len(old_dumps)} (le plus récent est conservé)")
        _move_to_trash(old_dumps, apply=args.apply, label="(dumps)")

    if not args.apply:
        print("\nDRY RUN — ajoute --apply pour déplacer réellement.")
    else:
        print("\n[✓] Déplacement terminé. Vérifie .trash/, puis commit/push si ok.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())--- [18/191] ./tools/export_to_pages.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Exporte les artefacts du dashboard vers /docs pour GitHub Pages.
- Copie dashboard.html (→ docs/index.html) + debug.txt/html
- Copie les JSON (summary/status/last_errors) vers docs/data/
- Option --push pour git add/commit/push (utilise GH_TOKEN=[REDACTED] si présent)
"""

from __future__ import annotations
import os, sys, shutil, json, time, subprocess
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parents[1]
DOCS_DIR  = REPO_ROOT / "docs"
DATA_DIR  = Path("/notebooks/scalp_data/data")
REPORTS_DIR = Path("/notebooks/scalp_data/reports")

def info(msg): print(f"[export] {msg}")

def ensure_dirs():
    (DOCS_DIR / "data").mkdir(parents=True, exist_ok=True)

def copy_if_exists(src: Path, dst: Path):
    if src.exists():
        dst.parent.mkdir(parents=True, exist_ok=True)
        shutil.copy2(str(src), str(dst))
        info(f"copied: {src.relative_to(REPO_ROOT)} -> {dst.relative_to(REPO_ROOT)}")
    else:
        info(f"missing: {src}")

def [REDACTED]():
    """Si dashboard.html n'existe pas, créer une page minimale."""
    idx = DOCS_DIR / "index.html"
    if not idx.exists():
        idx.write_text("""<!doctype html><meta charset="utf-8">
<title>SCALP</title><h1>SCALP</h1>
<p>dashboard.html introuvable — lance le maintainer pour le générer.</p>
""", encoding="utf-8")

def export_docs():
    ensure_dirs()

    # 1) dashboard -> index.html (Pages sert /docs/index.html)
    dash_src = REPO_ROOT / "dashboard.html"
    dash_dst = DOCS_DIR / "index.html"
    copy_if_exists(dash_src, dash_dst)

    # 2) debug
    copy_if_exists(REPO_ROOT / "debug.txt",  DOCS_DIR / "debug.txt")
    copy_if_exists(REPO_ROOT / "debug.html", DOCS_DIR / "debug.html")

    # 3) data JSON (pour éventuelles pages JS futures)
    copy_if_exists(REPORTS_DIR / "summary.json",     DOCS_DIR / "data" / "summary.json")
    copy_if_exists(REPORTS_DIR / "status.json",      DOCS_DIR / "data" / "status.json")
    copy_if_exists(REPORTS_DIR / "last_errors.json", DOCS_DIR / "data" / "last_errors.json")

    # 4) fallback si pas de dashboard
    [REDACTED]()

def git(*args, check=True):
    return subprocess.run(["git", *args], cwd=str(REPO_ROOT), check=check, capture_output=True, text=True)

def maybe_push(commit_msg: str):
    # Config git si besoin
    try:
        git("rev-parse", "--is-inside-work-tree")
    except subprocess.CalledProcessError:
        info("Ce dossier n'est pas un dépôt git. Skip push.")
        return

    # User/Email fallback
    try:
        git("config", "user.email")
    except subprocess.CalledProcessError:
        git("config", "user.email", "bot@local")
    try:
        git("config", "user.name")
    except subprocess.CalledProcessError:
        git("config", "user.name", "SCALP Bot")

    # Ajouter /docs
    git("add", "docs")

    # Commit si changements
    cp = subprocess.run(["git", "diff", "--cached", "--quiet"], cwd=str(REPO_ROOT))
    if cp.returncode == 0:
        info("Aucun changement à committer.")
        return
    git("commit", "-m", commit_msg)

    # Remote avec token si GH_TOKEN=[REDACTED] et GH_REPO fournis
    token = [REDACTED]"GH_TOKEN=[REDACTED] "").strip()
    gh_repo = os.environ.get("GH_REPO", "").strip()  # ex: "monuser/scalp"
    if token and gh_repo:
        # set remote origin si absent
        remotes = git("remote", "show").stdout.strip().splitlines()
        if "origin" not in remotes:
            url = f"https://x-access-token:[REDACTED]"
            git("remote", "add", "origin", url)
        else:
            # réécrit l'URL en https token si besoin
            url = f"https://x-access-token:[REDACTED]"
            git("remote", "set-url", "origin", url)

        # push
        try:
            git("push", "origin", "HEAD:main")
            info("Pushed to origin main.")
        except subprocess.CalledProcessError as e:
            info(f"Push failed: {e.stderr}")
    else:
        info("GH_TOKEN=[REDACTED] ou GH_REPO manquant → push ignoré.")

def main():
    import argparse
    ap = argparse.ArgumentParser()
    ap.add_argument("--push", action="store_true", help="git add/commit/push après export")
    args = ap.parse_args()

    export_docs()
    if args.push:
        ts = time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())
        maybe_push(commit_msg=f"pages: update artifacts at {ts} UTC")

    print("\n[export] Done. Ouvre GitHub Pages après déploiement: https://<user>.github.io/<repo>/")

if __name__ == "__main__":
    main()--- [19/191] ./tools/exp_tracker.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations
import os, json, time, uuid
from typing import Dict, Any

def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)

def new_run_id() -> str:
    return time.strftime("%Y%m%d-%H%M%S") + "-" + uuid.uuid4().hex[:8]

def log_event(base_dir: str, run_id: str, event: Dict[str, Any]):
    ensure_dir(base_dir)
    path = os.path.join(base_dir, f"{run_id}.jsonl")
    event = {"ts": int(time.time()), **event}
    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(event, ensure_ascii=False) + "\n")--- [20/191] ./tools/safe_dump.sh ---
#!/usr/bin/env bash
set -euo pipefail

ROOT="${1:-/opt/scalp}"
OUT="${2:-dump.txt}"

cd "$ROOT"
: > "$OUT"

FILES=$(find . \
  -path "./.git" -prune -o \
  -path "./venv" -prune -o \
  -path "./.venv" -prune -o \
  -path "./__pycache__" -prune -o \
  -path "./logs" -prune -o \
  -path "./docs" -prune -o \
  -path "./notebooks/scalp_data/data" -prune -o \
  -path "./notebooks/scalp_data/reports" -prune -o \
  -type f \( -iname "*.py" -o -iname "*.sh" -o -iname "*.txt" -o -iname "*.md" -o -iname "*.yml" -o -iname "*.yaml" -o -iname "*.json" -o -iname "*.ini" -o -iname "*.cfg" -o -iname "*.toml" -o -iname "*.service" \) \
  -print)

COUNT=$(echo "$FILES" | wc -l)
echo "### FILES=$COUNT date=$(date -u +%F_%T) root=$ROOT" | tee -a "$OUT"

i=0
echo "$FILES" | while IFS= read -r f; do
  i=$((i+1))
  echo "--- [$i/$COUNT] $f ---" | tee -a "$OUT"
  sed -n '1,400p' "$f" | sed -E \
    -e 's/([A-Z_]*?(API|ACCESS)?[_-]?KEY=[REDACTED] \
    -e 's/([A-Z_]*?(API)?[_-]?SECRET=[REDACTED] \
    -e 's/([Pp]assphrase[[:space:]]*[:=][[:space:]]*)[^"'"'"'[:space:]]+/\1[REDACTED]/g' \
    -e 's/([Tt]oken[[:space:]]*[:=][[:space:]]*)[^"'"'"'[:space:]]+/\1[REDACTED]/g' \
    >> "$OUT"
done

echo "✅ Dump fini : $OUT"
wc -l "$OUT"; du -h "$OUT"
--- [21/191] ./tools/render_report.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
tools/render_report.py

Génère un dashboard statique (index.html) dans <REPO_PATH>/docs/
+ push automatique vers GitHub Pages (via tools.publish_pages).

Points clés:
- Auto-refresh 5 s (compte à rebours + bouton "Refresh")
- Cartes "Health" / "Compteurs" avec placeholders si pas de data
- Heatmap PF pair × TF (Plotly), sinon placeholder graphique
- TOP résultats avec filtre pair/TF côté client
- Lecture des JSON depuis <REPO_PATH>/reports/
- Copie/push des artefacts vers /docs/ via publish_pages

ENV utiles:
  REPO_PATH=/notebooks/scalp         # défaut
  GIT_USER / GIT_TOKEN=[REDACTED] / GIT_REPO    # pour publish_pages
"""

from __future__ import annotations
import json, os, sys, time, traceback
from pathlib import Path
from datetime import datetime, timezone

# -----------------------------------------------------------------------------
# Helpers
# -----------------------------------------------------------------------------

REPO_PATH = Path(os.environ.get("REPO_PATH", "/notebooks/scalp")).resolve()
DOCS_DIR  = REPO_PATH / "docs"
DATA_DIR  = DOCS_DIR / "data"
REPORTS_DIR = REPO_PATH / "reports"
DOCS_DIR.mkdir(parents=True, exist_ok=True)
DATA_DIR.mkdir(parents=True, exist_ok=True)

def _load_json(p: Path):
    try:
        if not p.exists():
            return None
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return None

def _ts_utc_now() -> int:
    return int(time.time())

def _ts_to_iso_utc(ts: int | float | None) -> str:
    if not ts:
        return "—"
    return datetime.fromtimestamp(int(ts), tz=timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")

def _age_human(ts: int | float | None) -> str:
    if not ts:
        return "n/a"
    delta = max(0, _ts_utc_now() - int(ts))
    if delta < 60:
        return f"{delta}s"
    if delta < 3600:
        m = delta // 60
        return f"{m} min"
    h = delta // 3600
    return f"{h} h"

def _safe_get(d: dict | None, *keys, default=None):
    cur = d or {}
    for k in keys:
        if not isinstance(cur, dict):
            return default
        cur = cur.get(k)
    return cur if cur is not None else default

# -----------------------------------------------------------------------------
# Lecture données
# -----------------------------------------------------------------------------

def read_inputs():
    # status.json: photo MIS/OLD/DAT/OK
    status = _load_json(REPORTS_DIR / "status.json") or {}
    # summary.json: résumé backtest (top, heatmap, etc.)
    summary = _load_json(REPORTS_DIR / "summary.json") or {}
    # health.json: généré au publish
    health = _load_json(DOCS_DIR / "health.json") or {}

    # Compteurs
    counts = {
        "MIS": int(_safe_get(status, "counts", "MIS", default=0) or 0),
        "OLD": int(_safe_get(status, "counts", "OLD", default=0) or 0),
        "DAT": int(_safe_get(status, "counts", "DAT", default=0) or 0),
        "OK":  int(_safe_get(status, "counts", "OK",  default=0) or 0),
    }
    # Fraîcheur (pour la bannière)
    generated_at = _safe_get(summary, "generated_at", default=None)
    risk_mode    = _safe_get(summary, "risk_mode", default="n/a")
    walkf        = _safe_get(summary, "meta", "walk_forward", default={})
    opti         = _safe_get(summary, "meta", "optuna", default={})
    rows         = _safe_get(summary, "rows", default=[]) or []

    # Pour heatmap: on attend des lignes avec pair, tf, pf
    heat_rows = []
    for r in rows:
        pair = r.get("pair") or r.get("symbol") or r.get("pair_tf", "").split(":")[0]
        tf   = r.get("tf")   or (r.get("pair_tf", "").split(":")[1] if ":" in r.get("pair_tf","") else None)
        pf   = r.get("metrics", {}).get("pf") or r.get("pf")
        if pair and tf and pf is not None:
            try:
                heat_rows.append((pair, tf, float(pf)))
            except Exception:
                pass

    # Liste pairs / tfs pour filtre TOP
    pairs = sorted({p for p, _, _ in heat_rows}) if heat_rows else sorted({r.get("pair") for r in rows if r.get("pair")})
    tfs   = sorted({t for _, t, _ in heat_rows}) if heat_rows else sorted({r.get("tf") for r in rows if r.get("tf")})

    payload = {
        "status": status,
        "summary": summary,
        "health": health,
        "counts": counts,
        "rows": rows,
        "heat_rows": heat_rows,
        "pairs": [p for p in pairs if p],
        "tfs":   [t for t in tfs   if t],
        "generated_at": generated_at,
        "risk_mode": risk_mode,
        "walk_forward": walkf,
        "optuna": opti,
    }
    return payload

# -----------------------------------------------------------------------------
# Rendu HTML
# -----------------------------------------------------------------------------

PLOTLY_CDN = "https://cdn.plot.ly/plotly-2.27.0.min.js"

def render_html(data: dict) -> str:
    counts = data["counts"]
    rows   = data["rows"]
    heat   = data["heat_rows"]
    pairs  = data["pairs"]
    tfs    = data["tfs"]

    now_iso = _ts_to_iso_utc(_ts_utc_now())
    gen_iso = _ts_to_iso_utc(data.get("generated_at"))
    gen_age = _age_human(data.get("generated_at"))

    # prépare données heatmap pour JS
    # on crée un dict {pair: {tf: pf}}
    heat_map = {}
    for p, tf, pf in heat:
        heat_map.setdefault(p, {})[tf] = pf

    # Top K (limité pour lisibilité)
    # score par défaut: PF desc, WR desc, -MDD
    def _score(r):
        m = r.get("metrics", {})
        pf   = m.get("pf") or 0.0
        wr   = m.get("wr") or 0.0
        mdd  = m.get("mdd") or 0.0
        # simple score
        return (float(pf), float(wr), -float(mdd))

    top_rows = sorted(rows, key=[REDACTED] reverse=True)[:50]

    # JS: on injecte heat_map, pairs, tfs, et top_rows pour filtres client
    def js_json_safe(obj):
        return json.dumps(obj, ensure_ascii=False)

    html = f"""
<!DOCTYPE html>
<html lang="fr">
<head>
<meta charset="utf-8">
<title>SCALP — Dashboard</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="{PLOTLY_CDN}"></script>
<style>
  :root {{
    --bg:#0b0f14; --card:#121820; --text:#e6edf3; --muted:#9aa6b2;
    --ok:#2aa745; --dat:#c89d28; --old:#d0443e; --mis:#7b8a97; --chip:#1f2937;
    --accent:#3b82f6; --border:#223040;
  }}
  html,body {{ background:var(--bg); color:var(--text); font:16px/1.45 system-ui, -apple-system, Segoe UI, Roboto, Arial; margin:0; padding:0; }}
  .wrap {{ max-width:1100px; margin:0 auto; padding:20px; }}
  h1 {{ font-size:2.1rem; margin:10px 0 8px; }}
  small.muted {{ color:var(--muted); }}
  .row {{ display:grid; grid-template-columns: 1fr; gap:16px; }}
  @media (min-width: 900px) {{
    .row-2 {{ grid-template-columns: 1fr 1fr; }}
  }}
  .card {{ background:var(--card); border:1px solid var(--border); border-radius:10px; padding:18px; }}
  .title {{ font-size:1.25rem; margin:0 0 10px; }}
  .chips span {{ display:inline-block; background:var(--chip); padding:6px 10px; border-radius:999px; margin-right:8px; font-weight:600; }}
  .chips .mis {{ background:var(--mis); color:#081217; }}
  .chips .old {{ background:var(--old); color:#fff; }}
  .chips .dat {{ background:var(--dat); color:#0b0f14; }}
  .chips .ok  {{ background:var(--ok);  color:#0b0f14; }}
  .muted {{ color:var(--muted); }}
  .flex {{ display:flex; align-items:center; gap:12px; flex-wrap:wrap; }}
  button.refresh {{ background:var(--accent); color:white; border:0; padding:6px 12px; border-radius:8px; cursor:pointer; }}
  table {{ width:100%; border-collapse:collapse; }}
  th,td {{ padding:8px 10px; border-bottom:1px solid var(--border); }}
  th {{ text-align:left; color:var(--muted); font-weight:600; }}
  tr:hover td {{ background:#0f141c; }}
  .right {{ text-align:right; }}
  .placeholder {{ border:1px dashed var(--border); color:var(--muted); padding:18px; border-radius:10px; }}
  .badge {{ padding:3px 8px; border-radius:6px; font-weight:700; }}
  .badge.ok  {{ background:var(--ok);  color:#0b0f14; }}
  .badge.dat {{ background:var(--dat); color:#0b0f14; }}
  .badge.old {{ background:var(--old); color:#fff;    }}
  .badge.mis {{ background:var(--mis); color:#081217;}}
</style>
</head>
<body>
<div class="wrap">
  <h1>SCALP — Dashboard <small class="muted">({now_iso})</small></h1>
  <div class="flex" style="margin:6px 0 18px;">
    <div class="muted">Auto-refresh: <b id="rf-interval">5s</b> · reload dans <span id="rf-count">5</span>s</div>
    <button class="refresh" onclick="location.reload()">Refresh</button>
  </div>

  <div class="row row-2">
    <div class="card">
      <div class="title">Health</div>
      <div id="health">
        <div><span class="muted">generated_at:</span> <b>{_safe_get(data,'health','generated_at',default='—')}</b></div>
        <div><span class="muted">commit:</span> <b>{_safe_get(data,'health','commit',default='—')}</b></div>
        <div><span class="muted">status:</span> <b style="color:var(--ok);">{_safe_get(data,'health','status',default='pending')}</b></div>
      </div>
    </div>

    <div class="card">
      <div class="title">Compteurs</div>
      <div class="chips">
        <span class="mis">MIS: {counts['MIS']}</span>
        <span class="old">OLD: {counts['OLD']}</span>
        <span class="dat">DAT: {counts['DAT']}</span>
        <span class="ok">OK: {counts['OK']}</span>
      </div>
      <div class="muted" style="margin-top:8px;">
        MIS: no data · OLD: stale · DAT: fresh CSV, no active strategy · OK: fresh CSV + active strategy
      </div>
    </div>
  </div>

  <div class="card" style="margin-top:16px;">
    <div class="title">Heatmap (pair × TF)</div>
    <div id="heat" class="placeholder">Matrice en préparation… en attente des premiers résultats/metrics.</div>
  </div>

  <div class="card" style="margin-top:16px;">
    <div class="title">TOP résultats</div>
    <div class="flex" style="margin-bottom:10px;">
      <label>Pair :
        <select id="f_pair">
          <option value="">(toutes)</option>
          {"".join(f'<option value="{p}">{p}</option>' for p in pairs)}
        </select>
      </label>
      <label>TF :
        <select id="f_tf">
          <option value="">(tous)</option>
          {"".join(f'<option value="{t}">{t}</option>' for t in tfs)}
        </select>
      </label>
    </div>
    <div id="top_table"></div>
  </div>

  <div class="muted" style="margin:20px 0 6px;">
    <small>Dernier backtest: <b>{gen_iso}</b> (age: {gen_age}) · risk_mode: <b>{data.get('risk_mode','n/a')}</b></small>
  </div>
</div>

<script>
  // ---- Auto-refresh 5s
  let left = 5;
  const span = document.getElementById('rf-count');
  setInterval(() => {{
    left = Math.max(0, left-1);
    if (span) span.textContent = left;
    if (left === 0) location.reload();
  }}, 1000);

  // ---- Données injectées (côté client) pour filtres/plot
  const HEAT = {js_json_safe(heat_map)};
  const PAIRS = {js_json_safe(pairs)};
  const TFS = {js_json_safe(tfs)};
  const TOP_ROWS = {js_json_safe(top_rows)};

  // ---- Heatmap Plotly
  function renderHeat() {{
    const el = document.getElementById('heat');
    if (!el) return;
    const pairs = PAIRS;
    const tfs = TFS;
    if (!pairs.length || !tfs.length) {{
      el.className = 'placeholder';
      el.innerText = 'Aucune matrice exploitable (pas encore de PF par pair×TF).';
      return;
    }}
    const z = [];
    for (let i=0;i<pairs.length;i++) {{
      const row = [];
      for (let j=0;j<tfs.length;j++) {{
        const p = pairs[i];
        const tf = tfs[j];
        const v = (HEAT[p] && HEAT[p][tf] != null) ? HEAT[p][tf] : null;
        row.push(v);
      }}
      z.push(row);
    }}
    el.className = '';
    el.innerHTML = '';
    const data = [{{
      z: z, x: tfs, y: pairs, type:'heatmap', colorscale:'Viridis', hoverongaps:false,
      colorbar: {{title:'PF', outlinewidth:0}}
    }}];
    const layout = {{
      paper_bgcolor:'#121820', plot_bgcolor:'#121820',
      font:{{color:'#e6edf3'}},
      margin:{{l:80,r:10,t:10,b:40}}
    }};
    Plotly.newPlot(el, data, layout, {{responsive:true, displayModeBar:false}});
  }}

  // ---- TOP table + filtres
  function number(x, d=2) {{
    if (x===null||x===undefined||isNaN(x)) return '–';
    return Number(x).toFixed(d);
  }}
  function badge(state) {{
    const cls = state || '';
    const val = (state||'').toUpperCase() || '—';
    cls = locals().get("cls", "bg-secondary"); cls = locals().get("cls", "bg-secondary"); return f'<span class=\"badge bg-secondary\">\2</span>';
  }}
  function renderTop() {{
    const selPair = document.getElementById('f_pair').value;
    const selTf = document.getElementById('f_tf').value;
    let rows = TOP_ROWS.slice();
    if (selPair) rows = rows.filter(r => (r.pair===selPair)||(r.symbol===selPair)||(r.pair_tf||'').startsWith(selPair+':'));
    if (selTf)   rows = rows.filter(r => (r.tf===selTf)||(r.pair_tf||'').endsWith(':'+selTf));
    const head = `
      <table>
        <thead><tr>
          <th>Pair</th><th>TF</th><th>PF</th><th>WR</th><th>MDD</th><th>Trades</th><th>Name</th>
        </tr></thead><tbody>`;
    const body = rows.map(r => {{
      const m = r.metrics||{{}};
      const pair = r.pair || (r.pair_tf||'').split(':')[0] || r.symbol || '—';
      const tf   = r.tf   || (r.pair_tf||'').split(':')[1] || '—';
      return `<tr>
        <td>${{pair}}</td>
        <td>${{tf}}</td>
        <td class="right">${{number(m.pf,2)}}</td>
        <td class="right">${{number(m.wr,2)}}</td>
        <td class="right">${{number(m.mdd,2)}}</td>
        <td class="right">${{m.trades??'–'}}</td>
        <td>${{r.name||'—'}}</td>
      </tr>`;
    }}).join('');
    const tail = `</tbody></table>`;
    document.getElementById('top_table').innerHTML = head + (body || `<div class="placeholder">Aucun résultat pour ce filtre.</div>`) + tail;
  }}

  document.getElementById('f_pair').addEventListener('change', renderTop);
  document.getElementById('f_tf').addEventListener('change', renderTop);

  renderHeat();
  renderTop();
</script>
</body>
</html>
"""
    return html

# -----------------------------------------------------------------------------
# Publication GitHub Pages
# -----------------------------------------------------------------------------

def publish_pages():
    try:
        from tools import publish_pages as pub
        print("[render] publish via import tools.publish_pages …")
        pub.main()
        return
    except Exception as e:
        print(f"[render] ⚠️Publication ignorée (erreur import): {e}")
        # trace si utile:
        # traceback.print_exc()

# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------

def main():
    data = read_inputs()
--- [22/191] ./tools/.ipynb_checkpoints/migrate-to-engine-checkpoint.py ---
# ops/migrate_to_engine.py
from __future__ import annotations
import re, shutil
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
ENGINE = ROOT / "engine"
ENGINE.mkdir(exist_ok=True)

# 1) Déplacer anciens packages s'ils existent
CANDIDATE_PKGS = ["scalper", "scalp"]  # anciens noms possibles de package interne
for name in CANDIDATE_PKGS:
    src = ROOT / name
    if src.exists() and src.is_dir():
        dst = ENGINE
        # on déplace le contenu interne dans engine/
        for p in src.iterdir():
            dest = dst / p.name
            if dest.exists():
                continue
            shutil.move(str(p), str(dest))
        # on laisse le répertoire racine (vide) à supprimer manuellement si besoin

# 2) Mettre à jour les imports dans tout le repo (hors TRASH et .git)
PATTERNS = [
    (re.compile(r"\bscalper\."), "engine."),
    (re.compile(r"\bscalp\."), "engine."),   # ancien package interne homonyme du repo
]
def fix_file(path: Path) -> None:
    try:
        txt = path.read_text(encoding="utf-8")
    except Exception:
        return
    orig = txt
    for rx, repl in PATTERNS:
        txt = rx.sub(repl, txt)
    if txt != orig:
        path.write_text(txt, encoding="utf-8")

for p in ROOT.rglob("*.py"):
    rel = p.relative_to(ROOT)
    if any(part.startswith("TRASH_") for part in rel.parts):
        continue
    if rel.parts and rel.parts[0] in (".git",):
        continue
    fix_file(p)

print("[✓] Migration terminée. Vérifie les imports et supprime l’ancien dossier vide si présent.")--- [23/191] ./tools/.ipynb_checkpoints/trashify-checkpoint.py ---
--- [24/191] ./tools/.ipynb_checkpoints/dump-repo-checkpoint.py ---
#!/usr/bin/env python3
from __future__ import annotations
import datetime as dt, sys
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parents[1]
DUMP_DIR = REPO_ROOT / "dumps"; DUMP_DIR.mkdir(parents=True, exist_ok=True)
TS = dt.datetime.now().strftime("%Y%m%d-%H%M%S")
OUT_PATH = DUMP_DIR / f"DUMP_{TS}.txt"

IGNORE_EXT = {".png",".jpg",".jpeg",".gif",".pdf",".pkl",".db",".sqlite",".zip",".tar",".gz",".pyc",".pyo"}
IGNORE_DIRS = {".git","__pycache__", ".ipynb_checkpoints", "dumps"}
IGNORE_PREFIX = {"TRASH_"}

def is_text_file(p: Path) -> bool:
    if p.suffix.lower() in IGNORE_EXT: return False
    try:
        with p.open("rb") as f:
            if b"\x00" in f.read(1024): return False
        return True
    except Exception: return False

def write_header(f, t): f.write("\n"+"="*80+"\n"+t+"\n"+"="*80+"\n")

def dump_tree(root: Path, f):
    write_header(f,"ARBORESCENCE")
    for p in sorted(root.rglob("*")):
        rel=p.relative_to(root)
        if any(part in IGNORE_DIRS for part in rel.parts): continue
        if any(str(rel).startswith(pref) for pref in IGNORE_PREFIX): continue
        if p.is_file():
            m=dt.datetime.fromtimestamp(p.stat().st_mtime).strftime("%Y-%m-%d %H:%M:%S")
            f.write(f"{rel}  (last modified: {m})\n")
        else:
            f.write(str(rel)+"/\n")

def dump_files(root: Path, f):
    write_header(f,"FICHIERS COMPLETS")
    for p in sorted(root.rglob("*")):
        rel=p.relative_to(root)
        if p.is_dir(): continue
        if any(part in IGNORE_DIRS for part in rel.parts): continue
        if any(str(rel).startswith(pref) for pref in IGNORE_PREFIX): continue
        if not is_text_file(p): continue
        try: lines=p.read_text(encoding="utf-8",errors="replace").splitlines()
        except Exception as e:
            f.write(f"\n[!!] Impossible de lire {rel}: {e}\n"); continue
        m=dt.datetime.fromtimestamp(p.stat().st_mtime).strftime("%Y-%m-%d %H:%M:%S")
        f.write("\n"+"-"*80+"\n"); f.write(f"FILE: {rel}  (last modified: {m})\n"); f.write("-"*80+"\n")
        for i, line in enumerate(lines, 1): f.write(f"{i:6d}: {line}\n")

def prune_old_dumps():
    for old in sorted(DUMP_DIR.glob("DUMP_*.txt")):
        if old != OUT_PATH:
            try: old.unlink(); print(f"[x] Ancien dump supprimé: {old.name}")
            except Exception as e: print(f"[!] Suppression échouée {old}: {e}")

def main() -> int:
    with OUT_PATH.open("w", encoding="utf-8") as f:
        f.write(f"# DUMP {TS}\nRepo: {REPO_ROOT}\n")
        dump_tree(REPO_ROOT, f); dump_files(REPO_ROOT, f)
    prune_old_dumps(); print(f"[✓] Dump écrit: {OUT_PATH}"); return 0

if __name__ == "__main__": sys.exit(main())--- [25/191] ./tools/push_dashboard.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Push automatique du dashboard HTML généré vers GitHub Pages (/docs/).
Utilise les variables d'environnement définies dans Paperspace.
"""

import os
import subprocess
import sys
from pathlib import Path

def run(cmd, cwd=None):
    print(f"[GIT] {cmd}")
    result = subprocess.run(cmd, cwd=cwd, shell=True,
                            stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
    print(result.stdout)
    if result.returncode != 0:
        sys.exit(result.returncode)

def main():
    # Récupère les variables d’env
    git_user   = os.environ.get("GIT_USER")
    git_token  = [REDACTED]"GIT_TOKEN=[REDACTED]
    git_repo   = os.environ.get("GIT_REPO", "Scalp")
    git_branch = os.environ.get("GIT_BRANCH", "main")
    repo_path  = os.environ.get("REPO_PATH", "/notebooks/scalp")

    if not all([git_user, git_token, git_repo]):
        print("[ERROR] Variables GIT_* manquantes")
        sys.exit(1)

    # Fichier dashboard généré
    docs_dir = Path(repo_path) / "docs"
    docs_dir.mkdir(parents=True, exist_ok=True)
    dashboard = docs_dir / "dashboard.html"

    if not dashboard.exists():
        print(f"[ERROR] Fichier introuvable: {dashboard}")
        sys.exit(1)

    # Remote avec token (authentification automatique)
    remote_url = f"https://{git_user}:{git_token}@github.com/{git_user}/{git_repo}.git"

    # Git push
    run("git config user.name 'AutoBot'", cwd=repo_path)
    run("git config user.email 'bot@scalp.local'", cwd=repo_path)
    run("git add docs/dashboard.html", cwd=repo_path)
    run("git commit -m 'Auto-update dashboard [skip ci]' || echo 'No changes'", cwd=repo_path)
    run(f"git push {remote_url} {git_branch}", cwd=repo_path)

    print("[OK] Dashboard poussé sur GitHub")

if __name__ == "__main__":
    main()--- [26/191] ./tools/publish-pages.py ---
# annulé--- [27/191] ./tools/start_ngrok.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Démarre un tunnel ngrok HTTP (port HTML, défaut 8888), écrit ngrok_url.txt.
- Auto-installe pyngrok si besoin
- Si NGROK_AUTHTOKEN=[REDACTED] est posé, le configure automatiquement
"""

import os, sys, subprocess

PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
NGROK_FILE   = os.path.join(PROJECT_ROOT, "ngrok_url.txt")

def _ensure_pyngrok():
    try:
        import pyngrok  # noqa
    except Exception:
        print("[ngrok] installation de pyngrok…")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "pyngrok"])

def [REDACTED]():
    token = [REDACTED]"NGROK_AUTHTOKEN=[REDACTED] "").strip()
    if not token:
        return
    try:
        subprocess.check_call(["ngrok", "config", "add-authtoken", token])
        print("[ngrok] authtoken configuré via NGROK_AUTHTOKEN=[REDACTED]
    except Exception:
        # fallback pyngrok
        from pyngrok import conf
        c = conf.get_default()
        c.auth_token = [REDACTED]
        conf.set_default(c)
        print("[ngrok] authtoken configuré via pyngrok")

def main():
    _ensure_pyngrok()
    [REDACTED]()
    from pyngrok import ngrok

    port = int(os.environ.get("HTML_PORT", "8888"))

    try: ngrok.kill()
    except Exception: pass

    public = ngrok.connect(port, "http")
    url = public.public_url.rstrip("/")
    print(f"[ngrok] Tunnel actif → {url}/dashboard.html")

    with open(NGROK_FILE, "w", encoding="utf-8") as f:
        f.write(url + "\n")

if __name__ == "__main__":
    main()--- [28/191] ./tools/__init__.py ---
# empty file, needed so "tools" is a package--- [29/191] ./tools/setup_dashboard.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Setup & démarrage du dashboard Streamlit (fixe le cas 'Cannot uninstall blinker').
- Force l'install de blinker via --ignore-installed (sans désinstaller la version distutils)
- Ajoute --break-system-packages si dispo (Debian/Ubuntu récents)
- Installe streamlit/plotly/pyarrow/altair/pydeck
- Vérifie les imports
- Lance Streamlit en arrière-plan
- Écrit l'URL dans dash/dashboard_url.txt
"""

import os, sys, subprocess, time, socket, re

PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
LOGS_DIR     = "/notebooks/scalp_data/logs"
APP_PATH     = os.path.join(PROJECT_ROOT, "dash", "app_streamlit.py")
PID_FILE     = os.path.join(LOGS_DIR, "streamlit.pid")
URL_FILE     = os.path.join(PROJECT_ROOT, "dash", "dashboard_url.txt")

def _run(cmd):
    print("[CMD]", " ".join(cmd))
    return subprocess.call(cmd)

def _pip_flags():
    flags = []
    # Certaines images requièrent ce flag pour écrire dans site-packages système
    # (si pip le supporte, sinon pip l'ignore)
    flags.append("--break-system-packages")
    return flags

def ensure_logs_dir():
    os.makedirs(LOGS_DIR, exist_ok=True)
    os.makedirs(os.path.dirname(URL_FILE), exist_ok=True)

def step_install():
    # 0) upgrade pip (silencieux si déjà à jour)
    _run([sys.executable, "-m", "pip", "install", "--upgrade", "pip"])

    # 1) forcer blinker récent sans désinstaller l'ancien (cause du bug)
    _run([sys.executable, "-m", "pip", "install", "blinker>=1.7", "--ignore-installed"] + _pip_flags())

    # 2) installer le reste (on tolère les déjà installés)
    base = [sys.executable, "-m", "pip", "install"] + _pip_flags()
    _run(base + ["streamlit", "plotly", "pyarrow", "altair", "pydeck"])

    # 3) vérif imports
    try:
        import streamlit, plotly, pyarrow, altair, pydeck  # noqa
        print("[OK] Imports: streamlit/plotly/pyarrow/altair/pydeck OK")
        return True
    except Exception as e:
        print("[ERR] Imports KO:", e)
        return False

def guess_public_url():
    # Permettre override par variable d'env si besoin
    env_override = os.environ.get("[REDACTED]")
    if env_override:
        return env_override.rstrip("/")
    # Sinon on déduit du hostname
    host = os.environ.get("[REDACTED]") or os.environ.get("HOSTNAME") or socket.gethostname() or "localhost"
    host = re.sub(r"[^a-zA-Z0-9\-]", "-", host).strip("-").lower() or "localhost"
    return f"https://{host}.paperspacegradient.com:8501"

def write_urls():
    with open(URL_FILE, "w", encoding="utf-8") as f:
        f.write("http://localhost:8501\n")
        f.write(guess_public_url() + "\n")
    print(f"[OK] URL écrite -> {URL_FILE}")

def start_streamlit():
    # Si déjà lancé, ne pas dupliquer
    if os.path.isfile(PID_FILE):
        try:
            with open(PID_FILE, "r") as f:
                pid = int(f.read().strip())
            os.kill(pid, 0)
            print(f"[INFO] Streamlit déjà actif (PID {pid}).")
            write_urls()
            return
        except Exception:
            try: os.remove(PID_FILE)
            except Exception: pass

    env = os.environ.copy()
    env["[REDACTED]"] = "false"
    env["SCALP_REPORTS_DIR"] = "/notebooks/scalp_data/reports"

    out_path = os.path.join(LOGS_DIR, "streamlit.out")
    err_path = os.path.join(LOGS_DIR, "streamlit.err")
    out = open(out_path, "a")
    err = open(err_path, "a")

    print("[START] Streamlit…")
    proc = subprocess.Popen(
        [sys.executable, "-m", "streamlit", "run", APP_PATH,
         "--server.headless", "true",
         "--server.address", "0.0.0.0",
         "--server.port", "8501"],
        stdout=out, stderr=err, env=env, preexec_fn=os.setsid
    )
    with open(PID_FILE, "w") as f:
        f.write(str(proc.pid))
    time.sleep(5)

    if proc.poll() is None:
        print(f"[OK] Streamlit lancé (PID {proc.pid}). Logs: {out_path}, {err_path}")
        write_urls()
    else:
        print("[ERR] Streamlit a quitté immédiatement. Voir logs:", err_path)
        # Affiche la fin du log
        try:
            tail = open(err_path, "r", encoding="utf-8").read()[-1500:]
            print("----- streamlit.err (tail) -----\n" + tail)
        except Exception:
            pass

def main():
    ensure_logs_dir()
    ok = step_install()
    if not ok:
        print("[STOP] Installation incomplète — corrige puis relance.")
        return
    start_streamlit()

if __name__ == "__main__":
    main()--- [30/191] ./dashboard_url.txt ---
https://console.paperspace.com/nbooks/<NOTEBOOK_ID>/files/dashboard.html
https://console.paperspace.com/nbooks/<NOTEBOOK_ID>?file=%2Fdashboard.html
--- [31/191] ./cli.py ---
# cli.py  (à la racine du repo)
from __future__ import annotations
import argparse

def parse_cli() -> argparse.Namespace:
    ap = argparse.ArgumentParser(description="SCALP — launcher")
    ap.add_argument("--once", action="store_true", help="Exécuter une seule passe orchestrateur")
    ap.add_argument("--log-level", default="INFO", help="DEBUG/INFO/WARN/ERROR (défaut: INFO)")
    return ap.parse_args()--- [32/191] ./jobs/walkforward.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations
import argparse, os, sys, json, time, logging
import yaml
import numpy as np
import pandas as pd

DEFAULT_CONFIG = "engine/config/config.yaml"

def load_yaml(p):
    with open(p, "r", encoding="utf-8") as f: return yaml.safe_load(f) or {}

def setup_logger(dir_):
    os.makedirs(dir_, exist_ok=True)
    log = logging.getLogger("walkforward"); log.setLevel(logging.INFO)
    log.handlers.clear()
    fh = logging.FileHandler(os.path.join(dir_, "walkforward.log"))
    sh = logging.StreamHandler(sys.stdout)
    fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
    fh.setFormatter(fmt); sh.setFormatter(fmt)
    log.addHandler(fh); log.addHandler(sh)
    return log

def load_csv(data_dir: str, pair: str, tf: str, limit: int | None):
    p = os.path.join(data_dir, "ohlcv", pair, f"{tf}.csv")
    df = pd.read_csv(p)
    ts0 = df["timestamp"].iloc[0]
    if isinstance(ts0, (int, np.integer)) and ts0 < 2e10:
        df["timestamp"] = df["timestamp"].astype(np.int64) * 1000
    return df.sort_values("timestamp").tail(limit or len(df)).reset_index(drop=True)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", default=DEFAULT_CONFIG)
    ap.add_argument("--pair", required=True)
    ap.add_argument("--entry-tf", default="1m")
    ap.add_argument("--schema-backtest", required=True)
    ap.add_argument("--schema-entries", required=True)
    ap.add_argument("--limit", type=int, default=5000)
    ap.add_argument("--optuna", action="store_true")
    ap.add_argument("--trials", type=int, default=30)
    args = ap.parse_args()

    cfg = load_yaml(args.config)
    rt = cfg.get("runtime", {})
    data_dir = rt.get("data_dir"); reports_dir = rt.get("reports_dir")
    logs_dir = os.path.join(os.path.dirname(data_dir), "logs")
    log = setup_logger(logs_dir)

    from engine.strategies.twolayer_scalp import (
        [REDACTED], [REDACTED],
        build_entries_frame, run_backtest_exec
    )
    from engine.utils.walkforward import make_segments
    sb, se = [REDACTED](None, args.schema_backtest, args.schema_entries)
    dir_tfs = sb["timeframes"]["direction"]

    df_entry = load_csv(data_dir, args.pair, args.entry_tf, args.limit)
    idx = pd.to_datetime(df_entry["timestamp"], unit="ms", utc=True)
    df_entry.index = idx

    df_by_tf = {}
    for rtf in dir_tfs:
        df_r = load_csv(data_dir, args.pair, rtf, args.limit*5)
        df_r.index = pd.to_datetime(df_r["timestamp"], unit="ms", utc=True)
        df_by_tf[rtf] = df_r

    wf = sb.get("backtest", {}).get("walk_forward", {"train_days": 90, "test_days": 30, "segments": 6})
    segs = make_segments(df_entry.index, wf["train_days"], wf["test_days"], wf["segments"])
    log.info(f"{len(segs)} segments WF")

    def eval_once(s_back, s_entries) -> dict:
        p_buy = [REDACTED](df_by_tf, s_back)
        p_buy = p_buy.reindex(df_entry.index, method="ffill")
        df_exec = pd.concat([df_entry.copy(), build_entries_frame(df_entry.copy(), s_entries)], axis=1)
        return run_backtest_exec(df_exec, p_buy, s_back, s_entries)

    # Baseline sans tuning (évalue sur chaque segment test)
    res = []
    for sg in segs:
        test = df_entry.loc[sg.test_start:sg.test_end]
        if test.empty: continue
        r = eval_once(sb, se)
        res.append(r)
    agg = {
        "pf_mean": float(np.mean([r["pf"] for r in res] or [0])),
        "mdd_max": float(np.max([r["mdd"] for r in res] or [0])),
        "wr_mean": float(np.mean([r["wr"] for r in res] or [0])),
        "sharpe_mean": float(np.mean([r["sharpe"] for r in res] or [0])),
        "segments": len(res),
    }

    # Optuna (optionnel)
    best = {}
    if args.optuna:
        try:
            import optuna
            def objective(trial):
                sback = json.loads(json.dumps(sb))
                sback["regime_layer"]["indicators"]["ema"]["fast"]["5m"] = trial.suggest_int("ema_fast_5m", 7, 14)
                sback["regime_layer"]["indicators"]["ema"]["slow"]["5m"] = trial.suggest_int("ema_slow_5m", 21, 34)
                sback["regime_layer"]["softmax_temperature"] = trial.suggest_float("tau", 0.25, 0.5)
                r = eval_once(sback, se)
                return r["pf"] - 0.5*r["mdd"]
            study = optuna.create_study(direction="maximize")
            study.optimize(objective, n_trials=args.trials)
            best = study.best_params
        except Exception as e:
            log.warning(f"Optuna indisponible/erreur: {e}")

    out = {"pair": args.pair, "entry_tf": args.entry_tf, "wf": agg, "optuna_best": best}
    os.makedirs(reports_dir, exist_ok=True)
    with open(os.path.join(reports_dir, f"walkforward_{args.pair}_{args.entry_tf}.json"), "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)
    log.info("WF résumé écrit.")

if __name__ == "__main__":
    main()--- [33/191] ./jobs/backfill.py ---
from __future__ import annotations
import argparse, os
from pathlib import Path
from engine.config.loader import load_config
from engine.exchange.bitget_rest import BitgetFuturesClient
from engine.backtest.loader_csv import write_csv_ohlcv

def main() -> int:
    ap = argparse.ArgumentParser(description="Backfill OHLCV -> DATA_ROOT/data")
    ap.add_argument("--symbols", required=True, help="BTCUSDT,ETHUSDT")
    ap.add_argument("--tfs", required=True, help="1m,5m,15m,1h")
    ap.add_argument("--limit", type=int, default=5000)
    args = ap.parse_args()

    cfg = load_config()
    out_dir = Path(cfg["runtime"]["data_dir"]).resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    client = BitgetFuturesClient(
        access_key=[REDACTED]"secrets"]["bitget"]["access"],
        secret_key=[REDACTED]"secrets"]["bitget"]["secret"],
        passphrase=[REDACTED]"secrets"]["bitget"]["passphrase"],
        base_url=os.getenv("BITGET_BASE_URL","https://api.bitget.com"),
    )

    symbols = [s.strip() for s in args.symbols.split(",") if s.strip()]
    tfs = [t.strip() for t in args.tfs.split(",") if t.strip()]
    for sym in symbols:
        for tf in tfs:
            resp = client.get_klines(sym, interval=tf, limit=args.limit)
            rows = resp.get("data") or []
            write_csv_ohlcv(out_dir, sym, tf, rows)
            print(f"[✓] {sym} {tf} -> {out_dir}")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())--- [34/191] ./jobs/refresh_pairs.py ---
#!/usr/bin/env python3
# jobs/refresh_pairs.py
from __future__ import annotations

import argparse
import csv
import json
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List

from engine.config.loader import load_config

# Exchange: CCXT si dispo sinon REST Bitget
from engine.exchange.bitget_rest import BitgetFuturesClient as BitgetRESTClient
try:
    from engine.exchange.bitget_ccxt import CCXTFuturesClient as BitgetCCXTClient  # type: ignore
    _HAS_CCXT = True
except Exception:
    _HAS_CCXT = False


# ---------- utils FS ----------

def _ensure_dir(p: Path) -> Path:
    p.mkdir(parents=True, exist_ok=True)
    return p

def _ohlcv_path(data_dir: str, symbol: str, tf: str) -> Path:
    return _ensure_dir(Path(data_dir) / "ohlcv" / symbol) / f"{tf}.csv"

def _write_csv_ohlcv(path: Path, rows: Iterable[List[float]]) -> None:
    write_header = not path.exists()
    with path.open("a", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        if write_header:
            w.writerow(["ts", "open", "high", "low", "close", "volume"])
        for r in rows:
            try:
                ts = int(r[0])
                o, h, l, c = float(r[1]), float(r[2]), float(r[3]), float(r[4])
                v = float(r[5]) if len(r) > 5 else 0.0
                w.writerow([ts, o, h, l, c, v])
            except Exception:
                continue


# ---------- exchange helpers ----------

def _fetch_ohlcv_any(ex, symbol: str, timeframe: str, limit: int = 1000) -> List[List[float]]:
    # 1) CCXT sync si dispo
    fetch = getattr(ex, "fetch_ohlcv", None)
    if callable(fetch):
        try:
            data = fetch(symbol, timeframe, limit=limit)
            return list(data or [])
        except Exception:
            pass
    # 2) REST Bitget (wrapper)
    get_klines = getattr(ex, "get_klines", None)
    if callable(get_klines):
        try:
            resp = get_klines(symbol, interval=timeframe, limit=int(limit))
            rows = resp.get("data") or []
            out: List[List[float]] = []
            for r in rows:
                try:
                    out.append([
                        int(r[0]), float(r[1]), float(r[2]), float(r[3]),
                        float(r[4]), float(r[5]) if len(r) > 5 else 0.0
                    ])
                except Exception:
                    continue
            out.sort(key=[REDACTED] x: x[0])
            return out
        except Exception:
            pass
    return []

def [REDACTED](ex) -> List[str]:
    # CCXT: via load_markets()
    load_markets = getattr(ex, "load_markets", None)
    if callable(load_markets):
        try:
            markets = load_markets() or {}
            syms: List[str] = []
            for m in markets.values():
                sym = str(m.get("symbol") or "")
                typ = str(m.get("type") or "")
                if "USDT" in sym and typ in {"swap", "future", "perpetual"}:
                    syms.append(sym.replace("_", "").upper())
            if syms:
                return sorted(set(syms))
        except Exception:
            pass
    # REST wrapper: méthode custom facultative
    list_symbols = getattr(ex, "list_symbols", None)
    if callable(list_symbols):
        try:
            items = list_symbols() or []
            out = [str(s).replace("_", "").upper() for s in items if "USDT" in str(s).upper()]
            if out:
                return sorted(set(out))
        except Exception:
            pass
    # Fallback statique
    return ["BTCUSDT","ETHUSDT","SOLUSDT","BNBUSDT","XRPUSDT",
            "ADAUSDT","DOGEUSDT","LTCUSDT","MATICUSDT","LINKUSDT"]

def _build_exchange():
    if _HAS_CCXT:
        try:
            return BitgetCCXTClient(paper=True)  # si ton wrapper supporte paper
        except Exception:
            pass
    return BitgetRESTClient()


# ---------- scoring ----------

@dataclass
class PairScore:
    symbol: str
    vol_usd_24h: float
    atr_pct_24h: float
    score: float

def _atr_pct_estimate(ohlcv: List[List[float]]) -> float:
    if len(ohlcv) < 50:
        return 0.0
    rng = [(r[2] - r[3]) for r in ohlcv[-200:]]
    atr = sum(abs(x) for x in rng) / max(1, len(rng))
    close = float(ohlcv[-1][4])
    return (atr / close) if close > 0 else 0.0

def _score_pairs(ex, symbols: List[str], timeframe: str, limit: int) -> List[PairScore]:
    out: List[PairScore] = []
    for s in symbols:
        ohlcv = _fetch_ohlcv_any(ex, s, timeframe, limit=min(1000, limit))
        if not ohlcv:
            continue
        vol_usd = 0.0
        for r in ohlcv[-500:]:
            try:
                vol_usd += float(r[4]) * float(r[5])
            except Exception:
                pass
        atr_pct = _atr_pct_estimate(ohlcv)
        score = vol_usd * (1.0 + 10.0 * atr_pct)
        out.append(PairScore(symbol=s, vol_usd_24h=vol_usd, atr_pct_24h=atr_pct, score=score))
        time.sleep(0.03)  # anti rate‑limit léger
    out.sort(key=[REDACTED] x: x.score, reverse=True)
    return out


# ---------- watchlist IO ----------

def _save_watchlist(path: Path, scores: List[PairScore], top: int) -> None:
    selected = scores[:top] if top > 0 else scores
    doc = {
        "generated_at": int(time.time() * 1000),
        "top": [
            {
                "symbol": s.symbol,
                "vol_usd_24h": s.vol_usd_24h,
                "atr_pct_24h": s.atr_pct_24h,
                "score": s.score,
            }
            for s in selected
        ],
    }
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(doc, indent=2), encoding="utf-8")


# ---------- main ----------

def main(argv=None) -> int:
    ap = argparse.ArgumentParser(description="Rafraîchit watchlist + backfill OHLCV")
    ap.add_argument("--timeframe", type=str, default="5m", help="TF utilisé pour scorer la watchlist")
    ap.add_argument("--top", type=int, default=10, help="Garder les N meilleures paires (0=toutes)")
    ap.add_argument("--backfill-tfs", type=str, default="1m,5m,15m", help="TFs à backfiller")
    ap.add_argument("--limit", type=int, default=1500, help="Nombre de bougies à récupérer par TF")
    ns = ap.parse_args(argv)

    cfg = load_config()
    rt = cfg.get("runtime", {})
    data_dir = str(rt.get("data_dir") or "/notebooks/scalp_data/data")
    reports_dir = str(rt.get("reports_dir") or "/notebooks/scalp_data/reports")

    ex = _build_exchange()

    # 1) universe
    symbols = [REDACTED](ex)
    if not symbols:
        print("[refresh] aucun symbole listé — abandon.")
        return 2

    # 2) scoring
    scores = _score_pairs(ex, symbols, ns.timeframe, ns.limit)
    if not scores:
        print("[refresh] aucun score — abandon.")
        return 3

    # 3) write watchlist
    _save_watchlist(Path(reports_dir) / "watchlist.yml", scores, ns.top)
    selected = [s.symbol for s in (scores[:ns.top] if ns.top > 0 else scores)]
    print(f"[refresh] watchlist top={ns.top}: {', '.join(selected)}")

    # 4) backfill
    tfs = [t.strip() for t in ns.backfill_tfs.split(",") if t.strip()]
    for tf in tfs:
        for sym in selected:
            ohlcv = _fetch_ohlcv_any(ex, sym, tf, limit=ns.limit)
            if not ohlcv:
                print(f"[refresh] pas de données pour {sym}:{tf}")
                continue
            _write_csv_ohlcv(_ohlcv_path(data_dir, sym, tf), ohlcv)
            time.sleep(0.02)  # anti rate‑limit

    print("[refresh] terminé.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())--- [35/191] ./jobs/promote.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations
import argparse, os, sys, time, json, yaml, subprocess
from copy import deepcopy
from typing import Dict

PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.append(PROJECT_ROOT)
from engine.utils.io_safe import atomic_write_text, backup_last_good
from engine.utils.logging_setup import setup_logger

DEFAULT_CONFIG = os.path.join(PROJECT_ROOT, "engine", "config", "config.yaml")
DEFAULT_DEST   = os.path.join(PROJECT_ROOT, "engine", "config", "strategies.yml")

POLICY = {
    "conservative": {"pf": 1.4, "mdd": 0.15, "trades": 35},
    "normal":       {"pf": 1.3, "mdd": 0.20, "trades": 30},
    "aggressive":   {"pf": 1.2, "mdd": 0.30, "trades": 25},
}

def load_yaml(path, missing_ok=False):
    if missing_ok and not os.path.isfile(path): return {}
    with open(path, "r", encoding="utf-8") as f: return yaml.safe_load(f) or {}

def dump_yaml(obj) -> str:
    return yaml.safe_dump(obj, sort_keys=True, allow_unicode=True, default_flow_style=False)

def tf_minutes(tf: str) -> int:
    if tf.endswith("m"): return int(tf[:-1])
    if tf.endswith("h"): return int(tf[:-1]) * 60
    if tf.endswith("d"): return int(tf[:-1]) * 1440
    raise ValueError(f"TF non supporté: {tf}")

def lifetime_minutes(tf: str, k: int) -> int:
    return k * tf_minutes(tf)

def _score_row(r: Dict) -> float:
    pf=float(r.get("pf",0)); mdd=float(r.get("mdd",1))
    sh=float(r.get("sharpe",0)); wr=float(r.get("wr",0))
    return pf*2.0 + sh*0.5 + wr*0.5 - mdd*1.5

def _pass(r, pol):
    return (r.get("pf",0)>=pol["pf"]) and (r.get("mdd",1)<=pol["mdd"]) and (r.get("trades",0)>=pol["trades"])

def print_top(reports_dir: str, risk_mode: str, k:int=12, logger=None):
    path = os.path.join(reports_dir, "summary.json")
    try:
        sm = json.load(open(path, "r", encoding="utf-8"))
    except Exception:
        if logger: logger.info("summary.json introuvable"); return
        print("[TOP] summary.json introuvable"); return
    rows = sm.get("rows", [])
    if not rows:
        if logger: logger.info("Aucun résultat en base."); return
        print("[TOP] Aucun résultat en base."); return
    pol = POLICY.get(risk_mode, POLICY["normal"])
    rows.sort(key=[REDACTED] reverse=True)
    passed = sum(1 for r in rows if _pass(r,pol))
    if logger:
        logger.info(json.dumps({"event":"top_summary","risk":risk_mode,"total":len(rows),"pass":passed}, ensure_ascii=False))
    else:
        print(f"[TOP] PASS=[REDACTED] (policy={risk_mode})")

def _call_render_guard(project_root: str):
    env = os.environ.copy()
    script = os.path.join(project_root, "tools", "render_guard.py")
    if not os.path.isfile(script):
        print("[render] tools/render_guard.py introuvable (skip).")
        return
    try:
        subprocess.check_call([sys.executable, script], env=env, cwd=project_root)
    except subprocess.CalledProcessError as e:
        print(f"[render] render_guard a échoué (code {e.returncode}).")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", default=DEFAULT_CONFIG)
    ap.add_argument("--source", default=None)
    ap.add_argument("--dest", default=DEFAULT_DEST)
    ap.add_argument("--top-k", type=int, default=12)
    args = ap.parse_args()

    cfg = load_yaml(args.config, missing_ok=True)
    rt = cfg.get("runtime", {}) if isinstance(cfg, dict) else {}
    risk_mode = rt.get("risk_mode", "normal")
    age_mult  = int(rt.get("age_mult", 5))
    data_dir  = rt.get("data_dir", "/notebooks/scalp_data/data")
    reports_dir = rt.get("reports_dir", "/notebooks/scalp_data/reports")
    logs_dir = os.path.join(os.path.dirname(data_dir), "logs")
    os.makedirs(logs_dir, exist_ok=True)
    logger = setup_logger("promote", os.path.join(logs_dir, "promote.log"))

    source = args.source or os.path.join(reports_dir, "strategies.yml.next")
    nxt = load_yaml(source, missing_ok=True); cand = nxt.get("strategies", {})
    dest_obj = load_yaml(args.dest, missing_ok=True); cur = dest_obj.get("strategies", {})

    if not cand:
        logger.info(json.dumps({"event":"promote","status":"no_candidates","source":source}, ensure_ascii=False))
        print_top(reports_dir, risk_mode, k=args.top_k, logger=logger)
        _call_render_guard(PROJECT_ROOT)
        return

    now = int(time.time())
    changes = []
    for key, strat in list(cur.items()):
        try: _, tf = key.split(":")
        except ValueError: continue
        created = int(strat.get("created_at") or now)
        exp = strat.get("expires_at") or (created + (lifetime_minutes(tf, age_mult)*60))
        if now >= exp and not strat.get("expired", False):
            strat["expired"] = True; strat["expires_at"] = exp; changes.append({"EXPIRE": key})

    pol = POLICY.get(risk_mode, POLICY["normal"])
    filt = {
        k: v for k, v in cand.items()
        if v.get("metrics", {}).get("pf", 0) >= pol["pf"]
        and v.get("metrics", {}).get("mdd", 1) <= pol["mdd"]
        and v.get("metrics", {}).get("trades", 0) >= pol["trades"]
    }

    if not filt:
        dest_obj["strategies"] = cur
        backup_last_good(args.dest)
        atomic_write_text(dump_yaml(dest_obj), args.dest)
        logger.info(json.dumps({"event":"promote","status":"[REDACTED]"}, ensure_ascii=False))
        print_top(reports_dir, risk_mode, k=args.top_k, logger=logger)
        _call_render_guard(PROJECT_ROOT)
        return

    for key, s in filt.items():
        try: _, tf = key.split(":")
        except ValueError:
            logger.info(json.dumps({"event":"bad_key","key":key}, ensure_ascii=False))
            continue
        created = int(s.get("created_at") or now)
        s["expires_at"] = created + (lifetime_minutes(tf, age_mult)*60)
        s["expired"] = False

        old = cur.get(key)
        if old is None:
            cur[key] = deepcopy(s); changes.append({"ADD": key, "pf": s.get("metrics",{}).get("pf")})
        else:
            newer = int(s.get("created_at") or 0) > int(old.get("created_at") or 0)
            better = (
                s.get("metrics", {}).get("pf", 0) > old.get("metrics", {}).get("pf", 0)
                or (
                    s.get("metrics", {}).get("pf", 0) == old.get("metrics", {}).get("pf", 0)
                    and s.get("metrics", {}).get("mdd", 1) < old.get("metrics", {}).get("mdd", 1)
                )
                or s.get("metrics", {}).get("sharpe", 0) > old.get("metrics", {}).get("sharpe", 0)
            )
            if (newer and better) or (old.get("expired", False) and better):
                cur[key] = deepcopy(s); changes.append({"REPLACE": key})

    dest_obj["strategies"] = cur
    backup_last_good(args.dest)
    atomic_write_text(dump_yaml(dest_obj), args.dest)

    logger.info(json.dumps({"event":"promote","status":"done","changes":changes}, ensure_ascii=False))
    print_top(reports_dir, risk_mode, k=args.top_k, logger=logger)
    _call_render_guard(PROJECT_ROOT)

if __name__ == "__main__":
    main()--- [36/191] ./jobs/seeds_strategies.py ---
#!/usr/bin/env python3
# jobs/seed_strategies.py
"""
Génère/actualise engine/config/strategies.yml à partir de la watchlist.

- Crée pour chaque symbole de la watchlist une stratégie EXPERIMENTAL
  (execute=false => observe-only) sur les TF demandés.
- TTL basé sur un nombre de barres (ttl_policy_bars) par niveau de risque.
- Merge non destructif par défaut (garde les entrées déjà présentes, sauf --overwrite).

Exemples:
  # seed pour 1m seulement, observe-only
  python jobs/seed_strategies.py --tfs 1m

  # seed 1m,5m pour les 15 1ers symboles de la watchlist
  python jobs/seed_strategies.py --tfs 1m,5m --top 15

  # forcer overwrite complet du fichier (remplace tout)
  python jobs/seed_strategies.py --tfs 1m,5m --overwrite
"""
from __future__ import annotations

import argparse
import json
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List

from engine.config.loader import load_config

# --- bootstrap chemin + sitecustomize ---
from pathlib import Path
import sys

ROOT = Path(__file__).resolve().parents[1]   # racine du repo
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))
try:
    import sitecustomize  # charge .env + deps + paths (si présent)
except Exception:
    pass
# --- fin bootstrap ---

# ---------- chemins ----------
def _paths() -> Dict[str, Path]:
    cfg = load_config()
    rt = cfg.get("runtime", {})
    reports_dir = Path(rt.get("reports_dir") or "/notebooks/scalp_data/reports")
    watchlist = reports_dir / "watchlist.yml"  # JSON lisible
    strat_yml = Path(__file__).resolve().parents[1] / "engine" / "config" / "strategies.yml"
    return {"watchlist": watchlist, "strategies": strat_yml}

# ---------- IO ----------
def _read_json(path: Path) -> Dict:
    if not path.exists():
        return {}
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}

def _write_json(path: Path, doc: Dict) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(doc, indent=2), encoding="utf-8")

# ---------- core ----------
def _now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()

def _normalize_symbol(s: str) -> str:
    return (s or "").replace("_", "").upper()

def seed_from_watchlist(tfs: List[str], top: int | None, overwrite: bool, ttl_bars_exp:int) -> int:
    P = _paths()
    wl = _read_json(P["watchlist"])
    if not wl:
        print(f"[seed] Watchlist introuvable ou vide: {P['watchlist']}")
        return 2

    symbols = [_normalize_symbol(d.get("symbol","")) for d in (wl.get("top") or []) if d.get("symbol")]
    if top:
        symbols = symbols[:top]
    if not symbols:
        print("[seed] Aucun symbole valide dans la watchlist.")
        return 3

    # doc courant (merge) ou nouveau
    if overwrite or not P["strategies"].exists():
        doc = {
            "meta": {
                "generated_at": _now_iso(),
                "note": "EXPERIMENTAL: observe-only (execute=false). TTL en nb de barres.",
                "ttl_policy_bars": {
                    "DEFAULT": 300, "LOW": 1000, "MEDIUM": 500, "HIGH": 250, "EXPERIMENTAL": ttl_bars_exp
                },
                "[REDACTED]": 1.0
            },
            "strategies": {}
        }
    else:
        doc = _read_json(P["strategies"]) or {"meta": {}, "strategies": {}}
        doc.setdefault("meta", {}).setdefault("ttl_policy_bars", {"DEFAULT": 300, "EXPERIMENTAL": ttl_bars_exp})
        doc["meta"]["generated_at"] = _now_iso()
        doc["meta"].setdefault("[REDACTED]", 1.0)

    strategies = doc.setdefault("strategies", {})

    created = 0
    for sym in symbols:
        for tf in tfs:
            key = [REDACTED]"{sym}:{tf}"
            # on n’écrase pas les entrées existantes sauf --overwrite
            if not overwrite and key in strategies:
                continue
            strategies[key] = {
                "name": "BASE_UNTESTED",
                "risk_label": "EXPERIMENTAL",
                "execute": False,                 # observe-only
                "ema_fast": 9,
                "ema_slow": 21,
                "atr_period": 14,
                "trail_atr_mult": 1.5,
                "risk_pct_equity": 0.005,
                "last_validated": _now_iso(),     # marquage initial
                # facultatif : forcer un TTL spécifique pour EXPERIMENTAL
                "ttl_bars": ttl_bars_exp
            }
            created += 1

    _write_json(P["strategies"], doc)
    print(f"[seed] OK • {created} entrées écrites • fichier: {P['strategies']}")
    return 0

# ---------- main ----------
def main(argv=None) -> int:
    ap = argparse.ArgumentParser(description="Seed strategies.yml depuis la watchlist")
    ap.add_argument("--tfs", type=str, default="1m", help="Liste de TF séparés par des virgules (ex: 1m,5m,15m)")
    ap.add_argument("--top", type=int, default=0, help="Limiter aux N premiers de la watchlist (0=illimité)")
    ap.add_argument("--overwrite", action="store_true", help="Remplace entièrement les entrées existantes")
    ap.add_argument("--ttl-bars-exp", type=int, default=120, help="TTL (nb de barres) pour EXPERIMENTAL")
    ns = ap.parse_args(argv)

    tfs = [t.strip() for t in ns.tfs.split(",") if t.strip()]
    top = int(ns.top) if ns.top and ns.top > 0 else None
    return seed_from_watchlist(tfs=tfs, top=top, overwrite=ns.overwrite, ttl_bars_exp=int(ns.ttl_bars_exp))

if __name__ == "__main__":
    raise SystemExit(main())--- [37/191] ./jobs/boot_live.py ---
#!/usr/bin/env python3
# jobs/boot_live.py
from __future__ import annotations

import argparse
import subprocess
import sys
from pathlib import Path

from engine.config.loader import load_config

ROOT = Path(__file__).resolve().parents[1]

def run(cmd: list[str]) -> int:
    print("+", " ".join(cmd))
    return subprocess.run(cmd, cwd=str(ROOT)).returncode

def main(argv=None) -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--top", type=int, default=None)
    ap.add_argument("--timeframe", type=str, default=None)
    ap.add_argument("--backfill-tfs", type=str, default=None)
    ap.add_argument("--limit", type=int, default=None)
    ap.add_argument("--seed-tfs", type=str, default=None)
    ap.add_argument("--ttl-bars-exp", type=int, default=None)
    ap.add_argument("--no-seed", action="store_true")
    ap.add_argument("--just-run", action="store_true")
    ns = ap.parse_args(argv)

    cfg = load_config()
    wl = cfg.get("watchlist", {})
    mt = cfg.get("maintainer", {})

    top = ns.top or int(wl.get("top", 10))
    timeframe = ns.timeframe or str(wl.get("score_tf", "5m"))
    backfill_tfs = ns.backfill_tfs or ",".join(wl.get("backfill_tfs", ["1m","5m","15m"]))
    limit = ns.limit or int(wl.get("backfill_limit", 1500))
    seed_tfs = ns.seed_tfs or ",".join(mt.get("seed_tfs", ["1m"]))
    ttl_bars_exp = ns.ttl_bars_exp or int(mt.get("[REDACTED]", 120))

    if not ns.just_run:
        rc = run([sys.executable, "-m", "jobs.refresh_pairs",
                  "--timeframe", timeframe, "--top", str(top),
                  "--backfill-tfs", backfill_tfs, "--limit", str(limit)])
        if rc != 0:
            print(f"[boot] refresh_pairs RC={rc} (continue)")

        if not ns.no_seed:
            rc = run([sys.executable, "-m", "jobs.seed_strategies",
                      "--tfs", seed_tfs, "--ttl-bars-exp", str(ttl_bars_exp)])
            if rc != 0:
                print(f"[boot] seed_strategies RC={rc} (continue)")

    return run([sys.executable, "bot.py"])

if __name__ == "__main__":
    raise SystemExit(main())--- [38/191] ./jobs/dash.py ---
#!/usr/bin/env python3
"""
jobs/dash.py — Lance le dashboard Streamlit du projet scalp
Usage:
    python jobs/dash.py
Options:
    --port 8501       Port HTTP (défaut 8501)
    --headless true   Mode headless (recommandé en remote)
"""

import argparse
import subprocess
import sys
from pathlib import Path

def main(argv=None):
    ap = argparse.ArgumentParser()
    ap.add_argument("--port", type=int, default=8501, help="Port HTTP (défaut 8501)")
    ap.add_argument("--headless", action="store_true", help="Force mode headless")
    args = ap.parse_args(argv)

    app_path = Path(__file__).resolve().parents[1] / "dash" / "app.py"
    if not app_path.exists():
        sys.exit(f"Dashboard introuvable: {app_path}")

    cmd = [
        sys.executable, "-m", "streamlit", "run", str(app_path),
        "--server.port", str(args.port),
    ]
    if args.headless:
        cmd += ["--server.headless", "true"]

    print(f"[i] Lancement du dashboard Streamlit sur port {args.port} ...")
    print(" ".join(cmd))
    subprocess.run(cmd)

if __name__ == "__main__":
    main()--- [39/191] ./jobs/termboard.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Termboard minimal :
- Respecte runtime.termboard_enabled (false par défaut) → ne rien afficher et sortir 0.
- Si true, affiche un résumé ultra-concis (1 ligne) basé sur summary.json s'il existe.
"""

from __future__ import annotations
import os, sys, json, time, yaml

DEFAULT_CONFIG = "engine/config/config.yaml"
REPORTS_FALLBACK = "/notebooks/scalp_data/reports/summary.json"

def load_yaml(path: str, missing_ok: bool = False):
    if missing_ok and not os.path.isfile(path): return {}
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}

def read_summary(path: str):
    if not os.path.isfile(path): return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None

def main():
    # config
    cfg_path = DEFAULT_CONFIG
    if len(sys.argv) >= 2 and sys.argv[1].endswith(".yaml"):
        cfg_path = sys.argv[1]
    cfg = load_yaml(cfg_path, missing_ok=True)
    rt = cfg.get("runtime", {}) if isinstance(cfg, dict) else {}
    enabled = bool(rt.get("termboard_enabled", False))  # défaut = False (silencieux)

    if not enabled:
        # mode silencieux → rien n'afficher, pas d’erreur
        return

    # sinon: mini résumé (1 ligne)
    reports_dir = rt.get("reports_dir", "/notebooks/scalp_data/reports")
    summary_path = os.path.join(reports_dir, "summary.json")
    if not os.path.isfile(summary_path):
        summary_path = REPORTS_FALLBACK

    sm = read_summary(summary_path) or {}
    rows = sm.get("rows", [])
    risk_mode = sm.get("risk_mode", "n/a")
    n = len(rows)
    if n == 0:
        print(f"[termboard] summary.json absent ou vide • risk={risk_mode}")
        return

    # Compter “PASS=[REDACTED] vs policy si présente dans summary
    pol = sm.get("policy") or {}
    def pass_policy(r):
        pf = r.get("pf", 0); mdd = r.get("mdd", 1); tr = r.get("trades", 0)
        return (
            pf >= pol.get("pf", 1.3)
            and mdd <= pol.get("mdd", 0.2)
            and tr >= pol.get("trades", 30)
        )
    passed = sum(1 for r in rows if pass_policy(r))
    print(f"[termboard] backtests={n} • pass={passed} • risk={risk_mode}")

if __name__ == "__main__":
    main()--- [40/191] ./jobs/visualise_results.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations
import os, json, argparse
from pathlib import Path

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--reports-dir", required=True, help="/notebooks/scalp_data/reports")
    ap.add_argument("--top", type=int, default=30)
    args = ap.parse_args()

    sm_path = Path(args.reports_dir) / "summary.json"
    if not sm_path.exists():
        print("summary.json absent — lance d'abord jobs/backtest.py"); return

    with open(sm_path, "r", encoding="utf-8") as f:
        sm = json.load(f)
    rows = sm.get("rows", [])
    rows.sort(key=[REDACTED] r: (r.get("pf",0), -r.get("mdd",1), r.get("sharpe",0)), reverse=True)

    try:
        from rich.table import Table
        from rich.console import Console
        from rich import box
        c = Console()
        t = Table(title="Backtest — meilleurs résultats", box=box.SIMPLE_HEAVY)
        for col in ("pair","tf","pf","mdd","trades","wr","sharpe","equity"):
            t.add_column(col.upper())
        for r in rows[:args.top]:
            t.add_row(
                r["pair"], r["tf"],
                f"{r['pf']:.2f}", f"{r['mdd']:.2%}",
                str(r["trades"]), f"{r['wr']:.2%}",
                f"{r['sharpe']:.2f}", f"{r.get('equity',1.0):.3f}"
            )
        c.print(t)
        c.print(f"[dim]Sélectionnés:[/dim] {', '.join(sm.get('selected', []))}")
    except Exception:
        print("PAIR\tTF\tPF\tMDD\tTRADES\tWR\tSHARPE\tEQ")
        for r in rows[:args.top]:
            print(f"{r['pair']}\t{r['tf']}\t{r['pf']:.2f}\t{r['mdd']:.2%}\t{r['trades']}\t{r['wr']:.2%}\t{r['sharpe']:.2f}\t{r.get('equity',1.0):.3f}")

if __name__ == "__main__":
    main()--- [41/191] ./jobs/viewer.py ---
#!/usr/bin/env python3
# jobs/viewer.py
from __future__ import annotations

import argparse
import json
import os
from pathlib import Path
from typing import Any, Dict, Iterable, List, Sequence

from engine.config.loader import load_config

# ---------- utils paths ----------

def _data_root_cfg() -> Dict[str, str]:
    cfg = load_config()
    rt = cfg.get("runtime", {})
    return {
        "data_dir": rt.get("data_dir") or "/notebooks/scalp_data/data",
        "reports_dir": rt.get("reports_dir") or "/notebooks/scalp_data/reports",
    }

def _live_dir() -> Path:
    return Path(_data_root_cfg()["data_dir"]) / "live"

def _orders_csv() -> Path:
    return _live_dir() / "orders.csv"

def _signals_csv() -> Path:
    return _live_dir() / "logs" / "signals.csv"

def _watchlist_yml() -> Path:
    return Path(_data_root_cfg()["reports_dir"]) / "watchlist.yml"

def _strategies_yml() -> Path:
    # stocké en JSON lisible (extension .yml)
    return Path(__file__).resolve().parents[1] / "engine" / "config" / "strategies.yml"

# ---------- pretty helpers ----------

def _print_table(rows: Sequence[Sequence[Any]], headers: Sequence[str] | None = None) -> None:
    if headers:
        rows = [headers, ["-"*len(h) for h in headers], *rows]
    widths = [max(len(str(r[i])) for r in rows) for i in range(len(rows[0]))] if rows else []
    for r in rows:
        line = " | ".join(str(v).ljust(widths[i]) for i, v in enumerate(r))
        print(line)

def _tail(path: Path, n: int = 20) -> List[str]:
    if not path.exists():
        return []
    try:
        # simple tail sans dépendances
        with path.open("r", encoding="utf-8", errors="ignore") as f:
            lines = f.readlines()[-n:]
        return [l.rstrip("\n") for l in lines]
    except Exception:
        return []

# ---------- viewers ----------

def cmd_watchlist(_: argparse.Namespace) -> int:
    p = _watchlist_yml()
    if not p.exists():
        print(f"(watchlist introuvable) {p}")
        return 1
    try:
        doc = json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        print(f"(format non lisible) {p}")
        return 1
    top = doc.get("top") or []
    rows = []
    for d in top:
        rows.append([d.get("symbol",""), f"{float(d.get('vol_usd_24h',0.0)):.0f}", f"{float(d.get('atr_pct_24h',0.0))*100:.2f}%", f"{float(d.get('score',0.0)):.3f}"])
    _print_table(rows, headers=["SYMBOL","VOL_USD_24H","ATR% (approx)","SCORE"])
    return 0

def cmd_strategies(_: argparse.Namespace) -> int:
    p = _strategies_yml()
    if not p.exists():
        print(f"(strategies.yml introuvable) {p}")
        return 1
    try:
        doc = json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        print(f"(format non lisible) {p}")
        return 1
    strat = doc.get("strategies") or {}
    rows = []
    for k, v in strat.items():
        rows.append([
            k,
            v.get("ema_fast",""),
            v.get("ema_slow",""),
            v.get("atr_period",""),
            v.get("trail_atr_mult",""),
            v.get("risk_pct_equity",""),
        ])
    rows.sort(key=[REDACTED] r: r[0])
    _print_table(rows, headers=["PAIR:TF","ema_fast","ema_slow","atr_period","trail_mult","risk_pct"])
    return 0

def cmd_tail_orders(ns: argparse.Namespace) -> int:
    p = _orders_csv()
    lines = _tail(p, ns.lines)
    if not lines:
        print(f"(pas de contenu) {p}")
        return 0
    print(f"# {p}")
    for ln in lines:
        print(ln)
    return 0

def cmd_tail_signals(ns: argparse.Namespace) -> int:
    p = _signals_csv()
    lines = _tail(p, ns.lines)
    if not lines:
        print(f"(pas de contenu) {p}")
        return 0
    print(f"# {p}")
    for ln in lines:
        print(ln)
    return 0

def cmd_status(_: argparse.Namespace) -> int:
    dr = _data_root_cfg()
    print("=== STATUS ===")
    print("DATA_DIR    :", dr["data_dir"])
    print("REPORTS_DIR :", dr["reports_dir"])
    wl = _watchlist_yml()
    st = _strategies_yml()
    od = _orders_csv()
    sg = _signals_csv()
    print("watchlist   :", wl, "(ok)" if wl.exists() else "(absent)")
    print("strategies  :", st, "(ok)" if st.exists() else "(absent)")
    print("orders.csv  :", od, f"(tail {len(_tail(od,1)) and 'non-vide' or 'vide'})")
    print("signals.csv :", sg, f"(tail {len(_tail(sg,1)) and 'non-vide' or 'vide'})")
    return 0

# ---------- main ----------

def main(argv: Iterable[str] | None = None) -> int:
    ap = argparse.ArgumentParser(description="Mini viewer scalp")
    sub = ap.add_subparsers(dest="cmd", required=True)

    sub.add_parser("status", help="Résumé chemins + présence des fichiers").set_defaults(func=cmd_status)
    sub.add_parser("watchlist", help="Affiche la watchlist top N").set_defaults(func=cmd_watchlist)
    sub.add_parser("strategies", help="Affiche les stratégies promues").set_defaults(func=cmd_strategies)

    p1 = sub.add_parser("orders", help="Tail des ordres (paper ou réel)")
    p1.add_argument("--lines", type=int, default=30)
    p1.set_defaults(func=cmd_tail_orders)

    p2 = sub.add_parser("signals", help="Tail des signaux prix live")
    p2.add_argument("--lines", type=int, default=30)
    p2.set_defaults(func=cmd_tail_signals)

    ns = ap.parse_args(argv)
    return int(ns.func(ns))

if __name__ == "__main__":
    raise SystemExit(main())--- [42/191] ./jobs/maintainer.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Maintainer asynchrone :
- Worker refresh en continu (thread)
- Déclenche backtest (parallèle) + promote dès que les CSV sont assez frais
- Écrit status.json / last_errors.json pour le dashboard
"""

from __future__ import annotations
import os, sys, pathlib, time, json, yaml, subprocess, threading
from typing import Dict, List, Tuple, Optional

# -- bootstrap path repo
_REPO_ROOT = str(pathlib.Path(__file__).resolve().parents[1])
if _REPO_ROOT not in sys.path:
    sys.path.insert(0, _REPO_ROOT)
import sitecustomize  # noqa: F401

CFG_PATH = os.path.join(_REPO_ROOT, "engine", "config", "config.yaml")

# ------------ utils ------------
def _read_yaml(path):
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}

def write_json(obj, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

def ohlcv_path(data_dir: str, pair: str, tf: str) -> str:
    return os.path.join(data_dir, "ohlcv", pair, f"{tf}.csv")

def tf_minutes(tf: str) -> int:
    assert tf.endswith("m")
    return int(tf[:-1])

def file_age_minutes(path: str) -> float:
    try:
        return (time.time() - os.path.getmtime(path)) / 60.0
    except FileNotFoundError:
        return float("inf")

def [REDACTED](reports_dir: str) -> List[str]:
    p = os.path.join(reports_dir, "watchlist.yml")
    if not os.path.isfile(p): return []
    doc = _read_yaml(p)
    pairs = doc.get("pairs") or doc.get("watchlist") or []
    return [x for x in pairs if isinstance(x, str)]

# ------------ subprocess helpers ------------
def run_module_blocking(modname: str, *args, cwd: Optional[str] = None) -> Tuple[int, str, str]:
    env = os.environ.copy()
    env["PYTHONPATH"] = _REPO_ROOT + (":" + env.get("PYTHONPATH","") if env.get("PYTHONPATH") else "")
    cp = subprocess.run([sys.executable, "-m", modname, *args],
                        cwd=cwd or _REPO_ROOT, env=env,
                        capture_output=True, text=True)
    return cp.returncode, cp.stdout or "", cp.stderr or ""

def run_module_detached(modname: str, *args, cwd: Optional[str] = None) -> subprocess.Popen:
    env = os.environ.copy()
    env["PYTHONPATH"] = _REPO_ROOT + (":" + env.get("PYTHONPATH","") if env.get("PYTHONPATH") else "")
    return subprocess.Popen([sys.executable, "-m", modname, *args],
                            cwd=cwd or _REPO_ROOT, env=env,
                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

# ------------ status ------------
def compute_status(data_dir: str, reports_dir: str, tf_list: List[str], age_mult: int) -> Dict:
    counts = {"MIS":0, "OLD":0, "DAT":0, "OK":0}
    matrix = []

    promoted = {}
    strat_path = os.path.join(_REPO_ROOT, "engine", "config", "strategies.yml")
    if os.path.isfile(strat_path):
        doc = _read_yaml(strat_path) or {}
        for k, v in (doc.get("strategies") or {}).items():
            promoted[k] = v

    pairs = [REDACTED](reports_dir)
    for pair in pairs:
        row = {"pair": pair}
        for tf in tf_list:
            csvp = ohlcv_path(data_dir, pair, tf)
            if not os.path.isfile(csvp):
                st = "MIS"
            else:
                age_m = file_age_minutes(csvp)
                lifetime_m = age_mult * tf_minutes(tf)
                key = [REDACTED]"{pair}:{tf}"
                active = key in promoted and not promoted[key].get("expired", False)
                if age_m > lifetime_m:
                    st = "OLD"
                elif active:
                    st = "OK"
                else:
                    st = "DAT"
            counts[st] += 1
            row[tf] = st
        matrix.append(row)

    total_cells = sum(counts.values()) or 1
    fresh_cells = counts["DAT"] + counts["OK"]
    fresh_ratio = fresh_cells / total_cells

    return {
        "generated_at": int(time.time()),
        "counts": counts,
        "matrix": matrix,
        "fresh_ratio": fresh_ratio,
        "notes": "MIS=no data · OLD=stale · DAT=fresh CSV, no active strategy · OK=fresh CSV + active strategy"
    }

# ------------ refresh worker ------------
class RefreshWorker:
    def __init__(self, tf_for_backfill: List[str], topN: int, limit: int, every_secs: int):
        self.tf_for_backfill = tf_for_backfill
        self.topN = topN
        self.limit = limit
        self.every_secs = every_secs
        self._stop = threading.Event()
        self._proc: Optional[subprocess.Popen] = None

    def start(self):
        threading.Thread(target=self._loop, daemon=True).start()

    def stop(self):
        self._stop.set()
        if self._proc and self._proc.poll() is None:
            try: self._proc.terminate()
            except Exception: pass

    def _loop(self):
        while not self._stop.is_set():
            if self._proc is None or self._proc.poll() is not None:
                args = ["--timeframe","5m","--top",str(self.topN),
                        "--backfill-tfs", ",".join(self.tf_for_backfill),
                        "--limit", str(self.limit)]
                self._proc = run_module_detached("jobs.refresh_pairs", *args)
            self._stop.wait(self.every_secs)

# ------------ main orchestrator ------------
def main():
    cfg = _read_yaml(CFG_PATH)
    rt = cfg.get("runtime", {}) if isinstance(cfg, dict) else {}

    data_dir    = rt.get("data_dir", "/notebooks/scalp_data/data")
    reports_dir = rt.get("reports_dir", "/notebooks/scalp_data/reports")
    tf_list     = list(rt.get("tf_list", ["1m","5m","15m"]))
    age_mult    = int(rt.get("age_mult", 5))
    topN        = int(rt.get("topN", 10))

    auto_refresh            = bool(rt.get("auto_refresh", True))
    refresh_every_secs      = int(rt.get("refresh_every_secs", 30))
    backfill_limit          = int(rt.get("backfill_limit", 1500))
    min_fresh_ratio         = float(rt.get("min_fresh_ratio", 0.8))
    [REDACTED]  = int(rt.get("[REDACTED]", 120))
    max_workers             = int(rt.get("[REDACTED]", 4))

    worker = RefreshWorker(tf_for_backfill=tf_list, topN=topN, limit=backfill_limit, every_secs=refresh_every_secs) if auto_refresh else None
    if worker: worker.start()

    last_bt_end = 0.0
    bt_running = False

    while True:
        # 1) statut courant
        stat = compute_status(data_dir, reports_dir, tf_list, age_mult)
        write_json(stat, os.path.join(reports_dir, "status.json"))

        c = stat["counts"]
        print(f"[maintainer] STATUS MIS={c['MIS']} OLD={c['OLD']} DAT={c['DAT']} OK={c['OK']} · fresh={stat['fresh_ratio']:.0%}")

        # 2) trigger backtest parallèle + promote
        now = time.time()
        if (not bt_running) and (now - last_bt_end >= [REDACTED]) and (stat["fresh_ratio"] >= min_fresh_ratio):
            bt_running = True
            print(f"[maintainer] Trigger backtest (max_workers={max_workers}) + promote…")
            rc_bt, out_bt, err_bt = run_module_blocking("jobs.backtest")
            if rc_bt != 0:
                print(f"[maintainer] WARN backtest RC={rc_bt} :: { (err_bt or out_bt)[-500:] }")
            # promote (sans --draft)
            src_next = os.path.join(reports_dir, "strategies.yml.next")
            rc_pr, out_pr, err_pr = run_module_blocking("jobs.promote", "--source", src_next)
            if rc_pr != 0:
                print(f"[maintainer] WARN promote RC={rc_pr} :: { (err_pr or out_pr)[-500:] }")

            # journal actions
            last = {
                "ts": int(time.time()),
                "backtest_rc": rc_bt, "backtest_err_tail": (err_bt or out_bt)[-800:],
                "promote_rc": rc_pr,  "promote_err_tail": (err_pr or out_pr)[-800:],
            }
            write_json(last, os.path.join(reports_dir, "last_errors.json"))

            last_bt_end = time.time()
            bt_running = False

        # 3) régénère dashboard.html
        run_module_blocking("tools.render_report")

        time.sleep(5)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n[maintainer] stop.")
    except Exception as e:
        print(f"[maintainer] FATAL: {e}")
        sys.exit(1)--- [43/191] ./jobs/backtest.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Backtest multi-process (ProcessPool)
- Lit config.yaml (+ backtest_config.json / entries_config.json si présents)
- Résout strategy_params via risk_mode (base ∪ profil)
- Construit la liste des tâches (pair×tf) depuis watchlist.yml ou backtest_config.assets
- Exécute en parallèle (max_workers) et agrège
- Écrit: summary.json, strategies.yml.next, debug.{txt,html}
"""

from __future__ import annotations
import os, sys, json, time, yaml
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ProcessPoolExecutor, as_completed

# bootstrap repo root
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
import sitecustomize  # noqa: F401

from engine.utils.io_safe import atomic_write_json, atomic_write_text, backup_last_good
from engine.utils.logging_setup import setup_logger
from engine.utils.params import [REDACTED]
from engine.strategies.base import Metrics
from engine.strategies import registry as strat_registry
from tools.exp_tracker import new_run_id, log_event

CONFIG_YAML    = os.path.join(PROJECT_ROOT, "engine", "config", "config.yaml")
BACKTEST_JSON  = os.path.join(PROJECT_ROOT, "backtest_config.json")
ENTRIES_JSON   = os.path.join(PROJECT_ROOT, "entries_config.json")
STRATS_NEXT    = lambda rd: os.path.join(rd, "strategies.yml.next")
SUMMARY_JSON   = lambda rd: os.path.join(rd, "summary.json")
WATCHLIST_YML  = lambda rd: os.path.join(rd, "watchlist.yml")
EXPS_DIR       = lambda rd: os.path.join(rd, "experiments")

DEBUG_TXT      = os.path.join(PROJECT_ROOT, "debug.txt")
DEBUG_HTML     = os.path.join(PROJECT_ROOT, "debug.html")

# ------------- utilitaires I/O -------------
def load_yaml(path, missing_ok=False):
    if missing_ok and not os.path.isfile(path): return {}
    with open(path, "r", encoding="utf-8") as f: return yaml.safe_load(f) or {}

def save_yaml(obj, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    atomic_write_text(yaml.safe_dump(obj, sort_keys=True, allow_unicode=True, default_flow_style=False), path)

def load_json(path, missing_ok=True):
    if missing_ok and not os.path.isfile(path): return {}
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def ohlcv_path(data_dir: str, pair: str, tf: str) -> str:
    return os.path.join(data_dir, "ohlcv", pair, f"{tf}.csv")

def load_csv_fast(path: str):
    import pandas as pd
    usecols = ["timestamp","open","high","low","close","volume"]
    if not os.path.isfile(path): return None
    try:
        df = pd.read_csv(path, usecols=usecols, dtype={
            "timestamp":"int64","open":"float64","high":"float64","low":"float64","close":"float64","volume":"float64"
        }, engine="c")
        return df.sort_values("timestamp").dropna()
    except Exception:
        return None

def load_watchlist(reports_dir: str, topN: int | None = None) -> List[str]:
    wl = load_yaml(WATCHLIST_YML(reports_dir), missing_ok=True)
    pairs = wl.get("pairs") or wl.get("watchlist") or []
    pairs = [p for p in pairs if isinstance(p, str)]
    return pairs[:topN] if topN else pairs

# ------------- scoring + debug -------------
def _score(row):
    pf=float(row.get("pf",0)); mdd=float(row.get("mdd",1)); sh=float(row.get("sharpe",0)); wr=float(row.get("wr",0))
    return pf*2.0 + sh*0.5 + wr*0.5 - mdd*1.5

def [REDACTED](rows, top_k=20, meta=None):
    rows_sorted = sorted(rows, key=[REDACTED] reverse=True)
    lines = []
    lines.append("RANK | PAIR | TF  | PF    | MDD   | TR  | WR    | Sharpe | Note")
    for i, r in enumerate(rows_sorted[:top_k], 1):
        note = _score(r)
        lines.append(
            f"{i:>4} | {r['pair']:<8} | {r['tf']:<3} | "
            f"{r['pf']:.3f} | {r['mdd']:.1%} | {r['trades']:>3} | {r['wr']:.1%} | {r['sharpe']:.2f} | {note:.2f}"
        )
    if meta:
        lines.append("")
        lines.append(f"meta: {json.dumps(meta, ensure_ascii=False)}")
    with open(DEBUG_TXT, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    html = ["<!doctype html><meta charset='utf-8'><title>SCALP Debug</title><pre>"]
    html.extend(lines); html.append("</pre>")
    with open(DEBUG_HTML, "w", encoding="utf-8") as f:
        f.write("\n".join(html))

# ------------- worker parallèle -------------
def _worker_task(pair: str, tf: str, data_dir: str, strat_name: str, strat_params: dict) -> Tuple[dict, dict]:
    """
    Fonction picklable exécutée dans un worker.
    Renvoie (row, candidate) ou (None, None) si KO.
    """
    from engine.strategies import registry as strat_registry  # import dans le worker
    from engine.strategies.base import Metrics

    df = load_csv_fast(ohlcv_path(data_dir, pair, tf))
    if df is None or len(df) < 200:
        return None, None

    strat = strat_registry.create(strat_name, strat_params)
    met: Metrics = strat.backtest(df)

    row = {
        "pair": pair, "tf": tf,
        "pf": round(met.pf,6), "mdd": round(met.mdd,6),
        "trades": int(met.trades), "wr": round(met.wr,6),
        "sharpe": round(met.sharpe,6),
    }

    params = strat.describe()
    cand = {
        **{k:v for k,v in params.items() if k!="name"},
        "name": params.get("name"),
        "created_at": int(time.time()),
        "expires_at": None,
        "expired": False,
        "metrics": {
            "pf": row["pf"], "mdd": row["mdd"], "trades": row["trades"],
            "wr": row["wr"], "sharpe": row["sharpe"]
        }
    }
    return row, cand

# ------------- run principal -------------
def run():
    # 1) runtime
    cfg = load_yaml(CONFIG_YAML, missing_ok=True)
    rt = cfg.get("runtime", {}) if isinstance(cfg, dict) else {}

    data_dir    = rt.get("data_dir", "./data")
    reports_dir = rt.get("reports_dir", "./reports")
    tf_list     = list(rt.get("tf_list", ["1m","5m","15m"]))
    topN        = int(rt.get("topN", 10))
    risk_mode   = (rt.get("risk_mode") or "normal").lower().strip()
    max_workers = int(rt.get("[REDACTED]", 4))
    chunk_size  = int(rt.get("backtest_chunk_size", 32))

    # 2) JSON configs
    bt_cfg = load_json(BACKTEST_JSON, missing_ok=True) or {}
    en_cfg = load_json(ENTRIES_JSON,  missing_ok=True) or {}

    assets_json = list(bt_cfg.get("assets", []) or [])
    tfs_json    = list(bt_cfg.get("timeframes", []) or [])
    if tfs_json: tf_list = tfs_json

    strat_name   = (rt.get("strategy_name") or "ema_atr_v1").strip()
    strat_params = [REDACTED](rt)

    # 3) loggers
    logs_dir = os.path.join(os.path.dirname(data_dir), "logs")
    os.makedirs(logs_dir, exist_ok=True)
    log = setup_logger("backtest", os.path.join(logs_dir, "backtest.log"))
    run_id = new_run_id()

    # 4) paires
    if assets_json:
        pairs = assets_json[:topN] if topN else assets_json
        src = "backtest_config.assets"
    else:
        pairs = load_watchlist(reports_dir, topN=topN)
        src = "watchlist.yml"
    if not pairs:
        log.info({"event":"no_pairs","src":src})
        log_event(EXPS_DIR(reports_dir), run_id, {"event":"no_pairs","src":src})
        return

    # 5) contraintes
    constraints = (bt_cfg.get("constraints") or {})
    min_trades = int(constraints.get("min_trades", 0))
    min_pf     = float(constraints.get("min_pf", 0.0))

    costs       = (bt_cfg.get("costs") or {})
    walkf       = (bt_cfg.get("walk_forward") or {})
    opti        = (bt_cfg.get("optimization") or {})

    meta = {
        "event":"start",
        "strategy": strat_name,
        "params": strat_params,
        "pairs_src": src,
        "pairs": pairs,
        "tf_list": tf_list,
        "constraints": constraints,
        "costs": costs,
        "walk_forward": walkf,
        "optimization": opti,
        "entries_cfg_present": bool(en_cfg),
        "risk_mode": risk_mode,
        "max_workers": max_workers,
    }
    log_event(EXPS_DIR(reports_dir), run_id, meta)

    # 6) tâches (pair×tf)
    tasks: List[Tuple[str,str]] = [(p, tf) for p in pairs for tf in tf_list]

    rows: List[Dict] = []
    cands_all: Dict[str, Dict] = {}

    # 7) exécution parallèle
    if max_workers <= 1:
        # fallback séquentiel
        for pair, tf in tasks:
            row, cand = _worker_task(pair, tf, data_dir, strat_name, strat_params)
            if row and cand:
                rows.append(row)
                cands_all[f"{pair}:{tf}"] = cand
    else:
        with ProcessPoolExecutor(max_workers=max_workers) as ex:
            futures = []
            for pair, tf in tasks:
                futures.append(ex.submit(_worker_task, pair, tf, data_dir, strat_name, strat_params))
            for fut in as_completed(futures):
                try:
                    row, cand = fut.result()
                    if row and cand:
                        rows.append(row)
                        key = [REDACTED]"{row['pair']}:{row['tf']}"
                        cands_all[key] = cand
                except Exception as e:
                    log.info({"event":"task_error","err":str(e)})

    # 8) contraintes
    cands = {}
    for k, v in cands_all.items():
        met = v.get("metrics", {})
        if int(met.get("trades", 0)) < min_trades: continue
        if float(met.get("pf", 0.0)) < min_pf: continue
        cands[k] = v

    # 9) sorties
    summary_path = SUMMARY_JSON(reports_dir)
    next_path    = STRATS_NEXT(reports_dir)

    summary_obj = {
        "generated_at": int(time.time()),
        "risk_mode": risk_mode,
        "meta": {
            "pairs_src": src, "pairs": pairs, "tf_list": tf_list,
            "constraints": constraints, "costs": costs,
            "walk_forward": walkf, "optimization": opti,
            "entries_cfg_present": bool(en_cfg),
            "strategy": {"name": strat_name, "params": strat_params},
            "max_workers": max_workers
        },
        "rows": rows
    }

    [REDACTED](rows, top_k=20, meta=summary_obj.get("meta"))
    backup_last_good(summary_path); atomic_write_json(summary_obj, summary_path)
    backup_last_good(next_path);    save_yaml({"strategies": cands}, next_path)

    log.info({"event":"done","rows":len(rows),"cands_total":len(cands_all),"[REDACTED]":len(cands)})
    log_event(EXPS_DIR(reports_dir), run_id, {
        "event":"done","rows":len(rows),
        "cands_total":len(cands_all),
        "[REDACTED]":len(cands)
    })

if __name__ == "__main__":
    try:
        run()
    except Exception as e:
        print(json.dumps({"lvl":"ERROR","msg":str(e)}))
        sys.exit(1)--- [44/191] ./jobs/topk.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations
import os, json, argparse
from typing import List, Dict

RISK_POLICIES = {
    "conservative": {"pf": 1.4, "mdd": 0.15, "trades": 35},
    "normal":       {"pf": 1.3, "mdd": 0.20, "trades": 30},
    "aggressive":   {"pf": 1.2, "mdd": 0.30, "trades": 25},
}

def load_summary(path: str) -> Dict:
    if not os.path.isfile(path):
        raise FileNotFoundError(f"summary.json introuvable: {path}")
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def score_row(r: Dict) -> float:
    """
    Petite 'note' lisible:
    - PF pèse fort (+)
    - MDD pénalise (--)
    - Sharpe (+)
    - Win rate (+ léger)
    """
    pf = float(r.get("pf", 0))
    mdd = float(r.get("mdd", 1))
    sh  = float(r.get("sharpe", 0))
    wr  = float(r.get("wr", 0))
    # Note simple, stable pour trier
    return pf*2.0 + sh*0.5 + wr*0.5 - mdd*1.5

def pass_policy(r: Dict, mode: str) -> bool:
    pol = RISK_POLICIES.get(mode, RISK_POLICIES["normal"])
    return (r.get("pf",0) >= pol["pf"]) and (r.get("mdd",1) <= pol["mdd"]) and (r.get("trades",0) >= pol["trades"])

def explain_fail(r: Dict, mode: str) -> str:
    pol = RISK_POLICIES.get(mode, RISK_POLICIES["normal"])
    bad = []
    if r.get("pf",0) < pol["pf"]:
        bad.append(f"PF {r.get('pf',0):.2f} < {pol['pf']:.2f}")
    if r.get("mdd",1) > pol["mdd"]:
        bad.append(f"MDD {r.get('mdd',1):.2%} > {pol['mdd']:.0%}")
    if r.get("trades",0) < pol["trades"]:
        bad.append(f"TR {r.get('trades',0)} < {pol['trades']}")
    return "; ".join(bad) if bad else "OK"

def print_table(rows: List[Dict], k: int, risk_mode: str):
    try:
        from rich.table import Table
        from rich.console import Console
        from rich import box
        c = Console()
        t = Table(title=f"TOP {k} — meilleurs backtests (risk_mode={risk_mode})", box=box.SIMPLE_HEAVY)
        for col in ("rank","pair","tf","PF","MDD","TR","WR","Sharpe","Note","Status"):
            t.add_column(col, justify="right" if col in ("rank","PF","MDD","TR","WR","Sharpe","Note") else "left")
        for i, r in enumerate(rows[:k], 1):
            status = "PASS=[REDACTED] if pass_policy(r, risk_mode) else f"FAIL: {explain_fail(r, risk_mode)}"
            t.add_row(
                str(i),
                r["pair"],
                r["tf"],
                f"{r.get('pf',0):.2f}",
                f"{r.get('mdd',0):.2%}",
                f"{r.get('trades',0)}",
                f"{r.get('wr',0):.2%}",
                f"{r.get('sharpe',0):.2f}",
                f"{score_row(r):.2f}",
                status
            )
        c.print(t)
    except Exception:
        # Fallback texte
        header = "RANK\tPAIR\tTF\tPF\tMDD\tTR\tWR\tSharpe\tNote\tStatus"
        print(header)
        for i, r in enumerate(rows[:k], 1):
            status = "PASS=[REDACTED] if pass_policy(r, risk_mode) else f"FAIL: {explain_fail(r, risk_mode)}"
            print(f"{i}\t{r['pair']}\t{r['tf']}\t{r.get('pf',0):.2f}\t{r.get('mdd',0):.2%}\t{r.get('trades',0)}\t{r.get('wr',0):.2%}\t{r.get('sharpe',0):.2f}\t{score_row(r):.2f}\t{status}")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--reports-dir", default="/notebooks/scalp_data/reports", help="Dossier des reports")
    ap.add_argument("--k", type=int, default=20, help="Nombre de lignes à afficher")
    ap.add_argument("--sort", default="score", choices=["score","pf","sharpe","mdd","trades"], help="Clé de tri")
    ap.add_argument("--risk-mode", default=None, help="Override du risk_mode (sinon lu dans summary.json)")
    ap.add_argument("--min-trades", type=int, default=0, help="Filtre minimal trades")
    args = ap.parse_args()

    summary_path = os.path.join(args.reports_dir, "summary.json")
    sm = load_summary(summary_path)
    rows = sm.get("rows", [])
    if not rows:
        print("Aucune ligne dans summary.json — lance d'abord jobs/backtest.py")
        return

    # risk mode
    risk_mode = args.risk_mode or sm.get("risk_mode", "normal")

    # filtre trades min
    if args.min_trades > 0:
        rows = [r for r in rows if r.get("trades", 0) >= args.min_trades]

    # tri
    if args.sort == "score":
        rows.sort(key=[REDACTED] reverse=True)
    elif args.sort == "pf":
        rows.sort(key=[REDACTED] r: (r.get("pf",0), -r.get("mdd",1), r.get("sharpe",0)), reverse=True)
    elif args.sort == "sharpe":
        rows.sort(key=[REDACTED] r: (r.get("sharpe",0), r.get("pf",0)), reverse=True)
    elif args.sort == "mdd":
        rows.sort(key=[REDACTED] r: r.get("mdd",1))  # plus petit d'abord
    elif args.sort == "trades":
        rows.sort(key=[REDACTED] r: r.get("trades",0), reverse=True)

    print_table(rows, args.k, risk_mode)

    # résumé de passage
    passed = [r for r in rows if pass_policy(r, risk_mode)]
    print(f"\nRésumé: {len(passed)} PASS=[REDACTED] / {len(rows)} total "
          f"(policy={risk_mode}: PF≥{RISK_POLICIES[risk_mode]['pf']} MDD≤{int(RISK_POLICIES[risk_mode]['mdd']*100)}% TR≥{RISK_POLICIES[risk_mode]['trades']})")

if __name__ == "__main__":
    main()--- [45/191] ./jobs/.ipynb_checkpoints/viewer-checkpoint.py ---
#!/usr/bin/env python3
# jobs/viewer.py
from __future__ import annotations

import argparse
import json
import os
from pathlib import Path
from typing import Any, Dict, Iterable, List, Sequence

from engine.config.loader import load_config

# ---------- utils paths ----------

def _data_root_cfg() -> Dict[str, str]:
    cfg = load_config()
    rt = cfg.get("runtime", {})
    return {
        "data_dir": rt.get("data_dir") or "/notebooks/scalp_data/data",
        "reports_dir": rt.get("reports_dir") or "/notebooks/scalp_data/reports",
    }

def _live_dir() -> Path:
    return Path(_data_root_cfg()["data_dir"]) / "live"

def _orders_csv() -> Path:
    return _live_dir() / "orders.csv"

def _signals_csv() -> Path:
    return _live_dir() / "logs" / "signals.csv"

def _watchlist_yml() -> Path:
    return Path(_data_root_cfg()["reports_dir"]) / "watchlist.yml"

def _strategies_yml() -> Path:
    # stocké en JSON lisible (extension .yml)
    return Path(__file__).resolve().parents[1] / "engine" / "config" / "strategies.yml"

# ---------- pretty helpers ----------

def _print_table(rows: Sequence[Sequence[Any]], headers: Sequence[str] | None = None) -> None:
    if headers:
        rows = [headers, ["-"*len(h) for h in headers], *rows]
    widths = [max(len(str(r[i])) for r in rows) for i in range(len(rows[0]))] if rows else []
    for r in rows:
        line = " | ".join(str(v).ljust(widths[i]) for i, v in enumerate(r))
        print(line)

def _tail(path: Path, n: int = 20) -> List[str]:
    if not path.exists():
        return []
    try:
        # simple tail sans dépendances
        with path.open("r", encoding="utf-8", errors="ignore") as f:
            lines = f.readlines()[-n:]
        return [l.rstrip("\n") for l in lines]
    except Exception:
        return []

# ---------- viewers ----------

def cmd_watchlist(_: argparse.Namespace) -> int:
    p = _watchlist_yml()
    if not p.exists():
        print(f"(watchlist introuvable) {p}")
        return 1
    try:
        doc = json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        print(f"(format non lisible) {p}")
        return 1
    top = doc.get("top") or []
    rows = []
    for d in top:
        rows.append([d.get("symbol",""), f"{float(d.get('vol_usd_24h',0.0)):.0f}", f"{float(d.get('atr_pct_24h',0.0))*100:.2f}%", f"{float(d.get('score',0.0)):.3f}"])
    _print_table(rows, headers=["SYMBOL","VOL_USD_24H","ATR% (approx)","SCORE"])
    return 0

def cmd_strategies(_: argparse.Namespace) -> int:
    p = _strategies_yml()
    if not p.exists():
        print(f"(strategies.yml introuvable) {p}")
        return 1
    try:
        doc = json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        print(f"(format non lisible) {p}")
        return 1
    strat = doc.get("strategies") or {}
    rows = []
    for k, v in strat.items():
        rows.append([
            k,
            v.get("ema_fast",""),
            v.get("ema_slow",""),
            v.get("atr_period",""),
            v.get("trail_atr_mult",""),
            v.get("risk_pct_equity",""),
        ])
    rows.sort(key=[REDACTED] r: r[0])
    _print_table(rows, headers=["PAIR:TF","ema_fast","ema_slow","atr_period","trail_mult","risk_pct"])
    return 0

def cmd_tail_orders(ns: argparse.Namespace) -> int:
    p = _orders_csv()
    lines = _tail(p, ns.lines)
    if not lines:
        print(f"(pas de contenu) {p}")
        return 0
    print(f"# {p}")
    for ln in lines:
        print(ln)
    return 0

def cmd_tail_signals(ns: argparse.Namespace) -> int:
    p = _signals_csv()
    lines = _tail(p, ns.lines)
    if not lines:
        print(f"(pas de contenu) {p}")
        return 0
    print(f"# {p}")
    for ln in lines:
        print(ln)
    return 0

def cmd_status(_: argparse.Namespace) -> int:
    dr = _data_root_cfg()
    print("=== STATUS ===")
    print("DATA_DIR    :", dr["data_dir"])
    print("REPORTS_DIR :", dr["reports_dir"])
    wl = _watchlist_yml()
    st = _strategies_yml()
    od = _orders_csv()
    sg = _signals_csv()
    print("watchlist   :", wl, "(ok)" if wl.exists() else "(absent)")
    print("strategies  :", st, "(ok)" if st.exists() else "(absent)")
    print("orders.csv  :", od, f"(tail {len(_tail(od,1)) and 'non-vide' or 'vide'})")
    print("signals.csv :", sg, f"(tail {len(_tail(sg,1)) and 'non-vide' or 'vide'})")
    return 0

# ---------- main ----------

def main(argv: Iterable[str] | None = None) -> int:
    ap = argparse.ArgumentParser(description="Mini viewer scalp")
    sub = ap.add_subparsers(dest="cmd", required=True)

    sub.add_parser("status", help="Résumé chemins + présence des fichiers").set_defaults(func=cmd_status)
    sub.add_parser("watchlist", help="Affiche la watchlist top N").set_defaults(func=cmd_watchlist)
    sub.add_parser("strategies", help="Affiche les stratégies promues").set_defaults(func=cmd_strategies)

    p1 = sub.add_parser("orders", help="Tail des ordres (paper ou réel)")
    p1.add_argument("--lines", type=int, default=30)
    p1.set_defaults(func=cmd_tail_orders)

    p2 = sub.add_parser("signals", help="Tail des signaux prix live")
    p2.add_argument("--lines", type=int, default=30)
    p2.set_defaults(func=cmd_tail_signals)

    ns = ap.parse_args(argv)
    return int(ns.func(ns))

if __name__ == "__main__":
    raise SystemExit(main())--- [46/191] ./jobs/__init__.py ---
 --- [47/191] ./bot.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
SCALP — bot launcher (point d'entrée unique)
"""

# 1) __future__ doit être en premier
from __future__ import annotations

# 2) bootstrap: ajouter la racine du repo au sys.path puis charger sitecustomize
import os, sys, pathlib
REPO_ROOT = str(pathlib.Path(__file__).resolve().parent)
if REPO_ROOT not in sys.path:
    sys.path.insert(0, REPO_ROOT)
import sitecustomize  # déclenche le bootstrap global (PATH, __init__.py, deps)

import os, sys, subprocess, time, yaml

PROJECT_ROOT = os.path.abspath(os.path.dirname(__file__))
CONFIG_YAML  = os.path.join(PROJECT_ROOT, "engine", "config", "config.yaml")

def _ensure(pkgs):
    import importlib
    miss=[]
    for p in pkgs:
        try: importlib.import_module(p)
        except Exception: miss.append(p)
    if miss:
        print(f"[bot] pip install {miss}")
        subprocess.check_call([sys.executable, "-m", "pip", "install"] + miss)

def load_config():
    if not os.path.isfile(CONFIG_YAML): return {"runtime": {}}
    with open(CONFIG_YAML,"r",encoding="utf-8") as f: return yaml.safe_load(f) or {"runtime": {}}

def ensure_http_server(port: int):
    pidfile = os.path.join(PROJECT_ROOT, ".httpserver.pid")
    if os.path.isfile(pidfile):
        try:
            pid = int(open(pidfile).read().strip()); os.kill(pid,0)
            print(f"[serve] http.server déjà actif (PID {pid}, port {port})")
            return
        except Exception:
            try: os.remove(pidfile)
            except Exception: pass
    out = open(os.path.join(PROJECT_ROOT, "httpserver.out"), "a")
    err = open(os.path.join(PROJECT_ROOT, "httpserver.err"), "a")
    proc = subprocess.Popen(
        [sys.executable, "-m", "http.server", str(port), "--bind", "0.0.0.0"],
        cwd=PROJECT_ROOT, stdout=out, stderr=err, preexec_fn=os.setsid
    )
    open(pidfile,"w").write(str(proc.pid))
    print(f"[serve] http.server lancé (PID {proc.pid}) → http://localhost:{port}/dashboard.html")

def start_ngrok(port: int):
    env = os.environ.copy(); env["HTML_PORT"] = str(port)
    script = os.path.join(PROJECT_ROOT, "tools", "start_ngrok.py")
    if not os.path.isfile(script):
        print("[ngrok] tools/start_ngrok.py introuvable.")
        return
    out = open(os.path.join(PROJECT_ROOT, "ngrok.out"), "a")
    err = open(os.path.join(PROJECT_ROOT, "ngrok.err"), "a")
    subprocess.Popen([sys.executable, script], env=env, cwd=PROJECT_ROOT,
                     stdout=out, stderr=err, preexec_fn=os.setsid)
    print("[ngrok] démarrage en arrière-plan… (consulte ngrok_url.txt)")

def render_html(reports_dir: str):
    env = os.environ.copy(); env["SCALP_REPORTS_DIR"] = reports_dir
    script = os.path.join(PROJECT_ROOT, "tools", "render_report.py")
    if not os.path.isfile(script):
        print("[render] tools/render_report.py introuvable."); return
    try:
        subprocess.check_call([sys.executable, script], env=env, cwd=PROJECT_ROOT)
    except subprocess.CalledProcessError as e:
        print(f"[render] erreur génération HTML (code {e.returncode})")

def run_maintainer():
    path = os.path.join(PROJECT_ROOT, "jobs", "maintainer.py")
    if not os.path.isfile(path):
        print("[bot] jobs/maintainer.py introuvable — veille/serve uniquement.")
        while True: time.sleep(60)
    subprocess.call([sys.executable, path])

def main():
    _ensure(["pyyaml"])  # safety
    cfg = load_config(); rt = cfg.get("runtime", {})
    reports_dir = rt.get("reports_dir", "/notebooks/scalp_data/reports")
    html_port   = int(rt.get("html_port", 8888))

    ensure_http_server(html_port)
    start_ngrok(html_port)
    render_html(reports_dir)
    run_maintainer()

if __name__ == "__main__":
    main()--- [48/191] ./schemas/schema_entries.json ---
{
  "entry_layer": {
    "sets": {
      "pullback_trend": {
        "context": { "min_buy_prob": 0.60, "min_adx": 0, "[REDACTED]": 0.0 },
        "filters": { "volume_percentile": 0.60, "[REDACTED]": 1.3 },
        "signals": {
          "touch": { "type": "or", "levels": ["EMA20","VWAP_sigma_1","BB_lower"] },
          "rsi_cross": { "length": 7, "level": 50, "direction": "up" },
          "[REDACTED]": true
        },
        "risk": { "sl_atr_mult": 1.2, "tp_atr_mult": 1.8, "[REDACTED]": 1.0, "trail_step_atr": 1.0 }
      },
      "breakout": {
        "context": { "min_buy_prob": 0.55, "min_adx": 20 },
        "signals": {
          "close_above_high_n": 20,
          "[REDACTED]": true,
          "squeeze": { "bb_length": 20, "bb_stddev": 2.0, "keltner_length": 20, "keltner_atr_mult": 1.5, "condition": "[REDACTED]" }
        },
        "risk": { "sl_method": "below_range_n_low", "tp_method": "range_height_or_atr", "tp_atr_mult": 1.5 }
      }
    },
    "common": {
      "vwap": { "session": "daily", "bands_sigma": [1, 2] },
      "bbands": { "length": 20, "stddev": 2.0 },
      "keltner": { "length": 20, "atr_mult": 1.5 },
      "fractals": { "lookback": 5 },
      "timeout_bars": 25
    }
  },
  "execution": {
    "prefer_maker": true,
    "post_only": true,
    "[REDACTED]": true,
    "order_timeout_sec": 20,
    "cancel_on_timeout": true
  }
}--- [49/191] ./schemas/schema_backtest.json ---
{
  "schema_version": "ai-crypto-buy/1.0",
  "strategy_name": "TwoLayer_Scalp",
  "assets": ["BTCUSDT", "ETHUSDT", "SOLUSDT", "BNBUSDT", "XRPUSDT", "ADAUSDT"],
  "timeframes": {
    "direction": ["30m", "15m", "5m"],
    "entries": ["1m", "3m", "5m"]
  },
  "regime_layer": {
    "tf_weights": { "30m": 0.5, "15m": 0.3, "5m": 0.2 },
    "indicators": {
      "ema": {
        "fast": { "30m": 12, "15m": 9, "5m": 9 },
        "slow": { "30m": 34, "15m": 21, "5m": 21 },
        "long": { "30m": 200, "15m": 200, "5m": 100 },
        "k_atr": 1.5,
        "weight": 0.35
      },
      "macd": {
        "fast": { "30m": 12, "15m": 12, "5m": 8 },
        "slow": { "30m": 26, "15m": 26, "5m": 21 },
        "signal": { "30m": 9, "15m": 9, "5m": 5 },
        "norm_window": 100,
        "weight": 0.25
      },
      "rsi": { "length": { "30m": 14, "15m": 14, "5m": 7 }, "smoothing_ema": 3, "weight": 0.20 },
      "adx": { "length": 14, "gate_below": 20, "gate_factor": 0.5, "weight": 0.10 },
      "obv": { "slope_lookback": 20, "[REDACTED]": 200, "weight": 0.10 }
    },
    "[REDACTED]": { "30m": 0.0025, "15m": 0.0018, "5m": 0.0012 },
    "score_normalization": "tanh",
    "softmax_temperature": 0.35,
    "hysteresis": { "buy_on_above": 0.60, "buy_exit_below": 0.45, "sell_on_above": 0.60, "sell_exit_below": 0.45 }
  },
  "risk_management": {
    "position_sizing": { "risk_pct_per_trade": 0.005, "max_leverage": 3.0, "min_notional": 10.0 },
    "portfolio": { "[REDACTED]": 3, "[REDACTED]": 0.5 },
    "session_limits": { "max_daily_loss_pct": 0.03, "max_daily_trades": 30 },
    "liquidity_filters": { "min_roll_vol_usd_20": 2000000.0, "max_spread_bps": 4.0 }
  },
  "costs": { "maker_fee_rate": 0.0002, "taker_fee_rate": 0.0008, "slippage_max_bps": 5.0, "[REDACTED]": 0.0010 },
  "backtest": {
    "timezone": "UTC",
    "include_fees": true,
    "include_slippage": true,
    "walk_forward": { "train_days": 90, "test_days": 30, "segments": 8 },
    "oos_holdout_days": 60,
    "metrics": ["NetProfit","PF","Sharpe","Sortino","MaxDD","HitRate","AvgRR","Turnover"]
  },
  "optimization": {
    "engine": "optuna",
    "objective": { "type": "multi", "maximize": ["PF","Sharpe"], "minimize": ["MaxDD","Turnover"], "weights": {"PF": 0.4, "Sharpe": 0.3, "MaxDD": -0.2, "Turnover": -0.1} },
    "global_constraints": { "min_trades": 200, "min_pf": 1.2 },
    "grids": {
      "regime": {
        "ema_fast": [7, 9, 12],
        "ema_slow": [21, 26, 34],
        "ema_long": [100, 200],
        "macd": [[8,21,5],[12,26,9]],
        "rsi_length": [7, 10, 14],
        "hysteresis_buy_exit": [[0.6,0.45],[0.65,0.5]],
        "atr_gate_pct_scale": [0.8, 1.0, 1.2],
        "softmax_temperature": [0.3, 0.35, 0.4],
        "tf_weights_sets": [
          { "30m": 0.5, "15m": 0.3, "5m": 0.2 },
          { "30m": 0.4, "15m": 0.4, "5m": 0.2 }
        ]
      }
    }
  },
  "outputs": {
    "signals": { "buy_threshold": 0.60, "sell_threshold": 0.60, "hold_band": [0.40, 0.60] },
    "export": { "format": "parquet", "columns": ["timestamp","symbol","tf","p_buy","p_hold","p_sell","entry_set","side","sl","tp","size"] }
  }
}--- [50/191] ./dash/app_streamlit.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os, json, glob
import pandas as pd
import streamlit as st
import plotly.express as px

st.set_page_config(page_title="SCALP — Backtest Dashboard", layout="wide")

st.title("SCALP — Backtest Dashboard")

reports_dir = st.sidebar.text_input("Reports dir", "/notebooks/scalp_data/reports")
signals_root = os.path.join(reports_dir, "signals")

# Summary
summary_path = os.path.join(reports_dir, "summary.json")
if not os.path.isfile(summary_path):
    st.warning("summary.json introuvable. Lance `python jobs/backtest.py` d'abord.")
    st.stop()

with open(summary_path, "r", encoding="utf-8") as f:
    summary = json.load(f)
rows = pd.DataFrame(summary.get("rows", []))
if rows.empty:
    st.info("Aucun résultat en base."); st.stop()

# Filtres
pairs = sorted(rows["pair"].unique().tolist())
tfs = sorted(rows["tf"].unique().tolist())
col1, col2, col3, col4 = st.columns(4)
with col1:
    pair_sel = st.multiselect("Paires", pairs, default=pairs)
with col2:
    tf_sel = st.multiselect("TF", tfs, default=tfs)
with col3:
    pf_min = st.number_input("PF min", value=1.2, step=0.1)
with col4:
    mdd_max = st.number_input("MDD max", value=0.30, step=0.05, format="%.2f")

f = rows[
    rows["pair"].isin(pair_sel)
    & rows["tf"].isin(tf_sel)
    & (rows["pf"] >= pf_min)
    & (rows["mdd"] <= mdd_max)
].copy()

st.subheader("Tableau des résultats filtrés")
st.dataframe(f.sort_values(["pf","sharpe","mdd"], ascending=[False, False, True]), use_container_width=True)

# Heatmap PF par paire/TF
st.subheader("Heatmap PF par paire / TF")
if not f.empty:
    pivot_pf = f.pivot_table(index="pair", columns="tf", values="pf", aggfunc="max")
    fig = px.imshow(pivot_pf, aspect="auto", [REDACTED]="Viridis", origin="lower")
    st.plotly_chart(fig, use_container_width=True)

# Sélection pour signaux
st.subheader("Signaux & probas (p_buy) — Parquet")
pair_view = st.selectbox("Paire", pairs)
tf_view = st.selectbox("TF", tfs)
parquet_path = os.path.join(signals_root, pair_view, f"{tf_view}.parquet")
csv_fallback = os.path.join(signals_root, pair_view, f"{tf_view}.csv")

if os.path.isfile(parquet_path) or os.path.isfile(csv_fallback):
    if os.path.isfile(parquet_path):
        sig = pd.read_parquet(parquet_path)
    else:
        sig = pd.read_csv(csv_fallback)

    sig["dt"] = pd.to_datetime(sig["timestamp"], unit="ms", utc=True).dt.tz_convert("UTC")
    st.write(f"Fichier: {parquet_path if os.path.isfile(parquet_path) else csv_fallback} — {len(sig)} lignes")

    cols = st.multiselect("Colonnes à afficher", ["p_buy","state","entry_long","entry_set","close","sl","tp"], default=["p_buy","entry_long","close"])
    st.dataframe(sig[["dt"] + cols].tail(500), use_container_width=True)

    # Chart p_buy & close
    left, right = st.columns([2,1])
    with left:
        fig = px.line(sig.tail(2000), x="dt", y=["p_buy","close"], title="p_buy & close (échantillon)")
        st.plotly_chart(fig, use_container_width=True)
    with right:
        # distribution p_buy
        figh = px.histogram(sig, x="p_buy", nbins=30, title="Distribution p_buy")
        st.plotly_chart(figh, use_container_width=True)
else:
    st.info("Aucun fichier de signaux pour cette paire/TF. Active l'export dans backtest (--export-signals).")--- [51/191] ./dash/app.py ---
# dash/app.py
from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Dict, List, Tuple

import pandas as pd
import streamlit as st

# ---------- config & chemins ----------
def _load_config_paths() -> Dict[str, str]:
    try:
        # on lit la config du moteur si dispo
        from engine.config.loader import load_config
        cfg = load_config()
        r = cfg.get("runtime", {})
        return {
            "DATA_DIR": r.get("data_dir") or "/notebooks/scalp_data/data",
            "REPORTS_DIR": r.get("reports_dir") or "/notebooks/scalp_data/reports",
        }
    except Exception:
        root = os.getenv("DATA_ROOT", "/notebooks/scalp_data")
        return {
            "DATA_DIR": str(Path(root) / "data"),
            "REPORTS_DIR": str(Path(root) / "reports"),
        }

PATHS = _load_config_paths()

def p_live() -> Path: return Path(PATHS["DATA_DIR"]) / "live"
def p_orders() -> Path: return p_live() / "orders.csv"
def p_signals() -> Path: return p_live() / "logs" / "signals.csv"
def p_watchlist() -> Path: return Path(PATHS["REPORTS_DIR"]) / "watchlist.yml"
def p_strategies() -> Path: return Path(__file__).resolve().parents[1] / "engine" / "config" / "strategies.yml"
def p_summary() -> Path: return Path(PATHS["REPORTS_DIR"]) / "summary.json"

# ---------- helpers IO ----------
@st.cache_data(show_spinner=False)
def load_json_file(path: Path) -> Dict:
    if not path.exists(): return {}
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}

@st.cache_data(show_spinner=False)
def load_csv_tail(path: Path, n: int = 1000) -> pd.DataFrame:
    if not path.exists(): return pd.DataFrame()
    try:
        # lecture efficace tail n lignes
        with path.open("rb") as f:
            f.seek(0, os.SEEK_END)
            size = f.tell()
            block = 4096
            data = b""
            while len(data.splitlines()) <= n + 1 and f.tell() > 0:
                step = min(block, f.tell())
                f.seek(-step, os.SEEK_CUR)
                data = f.read(step) + data
                f.seek(-step, os.SEEK_CUR)
            s = data.decode("utf-8", errors="ignore")
        df = pd.read_csv(pd.compat.StringIO(s))
        return df.tail(n).reset_index(drop=True)
    except Exception:
        try:
            return pd.read_csv(path).tail(n).reset_index(drop=True)
        except Exception:
            return pd.DataFrame()

@st.cache_data(show_spinner=False)
def load_signals() -> pd.DataFrame:
    df = load_csv_tail(p_signals(), n=5000)
    if not df.empty:
        # ts en ms → datetime
        df["ts"] = pd.to_datetime(df["ts"], unit="ms")
    return df

@st.cache_data(show_spinner=False)
def load_orders() -> pd.DataFrame:
    df = load_csv_tail(p_orders(), n=2000)
    if not df.empty:
        df["ts"] = pd.to_datetime(df["ts"], unit="ms")
    return df

# ---------- UI ----------
st.set_page_config(page_title="Scalp Dashboard", layout="wide")
st.title("⚡ Scalp — Dashboard")

with st.sidebar:
    st.subheader("Paramètres")
    refresh_sec = st.number_input("Auto-refresh (sec)", min_value=0, max_value=120, value=10, step=1)
    if refresh_sec > 0:
        st.autorefresh = st.experimental_rerun  # alias doux
        st.[REDACTED](_=int(pd.Timestamp.now().timestamp()))
        st.[REDACTED]  # no-op; évite un warning IDE
        st.experimental_rerun  # pas déclenché ici; déclenché par autorefresh ci-dessous
        st.experimental_memo
        st_autorefresh = st.experimental_rerun
        st.empty()
    page = st.radio("Vue", ["Overview", "Watchlist", "Strategies", "Live: Signals", "Live: Orders"], index=0)

    st.caption(f"DATA_DIR: {PATHS['DATA_DIR']}")
    st.caption(f"REPORTS_DIR: {PATHS['REPORTS_DIR']}")

# déclencheur auto-refresh léger
if refresh_sec > 0:
    st.[REDACTED](ts=int(pd.Timestamp.now().timestamp()))
    st_autorefresh_id = st.[REDACTED](lambda: 0)
    st.experimental_rerun

# ---------- PAGES ----------
if page == "Overview":
    c1, c2, c3, c4 = st.columns(4)
    wl = load_json_file(p_watchlist())
    strat = load_json_file(p_strategies())
    sig = load_signals()
    ords = load_orders()

    c1.metric("Watchlist", len(wl.get("top", [])))
    c2.metric("Strategies", len((strat.get("strategies") or {})))
    c3.metric("Signals (tail)", len(sig))
    c4.metric("Orders (tail)", len(ords))

    st.markdown("### Fichiers")
    status = [
        ("watchlist.yml", p_watchlist(), p_watchlist().exists()),
        ("strategies.yml", p_strategies(), p_strategies().exists()),
        ("signals.csv", p_signals(), p_signals().exists()),
        ("orders.csv", p_orders(), p_orders().exists()),
        ("summary.json", p_summary(), p_summary().exists()),
    ]
    st.table(pd.DataFrame([{"file": n, "path": str(p), "exists": ok} for n, p, ok in status]))

    st.markdown("### Derniers signaux (aperçu)")
    st.dataframe(sig.tail(30), use_container_width=True)

elif page == "Watchlist":
    doc = load_json_file(p_watchlist())
    top = pd.DataFrame(doc.get("top", []))
    if top.empty:
        st.warning(f"Watchlist vide ou introuvable: {p_watchlist()}")
    else:
        top = top.sort_values("score", ascending=False).reset_index(drop=True)
        st.subheader("Top (score volume+volatilité)")
        st.dataframe(top, use_container_width=True)
        c1, c2 = st.columns(2)
        with c1:
            if {"symbol", "vol_usd_24h"}.issubset(top.columns):
                st.bar_chart(top.set_index("symbol")["vol_usd_24h"])
        with c2:
            if {"symbol", "atr_pct_24h"}.issubset(top.columns):
                st.bar_chart((top.set_index("symbol")["atr_pct_24h"] * 100).rename("ATR %"))

elif page == "Strategies":
    doc = load_json_file(p_strategies())
    strat = pd.DataFrame([
        {"pair_tf": k, **v} for k, v in (doc.get("strategies") or {}).items()
    ])
    if strat.empty:
        st.info("Aucune stratégie promue. Lance les jobs backtest + promote.")
    else:
        st.subheader("Stratégies promues (par pair:TF)")
        st.dataframe(strat.sort_values("pair_tf"), use_container_width=True)

elif page == "Live: Signals":
    df = load_signals()
    if df.empty:
        st.warning(f"Aucun signal (fichier introuvable ou vide): {p_signals()}")
    else:
        syms = sorted(df["symbol"].unique().tolist())
        sym = st.selectbox("Symbole", syms, index=0)
        tf = st.selectbox("Timeframe", sorted(df["tf"].unique().tolist()), index=0)
        d = df[(df["symbol"] == sym) & (df["tf"] == tf)].copy()
        d = d.sort_values("ts")
        st.line_chart(d.set_index("ts")["price"], height=300)
        st.dataframe(d.tail(200), use_container_width=True)

elif page == "Live: Orders":
    df = load_orders()
    if df.empty:
        st.info(f"Aucun ordre encore. (paper mode ?) Fichier: {p_orders()}")
    else:
        st.subheader("Journal des ordres")
        st.dataframe(df.tail(200), use_container_width=True)
        # stats rapides
        buys = (df["action"] == "BUY").sum()
        sells = (df["action"] == "SELL").sum()
        st.caption(f"BUY: {buys} • SELL: {sells}")

# pied de page
st.write("---")
st.caption("Scalp Dashboard • Streamlit • auto-refresh paramétrable • fichiers hors repo")--- [52/191] ./dash/README.md ---
# SCALP — Mini Dashboard (Streamlit)

## Prérequis
- `pip install streamlit plotly pyarrow` (pyarrow recommandé pour les Parquet)

## Lancement
```bash
streamlit run dash/app_streamlit.py--- [53/191] ./dash/.ipynb_checkpoints/app-checkpoint.py ---
# dash/app.py
from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Dict, List, Tuple

import pandas as pd
import streamlit as st

# ---------- config & chemins ----------
def _load_config_paths() -> Dict[str, str]:
    try:
        # on lit la config du moteur si dispo
        from engine.config.loader import load_config
        cfg = load_config()
        r = cfg.get("runtime", {})
        return {
            "DATA_DIR": r.get("data_dir") or "/notebooks/scalp_data/data",
            "REPORTS_DIR": r.get("reports_dir") or "/notebooks/scalp_data/reports",
        }
    except Exception:
        root = os.getenv("DATA_ROOT", "/notebooks/scalp_data")
        return {
            "DATA_DIR": str(Path(root) / "data"),
            "REPORTS_DIR": str(Path(root) / "reports"),
        }

PATHS = _load_config_paths()

def p_live() -> Path: return Path(PATHS["DATA_DIR"]) / "live"
def p_orders() -> Path: return p_live() / "orders.csv"
def p_signals() -> Path: return p_live() / "logs" / "signals.csv"
def p_watchlist() -> Path: return Path(PATHS["REPORTS_DIR"]) / "watchlist.yml"
def p_strategies() -> Path: return Path(__file__).resolve().parents[1] / "engine" / "config" / "strategies.yml"
def p_summary() -> Path: return Path(PATHS["REPORTS_DIR"]) / "summary.json"

# ---------- helpers IO ----------
@st.cache_data(show_spinner=False)
def load_json_file(path: Path) -> Dict:
    if not path.exists(): return {}
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}

@st.cache_data(show_spinner=False)
def load_csv_tail(path: Path, n: int = 1000) -> pd.DataFrame:
    if not path.exists(): return pd.DataFrame()
    try:
        # lecture efficace tail n lignes
        with path.open("rb") as f:
            f.seek(0, os.SEEK_END)
            size = f.tell()
            block = 4096
            data = b""
            while len(data.splitlines()) <= n + 1 and f.tell() > 0:
                step = min(block, f.tell())
                f.seek(-step, os.SEEK_CUR)
                data = f.read(step) + data
                f.seek(-step, os.SEEK_CUR)
            s = data.decode("utf-8", errors="ignore")
        df = pd.read_csv(pd.compat.StringIO(s))
        return df.tail(n).reset_index(drop=True)
    except Exception:
        try:
            return pd.read_csv(path).tail(n).reset_index(drop=True)
        except Exception:
            return pd.DataFrame()

@st.cache_data(show_spinner=False)
def load_signals() -> pd.DataFrame:
    df = load_csv_tail(p_signals(), n=5000)
    if not df.empty:
        # ts en ms → datetime
        df["ts"] = pd.to_datetime(df["ts"], unit="ms")
    return df

@st.cache_data(show_spinner=False)
def load_orders() -> pd.DataFrame:
    df = load_csv_tail(p_orders(), n=2000)
    if not df.empty:
        df["ts"] = pd.to_datetime(df["ts"], unit="ms")
    return df

# ---------- UI ----------
st.set_page_config(page_title="Scalp Dashboard", layout="wide")
st.title("⚡ Scalp — Dashboard")

with st.sidebar:
    st.subheader("Paramètres")
    refresh_sec = st.number_input("Auto-refresh (sec)", min_value=0, max_value=120, value=10, step=1)
    if refresh_sec > 0:
        st.autorefresh = st.experimental_rerun  # alias doux
        st.[REDACTED](_=int(pd.Timestamp.now().timestamp()))
        st.[REDACTED]  # no-op; évite un warning IDE
        st.experimental_rerun  # pas déclenché ici; déclenché par autorefresh ci-dessous
        st.experimental_memo
        st_autorefresh = st.experimental_rerun
        st.empty()
    page = st.radio("Vue", ["Overview", "Watchlist", "Strategies", "Live: Signals", "Live: Orders"], index=0)

    st.caption(f"DATA_DIR: {PATHS['DATA_DIR']}")
    st.caption(f"REPORTS_DIR: {PATHS['REPORTS_DIR']}")

# déclencheur auto-refresh léger
if refresh_sec > 0:
    st.[REDACTED](ts=int(pd.Timestamp.now().timestamp()))
    st_autorefresh_id = st.[REDACTED](lambda: 0)
    st.experimental_rerun

# ---------- PAGES ----------
if page == "Overview":
    c1, c2, c3, c4 = st.columns(4)
    wl = load_json_file(p_watchlist())
    strat = load_json_file(p_strategies())
    sig = load_signals()
    ords = load_orders()

    c1.metric("Watchlist", len(wl.get("top", [])))
    c2.metric("Strategies", len((strat.get("strategies") or {})))
    c3.metric("Signals (tail)", len(sig))
    c4.metric("Orders (tail)", len(ords))

    st.markdown("### Fichiers")
    status = [
        ("watchlist.yml", p_watchlist(), p_watchlist().exists()),
        ("strategies.yml", p_strategies(), p_strategies().exists()),
        ("signals.csv", p_signals(), p_signals().exists()),
        ("orders.csv", p_orders(), p_orders().exists()),
        ("summary.json", p_summary(), p_summary().exists()),
    ]
    st.table(pd.DataFrame([{"file": n, "path": str(p), "exists": ok} for n, p, ok in status]))

    st.markdown("### Derniers signaux (aperçu)")
    st.dataframe(sig.tail(30), use_container_width=True)

elif page == "Watchlist":
    doc = load_json_file(p_watchlist())
    top = pd.DataFrame(doc.get("top", []))
    if top.empty:
        st.warning(f"Watchlist vide ou introuvable: {p_watchlist()}")
    else:
        top = top.sort_values("score", ascending=False).reset_index(drop=True)
        st.subheader("Top (score volume+volatilité)")
        st.dataframe(top, use_container_width=True)
        c1, c2 = st.columns(2)
        with c1:
            if {"symbol", "vol_usd_24h"}.issubset(top.columns):
                st.bar_chart(top.set_index("symbol")["vol_usd_24h"])
        with c2:
            if {"symbol", "atr_pct_24h"}.issubset(top.columns):
                st.bar_chart((top.set_index("symbol")["atr_pct_24h"] * 100).rename("ATR %"))

elif page == "Strategies":
    doc = load_json_file(p_strategies())
    strat = pd.DataFrame([
        {"pair_tf": k, **v} for k, v in (doc.get("strategies") or {}).items()
    ])
    if strat.empty:
        st.info("Aucune stratégie promue. Lance les jobs backtest + promote.")
    else:
        st.subheader("Stratégies promues (par pair:TF)")
        st.dataframe(strat.sort_values("pair_tf"), use_container_width=True)

elif page == "Live: Signals":
    df = load_signals()
    if df.empty:
        st.warning(f"Aucun signal (fichier introuvable ou vide): {p_signals()}")
    else:
        syms = sorted(df["symbol"].unique().tolist())
        sym = st.selectbox("Symbole", syms, index=0)
        tf = st.selectbox("Timeframe", sorted(df["tf"].unique().tolist()), index=0)
        d = df[(df["symbol"] == sym) & (df["tf"] == tf)].copy()
        d = d.sort_values("ts")
        st.line_chart(d.set_index("ts")["price"], height=300)
        st.dataframe(d.tail(200), use_container_width=True)

elif page == "Live: Orders":
    df = load_orders()
    if df.empty:
        st.info(f"Aucun ordre encore. (paper mode ?) Fichier: {p_orders()}")
    else:
        st.subheader("Journal des ordres")
        st.dataframe(df.tail(200), use_container_width=True)
        # stats rapides
        buys = (df["action"] == "BUY").sum()
        sells = (df["action"] == "SELL").sum()
        st.caption(f"BUY: {buys} • SELL: {sells}")

# pied de page
st.write("---")
st.caption("Scalp Dashboard • Streamlit • auto-refresh paramétrable • fichiers hors repo")--- [54/191] ./bin/safe_render.sh ---
#!/usr/bin/env bash
set -Eeuo pipefail
[ -f /etc/scalp.env ] && set -a && . /etc/scalp.env && set +a

REPO_PATH="${REPO_PATH:-/opt/scalp}"
LOG_DIR="${LOG_DIR:-$REPO_PATH/logs}"; mkdir -p "$LOG_DIR"
LOG="$LOG_DIR/render-$(date -u +%Y%m%d-%H%M%S).log"
ln -sf "$(basename "$LOG")" "$LOG_DIR/latest.log" || true
/opt/scalp/bin/git-sync.sh || true
# lock anti doublon
LOCK=/tmp/scalp.render.lock
exec 9>"$LOCK"
flock -n 9 || { echo "[safe] skip: déjà en cours" | tee -a "$LOG"; exit 0; }
trap 'rm -f "$LOCK"' EXIT

cd "$REPO_PATH" || { echo "[safe] repo introuvable: $REPO_PATH" | tee -a "$LOG"; exit 2; }

# Python venv ou système
PY="$REPO_PATH/venv/bin/python"; [ -x "$PY" ] || PY="$(command -v python3)"
[ -n "$PY" ] || { echo "[safe] pas de python" | tee -a "$LOG"; exit 127; }
echo "[safe] $($PY -V 2>&1)" | tee -a "$LOG"

# anti-sitecustomize
export PYTHONNOUSERSITE=1
export PYTHONPATH="$REPO_PATH"

# helper retry
retry() {
  local max="${1:-1}"; shift
  local i rc
  for i in $(seq 1 "$max"); do
    if "$@"; then return 0; fi
    rc=$?
    echo "[retry] tentative $i/$max rc=$rc" | tee -a "$LOG"
    sleep $((1+i))
  done
  return ${rc:-1}
}

# rendu HTML atomique
mkdir -p docs
TMP_HTML="$(mktemp)"
if ! retry "${SCALP_FETCH_RETRIES:-2}" timeout "${SCALP_FETCH_TIMEOUT:-25}s" "$PY" -S -m tools.render_report >"$TMP_HTML" 2>>"$LOG"; then
  echo "[render] ❌ échec rendu (voir log)" | tee -a "$LOG"; exit 1
fi
mv "$TMP_HTML" docs/index.html
echo "[render] ✅ docs/index.html" | tee -a "$LOG"

# health.json minimal
COMMIT="$(git rev-parse --short HEAD 2>/dev/null || echo unknown)"
cat > docs/health.json <<JSON
{
  "generated_at": $(date -u +%s),
  "generated_at_human": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
  "commit": "$COMMIT",
  "status": "ok"
}
JSON
echo "[health] ✅ docs/health.json" | tee -a "$LOG"

/opt/scalp/bin/git-sync.sh || true

# publish si changements
UPDATED=0; git diff --quiet -- docs/ || UPDATED=1
if [ "${SCALP_AUTO_SYNC:-1}" = "1" ] && [ "$UPDATED" = "1" ]; then
  if retry "${SCALP_GIT_RETRIES:-2}" bash -lc 'bin/git-sync.sh >>"$LOG" 2>&1'; then
    echo "[publish] ✅ OK" | tee -a "$LOG"
  else
    echo "[publish] ⚠️ KO (voir log)" | tee -a "$LOG"
  fi
else
  echo "[publish] (skip) rien à pousser" | tee -a "$LOG"
fi

echo "[safe] ✅ rendu OK — log: $LOG"
--- [55/191] ./bin/git-doctor.sh ---
#!/usr/bin/env bash
set -Eeuo pipefail

say()  { printf '%b\n' "$*"; }
ok()   { say "✅ $*"; }
ko()   { say "❌ $*"; }
info() { say "ℹ️  $*"; }

# --- charge env global si dispo (/etc/scalp.env) ---
if [ -f /etc/scalp.env ]; then
  # attend GIT_USER, GIT_TOKEN=[REDACTED] GIT_REPO éventuellement
  # et REPO_PATH=/opt/scalp par défaut
  set -a; . /etc/scalp.env; set +a
fi

REPO_PATH="${REPO_PATH:-/opt/scalp}"
BRANCH="${BRANCH:-main}"
cd "$REPO_PATH" || { ko "Repo introuvable: $REPO_PATH"; exit 2; }

command -v git >/dev/null || { ko "git manquant"; exit 1; }

# --- remote origin : https simple ou URL token si fourni ---
REMOTE_SIMPLE="https://github.com/${GIT_REPO:-nicolasfoltzer77-tech/Scalp}.git"
if [ -n "${GIT_USER:-}" ] && [ -n "${GIT_TOKEN=[REDACTED] ] && [ -n "${GIT_REPO:-}" ]; then
  REMOTE_URL="https://${GIT_USER}:${GIT_TOKEN=[REDACTED]
else
  REMOTE_URL="$REMOTE_SIMPLE"
fi

if git remote get-url origin >/dev/null 2>&1; then
  git remote set-url origin "$REMOTE_URL"
else
  git remote add origin "$REMOTE_URL"
fi
ok "remote origin = $(git remote get-url origin)"

# --- état du repo ---
git status --porcelain=v1 > /tmp/gitstat.$$ || true
CHANGES=$(cat /tmp/gitstat.$$)
# stop tracking de venv si jamais indexée
if git ls-files --error-unmatch venv >/dev/null 2>&1; then
  info "Retire venv du suivi"
  git rm -r --cached venv || true
fi

# .gitignore minimal béton
GI=.gitignore
touch "$GI"
grep -qxF 'venv/'           "$GI" || echo 'venv/' >> "$GI"
grep -qxF '__pycache__/'    "$GI" || echo '__pycache__/' >> "$GI"
grep -qxF '.pytest_cache/'  "$GI" || echo '.pytest_cache/' >> "$GI"
grep -qxF 'logs/'           "$GI" || echo 'logs/' >> "$GI"
grep -qxF '*.log'           "$GI" || echo '*.log' >> "$GI"
grep -qxF '*.map'           "$GI" || echo '*.map' >> "$GI"

# (ré)ajoute un .gitattributes pour éviter CRLF chelou (optionnel)
GA=.gitattributes
if [ ! -f "$GA" ]; then
  echo "* text=auto" > "$GA"
fi

# Si modifications locales -> commit auto (message explicite)
if [ -n "$CHANGES" ] || ! git diff --quiet -- "$GI" "$GA"; then
  git add -A
  git commit -m "chore(git-doctor): auto-commit (ignore/clean + changes locales)" || true
  ok "commit auto effectué (s’il y avait des changements)"
else
  ok "aucun changement local à committer"
fi

# --- fetch + pull (rebase) robuste ---
git fetch origin || { ko "fetch origin KO"; exit 1; }

# si rebase déjà en cours, on l’abandonne proprement
[ -d .git/rebase-merge ] && { info "rebase en cours détecté → abort"; git rebase --abort || true; }
[ -d .git/rebase-apply ] && { info "rebase en cours détecté → abort"; git rebase --abort || true; }

info "pull --rebase --autostash depuis origin/${BRANCH}"
git pull --rebase --autostash origin "$BRANCH" || {
  ko "pull --rebase a échoué"; exit 1;
}
ok "pull OK"

# --- configure le suivi si absent puis push ---
if ! git rev-parse --abbrev-ref --symbolic-full-name @{u} >/dev/null 2>&1; then
  info "Crée le suivi upstream ($BRANCH → origin/$BRANCH)"
  git push -u origin "$BRANCH"
else
  git push origin HEAD:"$BRANCH" || { ko "push KO"; exit 1; }
fi
ok "push OK"

# Résumé
say "──────────"
ok "Git doctor terminé ✔"
git --no-pager log -1 --oneline
--- [56/191] ./bin/watch_render.sh ---
#!/usr/bin/env bash
set -Eeuo pipefail
SCRIPT_DIR="$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" && pwd)"
while true; do
  "$SCRIPT_DIR/safe_render.sh" || true
  sleep 120
done--- [57/191] ./bin/git-autopush.sh ---
#!/usr/bin/env bash
set -Eeuo pipefail

say(){ printf '%b\n' "$*"; }
ok(){  say "✅ $*"; }
ko(){  say "❌ $*"; }
info(){ say "ℹ️  $*"; }

LOCK=/tmp/scalp_git_autopush.lock
cleanup(){ rm -f "$LOCK"; }
if [[ -e "$LOCK" ]] && ps -p "$(cut -d: -f1 <"$LOCK" 2>/dev/null || echo 0)" &>/dev/null; then
  info "autopush déjà en cours — je sors"; exit 0; fi
echo "$$:$(date +%s)" > "$LOCK"; trap cleanup EXIT

ROOT=/opt/scalp
BRANCH=${BRANCH:-main}
cd "$ROOT" || { ko "repo introuvable: $ROOT"; exit 2; }
[[ -d .git ]] || { ko "pas un dépôt git"; exit 2; }

# Secrets/env (user/token/repo)
if [[ -f /etc/scalp.env ]]; then set -a; . /etc/scalp.env; set +a; fi
GIT_REPO="${GIT_REPO:-nicolasfoltzer77-tech/Scalp}"
REMOTE_URL="https://github.com/${GIT_REPO}.git"
if [[ -n "${GIT_USER:-}" && -n "${GIT_TOKEN=[REDACTED] ]]; then
  REMOTE_URL="https://${GIT_USER}:${GIT_TOKEN=[REDACTED]
fi
git remote set-url origin "$REMOTE_URL" || true

# Rien à pousser ? (respecte .gitignore)
if git diff --quiet && git diff --cached --quiet && [[ -z "$(git ls-files --others --exclude-standard)" ]]; then
  info "aucun changement à pousser"; exit 0; fi

# Ajout + commit
git add -A
if ! git diff --cached --quiet; then
  git commit -m "chore(autopush): $(hostname) $(date -u +%F_%H:%MZ)"
else
  info "rien de staged après add -A — je sors"; exit 0
fi

# Rebase propre + push
git fetch origin || true
if ! git pull --rebase --autostash origin "$BRANCH"; then git rebase --abort || true; ko "conflit rebase"; exit 1; fi
git push -u origin "$BRANCH"
ok "auto-push terminé"
--- [58/191] ./bin/bootstrap.sh ---
#!/usr/bin/env bash
set -Eeuo pipefail
cd "$(dirname "$(realpath "$0")")/.."  # racine repo

# venv idempotent
if [ ! -x venv/bin/python ]; then
  python3 -m venv venv
fi
. venv/bin/activate

# deps
python -m pip install -U pip setuptools wheel
[ -f requirements.txt ] && pip install -r requirements.txt || true

echo "[bootstrap] OK"--- [59/191] ./bin/doctor.sh ---
#!/usr/bin/env bash
set -Eeuo pipefail

say() { printf '%b\n' "$*"; }
ok()  { say "✅ $*"; }
ko()  { say "❌ $*"; }
info(){ say "ℹ️  $*"; }

# ---------- anti-boucle : lock re-entrance ----------
LOCK=/tmp/scalp_doctor.lock
cleanup(){ rm -f "$LOCK"; }
if [[ -e "$LOCK" ]]; then
  # si PID encore vivant -> on sort gentiment
  if ps -p "$(cut -d: -f1 <"$LOCK" 2>/dev/null || echo 0)" &>/dev/null; then
    info "doctor déjà en cours (lock $LOCK) — je sors"
    exit 0
  fi
fi
echo "$$:$(date +%s)" > "$LOCK"
trap cleanup EXIT

# ---------- checks ----------
if command -v python3 >/dev/null 2>&1; then
  PY=$(command -v python3)
  ok "Python trouvé : $PY"
else
  ko "Python3 manquant"
  exit 1
fi

if command -v pip3 >/dev/null 2>&1; then
  ok "pip3 trouvé"
else
  ko "pip3 manquant"
  exit 1
fi

# pytest (sans réinstaller inutilement)
if python3 -c 'import pytest' 2>/dev/null; then
  ok "pytest installé"
else
  info "Installation de pytest…"
  python3 -m pip install --quiet --disable-pip-version-check pytest || {
    ko "install pytest"
    exit 1
  }
  ok "pytest installé"
fi

# ---------- tests ----------
info "Lancement des tests…"
python3 - <<'PY'
from pathlib import Path
import json, sys
# test 1 : docs/health.json
p = Path("/opt/scalp/docs/health.json")
assert p.exists(), "docs/health.json absent"
j = json.loads(p.read_text())
assert j.get("status") == "ok"
assert "generated_at" in j
# test 2 : docs/index.html
p2 = Path("/opt/scalp/docs/index.html")
assert p2.exists() and p2.stat().st_size > 0
print("OK")
PY
ok "Tous les tests ont réussi 🎉"
--- [60/191] ./bin/git-sync.sh ---
#!/usr/bin/env bash
#!/usr/bin/env bash
set -Eeuo pipefail

say(){ printf '%b\n' "$*"; }
ok(){  say "✅ $*"; }
ko(){  say "❌ $*"; }
info(){ say "ℹ️  $*"; }

ROOT=/opt/scalp
BRANCH=${BRANCH:-main}
LOGDIR=$ROOT/logs; mkdir -p "$LOGDIR"
LOG="$LOGDIR/git-sync_$(date -u +%F_%H%M%S).log"

# -------- anti double-run (lock) --------
LOCK=/tmp/scalp_git_sync.lock
cleanup(){ rm -f "$LOCK"; }
if [ -e "$LOCK" ]; then
  if ps -p "$(cut -d: -f1 <"$LOCK" 2>/dev/null || echo 0)" >/dev/null 2>&1; then
    info "git-sync déjà en cours (lock $LOCK) — je sors"
    exit 0
  fi
fi
echo "$$:$(date +%s)" > "$LOCK"
trap cleanup EXIT

# -------- env + repo --------
cd "$ROOT" || { ko "repo introuvable ($ROOT)"; exit 2; }
if [ -f /etc/scalp.env ]; then set -a; . /etc/scalp.env; set +a; fi

GIT_REPO="${GIT_REPO:-nicolasfoltzer77-tech/Scalp}"
REMOTE_URL="https://github.com/${GIT_REPO}.git"
if [ -n "${GIT_USER:-}" ] && [ -n "${GIT_TOKEN=[REDACTED] ]; then
  REMOTE_URL="https://${GIT_USER}:${GIT_TOKEN=[REDACTED]
fi
# pour l’affichage, on masque le token
SAFE_URL="${REMOTE_URL/${GIT_TOKEN=[REDACTED]
info "→ sync $BRANCH sur $SAFE_URL" | tee -a "$LOG"

# -------- fonction retry --------
retry(){ # retry <n> <cmd...>
  local n=$1; shift
  local i=1
  until "$@" 2>&1 | tee -a "$LOG"; do
    if (( i>=n )); then return 1; fi
    info "retry $i/$n dans 2s…" | tee -a "$LOG"
    sleep 2; ((i++))
  done
}

# -------- sync --------
git remote set-url origin "$REMOTE_URL" 2>&1 | tee -a "$LOG"
retry 3 git fetch origin

# commit auto si changements
if ! git diff --quiet || ! git diff --cached --quiet; then
  git add -A
  git commit -m "chore(sync): auto $(date -u +%F_%H:%MZ)" 2>&1 | tee -a "$LOG" || true
  ok "commit local créé" | tee -a "$LOG"
else
  info "aucune modif locale" | tee -a "$LOG"
fi

# pull (rebase+autostash), en cas d’échec on abort proprement
if ! git pull --rebase --autostash origin "$BRANCH" 2>&1 | tee -a "$LOG"; then
  git rebase --abort 2>/dev/null || true
  ko "conflit rebase — corrige puis relance" | tee -a "$LOG"
  exit 1
fi

# push
retry 3 git push -u origin "$BRANCH" || { ko "push KO"; exit 1; }
ok "push terminé" | tee -a "$LOG"
--- [61/191] ./tests/test_smoke.py ---
from pathlib import Path
import json

def test_health_ok():
    p = Path("docs/health.json"); assert p.exists()
    j = json.loads(p.read_text())
    assert j.get("status") == "ok"
    assert "generated_at" in j

def [REDACTED]():
    p = Path("docs/index.html")
    assert p.exists() and p.stat().st_size > 100--- [62/191] ./tests/test_health.py ---
from pathlib import Path
import json

def test_health_ok():
    p = Path("docs/health.json")
    j = json.loads(p.read_text())
    assert j.get("status") == "ok"
    assert "generated_at" in j

def [REDACTED]():
    p = Path("docs/index.html")
    assert p.exists() and p.stat().st_size > 100
--- [63/191] ./.pytest_cache/README.md ---
# pytest cache directory #

This directory contains data from the pytest's cache plugin,
which provides the `--lf` and `--ff` options, as well as the `cache` fixture.

**Do not** commit this to version control.

See [the docs](https://docs.pytest.org/en/stable/how-to/cache.html) for more information.
--- [64/191] ./scalper/live/watchlist.py ---
# -*- coding: utf-8 -*-
from __future__ import annotations
from dataclasses import dataclass
from typing import List


@dataclass
class WatchlistManager:
    symbols: List[str]

    @classmethod
    def from_env_or_default(cls) -> "WatchlistManager":
        # Tu peux lire une variable d'env ici si tu veux surcharger
        default = [
            "BTCUSDT","ETHUSDT","SOLUSDT","BNBUSDT","XRPUSDT",
            "DOGEUSDT","ADAUSDT","LTCUSDT","AVAXUSDT","LINKUSDT"
        ]
        return cls(default)--- [65/191] ./scalper/live/runner.py ---
# scalper/live/runner.py
from __future__ import annotations
from typing import Dict, List, Optional
from engine.signals.factory import resolve_signal_fn

class JobRunner:
    def __init__(self, strategies_cfg: dict, equity: float, risk_pct: float) -> None:
        self.cfg = strategies_cfg
        self.equity = float(equity)
        self.risk = float(risk_pct)

    def run_once(
        self, *, symbol: str, timeframe: str,
        ohlcv: Dict[str, List[float]],
        ohlcv_1h: Optional[Dict[str, List[float]]] = None
    ):
        fn = resolve_signal_fn(symbol, timeframe, self.cfg)
        return fn(
            symbol=symbol, timeframe=timeframe, ohlcv=ohlcv,
            equity=self.equity, risk_pct=self.risk, ohlcv_1h=ohlcv_1h
        )--- [66/191] ./scalper/live/backtest_telegram.py ---
# scalper/live/backtest_telegram.py
from __future__ import annotations

import asyncio
import os
from typing import List

from engine.backtest import BTCfg, run_multi
from engine.services.utils import safe_call

# Exchange CCXT asynchrone pour OHLCV publics (Bitget)
async def _get_exchange():
    try:
        import ccxt.async_support as ccxt  # type: ignore
    except Exception:
        raise RuntimeError("CCXT n'est pas installé. Lance: pip install ccxt")
    return ccxt.bitget()

def _parse_symbols(defaults: List[str]) -> List[str]:
    env = os.getenv("BACKTEST_SYMBOLS", "")
    if env.strip():
        return [s.strip().upper() for s in env.split(",") if s.strip()]
    return defaults

async def [REDACTED](notifier, defaults: List[str], timeframe: str = "5m") -> None:
    """Lancé par l'orchestrateur quand l'utilisateur tape /backtest sur Telegram."""
    symbols = _parse_symbols(defaults)
    cash = float(os.getenv("BT_CASH", "10000"))
    risk = float(os.getenv("BT_RISK_PCT", "0.05"))
    slip = float(os.getenv("BT_SLIPPAGE_BPS", "0.0"))
    limit = int(os.getenv("BT_LIMIT", "1500"))

    await notifier.send(
        "🧪 Backtest en cours...\n"
        f"• Symbols: {', '.join(symbols)}\n"
        f"• TF: {timeframe}\n"
        f"• Cash: {cash:,.0f}  • Risk: {risk:0.4f}  • Slippage: {slip:0.1f} bps\n"
        f"• Source: exchange.fetch_ohlcv (adapté) + cache CSV"
    )

    async def _run():
        exchange = await _get_exchange()
        try:
            cfg = BTCfg(symbols=symbols, timeframe=timeframe, cash=cash,
                        risk_pct=risk, slippage_bps=slip, limit=limit)
            res = await run_multi(cfg, exchange)
            await notifier.send(f"✅ Backtest terminé. Résultats: `{res['out_dir']}`")
        finally:
            try:
                await exchange.close()
            except Exception:
                pass

    try:
        await safe_call(_run, label="backtest", max_retry=1)  # 1 tir = si fail on avertit
    except Exception as e:
        await notifier.send(f"⚠️ Backtest : erreur inattendue: {e}")--- [67/191] ./scalper/live/ohlcv_service.py ---
from __future__ import annotations
import time
from typing import Any, Dict, List, Optional

try:
    from engine.adapters.market_data import MarketData
except Exception:
    MarketData = None  # type: ignore

class OhlcvService:
    """Lecture/normalisation OHLCV avec fallback agressifs."""
    def __init__(self, exchange) -> None:
        self.exchange = exchange
        self.md = MarketData(exchange) if MarketData is not None else None

    @staticmethod
    def normalize_rows(rows: Any) -> List[Dict[str, float]]:
        out: List[Dict[str, float]] = []
        if not rows: return out
        for r in rows:
            if isinstance(r, dict):
                ts = int(r.get("ts") or r.get("time") or r.get("timestamp") or 0)
                o = float(r.get("open", 0.0)); h = float(r.get("high", o)); l = float(r.get("low", o)); c = float(r.get("close", o))
                v = float(r.get("volume", r.get("vol", 0.0)))
            else:
                rr = list(r)
                if len(rr) >= 6 and isinstance(rr[0], (int, float)) and rr[0] > 10**10:
                    ts, o, h, l, c = int(rr[0]), float(rr[1]), float(rr[2]), float(rr[3]), float(rr[4]); v = float(rr[5])
                else:
                    o = float(rr[0]) if len(rr) > 0 else 0.0
                    h = float(rr[1]) if len(rr) > 1 else o
                    l = float(rr[2]) if len(rr) > 2 else o
                    c = float(rr[3]) if len(rr) > 3 else o
                    v = float(rr[4]) if len(rr) > 4 else 0.0
                    ts = int(rr[5]) if len(rr) > 5 else 0
            out.append({"ts": ts, "open": o, "high": h, "low": l, "close": c, "volume": v})
        return out

    async def fetch_once(self, symbol: str, interval: str = "1m", limit: int = 100) -> List[Dict[str, float]]:
        # 1) MarketData (si dispo)
        if self.md is not None:
            try:
                d = self.md.get_ohlcv(symbol, interval, limit)
                if isinstance(d, dict) and d.get("success") and d.get("data"):
                    return self.normalize_rows(d["data"])
            except Exception:
                pass

        # 2) Exchange natif
        rows: List[Any] = []
        try:
            data = self.exchange.get_kline(symbol, interval=interval)
        except Exception:
            data = None

        if isinstance(data, dict):
            rows = (
                data.get("data") or data.get("result") or data.get("records") or
                data.get("list") or data.get("items") or data.get("candles") or []
            )
            guard = 0
            while isinstance(rows, dict) and guard < 3:
                rows = (
                    rows.get("data") or rows.get("result") or rows.get("records") or
                    rows.get("list") or rows.get("items") or rows.get("candles") or rows.get("klines") or rows.get("bars") or []
                )
                guard += 1
        elif isinstance(data, (list, tuple)):
            rows = list(data)

        out = self.normalize_rows(rows)[-limit:]
        if out: return out

        # 3) Fallback strict via ticker -> bougie synthétique
        try:
            tkr = self.exchange.get_ticker(symbol)
            items = []
            if isinstance(tkr, dict): items = tkr.get("data") or tkr.get("result") or tkr.get("tickers") or []
            elif isinstance(tkr, (list, tuple)): items = list(tkr)
            if items:
                last = items[0]
                if isinstance(last, dict):
                    p = float(last.get("lastPrice", last.get("close", last.get("markPrice", 0.0))))
                    v = float(last.get("volume", last.get("usdtVolume", last.get("quoteVolume", 0.0))))
                else:
                    seq = list(last); p = float(seq[3] if len(seq) > 3 else seq[-2]); v = float(seq[4] if len(seq) > 4 else seq[-1])
                ts = int(time.time()*1000)
                return [{"ts": ts, "open": p, "high": p, "low": p, "close": p, "volume": v}]
        except Exception:
            pass
        return []--- [68/191] ./scalper/live/loops/trade.py ---
# scalp/live/loops/trade.py
from __future__ import annotations
import asyncio, os
from dataclasses import dataclass, field
from typing import Any, Dict, List, Callable

from ...services.utils import safe_call
from ...risk.manager import compute_size

QUIET = int(os.getenv("QUIET", "0") or "0")
PRINT_OHLCV_SAMPLE = int(os.getenv("PRINT_OHLCV_SAMPLE", "0") or "0")

class PositionFSM:
    def __init__(self):
        self.state = "FLAT"
        self.side = "flat"
        self.entry = 0.0
        self.qty = 0.0
    def can_open(self): return self.state == "FLAT"
    def on_open(self, side, entry, qty): self.state, self.side, self.entry, self.qty = "OPEN", side, entry, qty
    def can_close(self): return self.state == "OPEN"
    def on_close(self): self.state, self.side, self.entry, self.qty = "FLAT", "flat", 0.0, 0.0

@dataclass
class SymbolContext:
    symbol: str
    timeframe: str
    ohlcv: List[List[float]] = field(default_factory=list)
    ticks: int = 0
    fsm: PositionFSM = field(default_factory=PositionFSM)

class TradeLoop:
    """
    Boucle par symbole, indépendante de l'orchestrateur.
    """
    def __init__(
        self,
        symbol: str,
        timeframe: str,
        ohlcv_fetch: Callable[..., Any],           # async fn(symbol, timeframe, limit) -> ohlcv
        order_market: Callable[..., Any],          # async fn(symbol, side, qty) -> order dict
        generate_signal: Callable[[List[List[float]], Dict[str, Any]], Dict[str, Any]],
        config: Dict[str, Any],
        mode_getter: Callable[[], str],
        log_signals, log_orders, log_fills,
        tick_counter_add: Callable[[int], None],
    ):
        self.symbol = symbol
        self.timeframe = timeframe
        self.fetch = ohlcv_fetch
        self.order_market = order_market
        self.generate_signal = generate_signal
        self.config = config
        self.get_mode = mode_getter
        self.log_signals = log_signals
        self.log_orders = log_orders
        self.log_fills = log_fills
        self.ctx = SymbolContext(symbol, timeframe)
        self._tick_add = tick_counter_add

        # Risk/frais
        self.risk_pct = float(self.config.get("risk_pct", 0.5))
        self.caps_by_symbol = self.config.get("caps", {})  # dict optionnel
        self.fees_map = self.config.get("fees_by_symbol", {})  # {sym: {"maker_bps":..., "taker_bps":...}}
        self.slippage_bps = float(self.config.get("slippage_bps", 0.0))

    def _bps_for(self, order_type: str = "market") -> float:
        # market -> taker; limit post-only -> maker
        per = self.fees_map.get(self.symbol, {})
        if order_type == "limit":
            return float(per.get("maker_bps", 0.0))
        return float(per.get("taker_bps", 0.0))

    async def run(self, running: Callable[[], bool]):
        lookback = 200
        while running():
            if self.get_mode() != "RUNNING":
                await asyncio.sleep(0.5); continue

            async def _fetch():
                return await self.fetch(self.symbol, timeframe=self.timeframe, limit=lookback+2)
            ohlcv = await safe_call(_fetch, label=f"fetch_ohlcv:{self.symbol}")
            if not ohlcv or len(ohlcv) < lookback+1:
                await asyncio.sleep(1.0); continue

            self.ctx.ohlcv = ohlcv
            self.ctx.ticks += 1
            self._tick_add(1)

            window = ohlcv[-(lookback+1):]
            ts, _o, _h, _l, c, _v = window[-1]

            try:
                sig = self.generate_signal(window, self.config) or {}
            except Exception as e:
                if not QUIET:
                    print(f"[trade:{self.symbol}] generate_signal error: {e}", flush=True)
                await asyncio.sleep(0.5); continue

            side = sig.get("side","flat"); entry = float(sig.get("entry", c)); sl = sig.get("sl"); tp = sig.get("tp")
            self.log_signals.write_row({"ts": ts, "symbol": self.symbol, "side": side, "entry": entry, "sl": sl, "tp": tp, "last": c})

            # --- Entrée (market -> taker)
            if self.ctx.fsm.state == "FLAT" and side in ("long","short"):
                balance = float(self.config.get("cash", 10_000.0))
                qty = compute_size(
                    symbol=self.symbol, price=entry or c, balance_cash=balance,
                    risk_pct=self.risk_pct, caps_by_symbol=self.caps_by_symbol
                )
                if qty > 0:
                    async def _place():
                        return await self.order_market(self.symbol, side, qty)
                    order = await safe_call(_place, label=f"order:{self.symbol}")
                    self.ctx.fsm.on_open(side, entry or c, qty)
                    self.log_orders.write_row({"ts": ts, "symbol": self.symbol, "side": side, "qty": qty,
                                               "status": "placed", "order_id": (order or {}).get("id",""), "note": f"entry taker={self._bps_for('market')}bps"})

            # --- Sortie (market -> taker)
            elif self.ctx.fsm.state == "OPEN" and (side == "flat" or (side in ("long","short") and side != self.ctx.fsm.side)):
                qty = self.ctx.fsm.qty
                exit_side = "sell" if self.ctx.fsm.side == "long" else "buy"
                async def _close():
                    return await self.order_market(self.symbol, exit_side, qty)
                order = await safe_call(_close, label=f"close:{self.symbol}")

                # fill avec slippage + frais (taker)
                price_fill = float(c)
                price_fill *= (1 + (self.slippage_bps/10000.0)) if exit_side == "buy" else (1 - (self.slippage_bps/10000.0))
                self.log_orders.write_row({"ts": ts, "symbol": self.symbol, "side": exit_side, "qty": qty,
                                           "status": "placed", "order_id": (order or {}).get("id",""), "note": f"exit taker={self._bps_for('market')}bps"})
                self.log_fills.write_row({"ts": ts, "symbol": self.symbol, "side": exit_side, "price": price_fill, "qty": qty,
                                          "order_id": (order or {}).get("id","")})
                self.ctx.fsm.on_close()

            if PRINT_OHLCV_SAMPLE and (self.ctx.ticks % 20 == 0) and not QUIET:
                print(f"[{self.symbol}] last={c} ticks={self.ctx.ticks}", flush=True)

            await asyncio.sleep(0.1 if QUIET else 0.01)--- [69/191] ./scalper/live/state_store.py ---
# live/state_store.py
from __future__ import annotations
import json, os, time, asyncio
from typing import Callable, Dict, Any

class StateStore:
    """
    Persistance légère de l'état (FSM + horodatages) dans un JSON.
    - save_state(snapshot: dict) -> écrit sur disque
    - load_state() -> dict
    - task_autosave(get_snapshot: callable) -> boucle d’auto‑save
    """

    def __init__(self, filepath: str, period_s: float = 10.0) -> None:
        self.filepath = filepath
        self.period_s = period_s
        os.makedirs(os.path.dirname(self.filepath), exist_ok=True)
        self._running = False

    # -------- I/O --------
    def save_state(self, snapshot: Dict[str, Any]) -> None:
        tmp = self.filepath + ".tmp"
        with open(tmp, "w", encoding="utf-8") as f:
            json.dump(snapshot, f, ensure_ascii=False, indent=2)
        os.replace(tmp, self.filepath)

    def load_state(self) -> Dict[str, Any]:
        if not os.path.exists(self.filepath):
            return {}
        try:
            with open(self.filepath, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return {}

    # -------- Autosave --------
    async def task_autosave(self, get_snapshot: Callable[[], Dict[str, Any]]):
        self._running = True
        while self._running:
            try:
                snap = get_snapshot()
                snap["saved_at"] = int(time.time() * 1000)
                self.save_state(snap)
            except Exception:
                pass
            await asyncio.sleep(self.period_s)

    def stop(self): self._running = False--- [70/191] ./scalper/live/journal.py ---
from __future__ import annotations
import os, csv
from typing import Any, Dict, List

class LogWriter:
    """Gestion simple des CSV (création à la volée + append)."""
    def __init__(self, dirpath: str) -> None:
        self.dir = dirpath
        os.makedirs(self.dir, exist_ok=True)

    def init(self, fname: str, headers: List[str]) -> None:
        p = os.path.join(self.dir, fname)
        if not os.path.exists(p):
            with open(p, "w", newline="", encoding="utf-8") as f:
                csv.DictWriter(f, fieldnames=headers).writeheader()

    def row(self, fname: str, row: Dict[str, Any]) -> None:
        p = os.path.join(self.dir, fname)
        with open(p, "a", newline="", encoding="utf-8") as f:
            csv.DictWriter(f, fieldnames=list(row.keys())).writerow(row)--- [71/191] ./scalper/live/commands.py ---
# scalper/live/commands.py
from __future__ import annotations

import asyncio
from typing import Awaitable, Callable


class CommandHandler:
    """
    Gère les commandes reçues d'un CommandStream (Telegram ou Null).
    Chaque commande est routée vers un callback approprié.
    Les erreurs de callbacks sont capturées pour ne pas tuer l'orchestrateur.
    """

    def __init__(self, notifier, command_stream, status_getter, status_sender):
        self.notifier = notifier
        self.stream = command_stream
        self.status_getter = status_getter
        self.status_sender = status_sender

    async def _safe_call(self, coro: Awaitable[None], err_msg: str) -> None:
        try:
            await coro
        except Exception as e:
            try:
                await self.notifier.send(f"⚠️ {err_msg}: {e}")
            except Exception:
                pass  # on ne propage jamais

    async def run(
        self,
        on_pause: Callable[[], None],
        on_resume: Callable[[], None],
        on_stop: Callable[[], Awaitable[None]] | None,
        on_setup_apply: Callable[[dict], None],
        on_backtest: Callable[[str], Awaitable[None]] | None = None,
    ):
        """
        Boucle asynchrone qui lit les lignes du CommandStream
        et exécute le callback approprié.
        TOUTE exception de callback est absorbée pour ne pas terminer cette task.
        """
        async for line in self.stream:
            txt = (line or "").strip()
            if not txt:
                continue

            try:
                if txt.startswith("/pause"):
                    on_pause()
                    await self.notifier.send("⏸️ Pause.")

                elif txt.startswith("/resume"):
                    on_resume()
                    await self.notifier.send("▶️ Resume.")

                elif txt.startswith("/stop"):
                    if on_stop:
                        await self._safe_call(on_stop(), "Arrêt échoué")

                elif txt.startswith("/status"):
                    snap = self.status_getter()
                    await self.notifier.send(f"ℹ️ {snap}")

                elif txt.startswith("/setup"):
                    await self.notifier.send("🧩 Setup wizard à compléter.")

                elif txt.startswith("/backtest"):
                    if on_backtest:
                        tail = txt[len("/backtest"):].strip()
                        # IMPORTANT : on ne bloque PAS la boucle de commandes.
                        asyncio.create_task(self._safe_call(
                            on_backtest(tail), "Backtest échoué"
                        ))
                        await self.notifier.send("🧪 Backtest lancé en tâche de fond.")
                    else:
                        await self.notifier.send("⚠️ Backtest non disponible.")

                else:
                    await self.notifier.send(
                        "❓ Commandes: /status /pause /resume /stop /setup /backtest"
                    )

            except Exception as e:
                # On protège la boucle quoi qu'il arrive
                try:
                    await self.notifier.send(f"⚠️ Erreur commande: {e}")
                except Exception:
                    pass--- [72/191] ./scalper/live/logs.py ---
# scalp/live/logs.py
from __future__ import annotations
import os, csv
from typing import Any, List, Dict

class CsvLog:
    def __init__(self, path: str, headers: List[str]):
        self.path = path
        self.headers = headers
        self._ensure_header()

    def _ensure_header(self):
        must_write = not os.path.exists(self.path) or os.path.getsize(self.path) == 0
        os.makedirs(os.path.dirname(self.path), exist_ok=True)
        if must_write:
            with open(self.path, "w", newline="") as f:
                csv.writer(f).writerow(self.headers)

    def write_row(self, row: Dict[str, Any]):
        with open(self.path, "a", newline="") as f:
            w = csv.DictWriter(f, fieldnames=self.headers)
            w.writerow({k: row.get(k, "") for k in self.headers})--- [73/191] ./scalper/live/data_utils.py ---
# scalper/live/data_utils.py
from __future__ import annotations
from typing import Dict, List, Sequence

Cols = ("timestamp", "open", "high", "low", "close", "volume")

def ohlcv_rows_to_dict(rows: Sequence[Sequence[float]]) -> Dict[str, List[float]]:
    """
    Convertit [[ts,o,h,l,c,v], ...] -> dict de listes.
    Tolère float|int|str numériques.
    """
    out: Dict[str, List[float]] = {k: [] for k in Cols}
    for r in rows:
        if len(r) < 6:
            raise ValueError("Ligne OHLCV invalide (6 colonnes attendues).")
        out["timestamp"].append(float(r[0]))
        out["open"].append(float(r[1]))
        out["high"].append(float(r[2]))
        out["low"].append(float(r[3]))
        out["close"].append(float(r[4]))
        out["volume"].append(float(r[5]))
    return out

def [REDACTED](obj) -> Dict[str, List[float]]:
    """
    Accepte:
      - pandas.DataFrame avec colonnes Cols
      - dict de listes
    """
    if hasattr(obj, "columns"):
        missing = [c for c in Cols if c not in obj.columns]
        if missing:
            raise ValueError(f"Colonnes OHLCV manquantes: {missing}")
        return {k: [float(x) for x in obj[k].tolist()] for k in Cols}
    if isinstance(obj, dict):
        missing = [c for c in Cols if c not in obj]
        if missing:
            raise ValueError(f"Clés OHLCV manquantes: {missing}")
        return {k: [float(x) for x in obj[k]] for k in Cols}
    raise TypeError("Format OHLCV non supporté (DataFrame ou dict attendu).")

def map_index_secondary(ts_main: float, ts_arr: List[float]) -> int:
    """
    Retourne l'index i du timestamp secondaire le plus proche
    inférieur/égal à ts_main. Recherche linéaire suffisante en live.
    """
    j = 0
    n = len(ts_arr)
    while j + 1 < n and ts_arr[j + 1] <= ts_main:
        j += 1
    return j--- [74/191] ./scalper/live/orders.py ---
# live/orders.py
from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Optional

from engine.services.order_service import OrderService, OrderRequest

@dataclass
class OrderResult:
    accepted: bool
    order_id: str | None = None
    status: str | None = None
    reason: str | None = None

class OrderExecutor:
    """
    Fine couche autour d'OrderService + exchange :
      - calcule l'équité USDT
      - place une entrée (risk_pct)
      - récupère les fills (normalisés)
    L'orchestrateur n’appelle plus OrderService directement.
    """

    def __init__(self, order_service: OrderService, exchange: Any, config: Any) -> None:
        self.order_service = order_service
        self.exchange = exchange
        self.config = config

    # ---------- Equity ----------
    def get_equity_usdt(self) -> float:
        equity = 0.0
        try:
            assets = self.exchange.get_assets()
            if isinstance(assets, dict):
                for a in (assets.get("data") or []):
                    if str(a.get("currency")).upper() == "USDT":
                        equity = float(a.get("equity", 0.0))
                        break
        except Exception:
            pass
        return equity

    # ---------- Entrée ----------
    def place_entry(self, *, symbol: str, side: str, price: float,
                    sl: float | None, tp: float | None, risk_pct: float) -> OrderResult:
        """
        side: 'long' | 'short'
        Retourne OrderResult(accepted, order_id, status, reason)
        """
        equity = self.get_equity_usdt()
        req = OrderRequest(symbol=symbol, side=side, price=float(price),
                           sl=(float(sl) if sl else None), tp=(float(tp) if tp else None),
                           risk_pct=float(risk_pct))
        try:
            res = self.order_service.prepare_and_place(equity, req)
            return OrderResult(accepted=bool(getattr(res, "accepted", False)),
                               order_id=getattr(res, "order_id", None),
                               status=getattr(res, "status", None),
                               reason=getattr(res, "reason", None))
        except Exception as e:
            return OrderResult(accepted=False, reason=str(e))

    # ---------- Fills ----------
    def fetch_fills(self, symbol: str, order_id: str | None, limit: int = 50) -> list[dict]:
        """
        Normalise le format en liste de dicts {orderId, tradeId, price, qty, fee}
        """
        try:
            raw = self.exchange.get_fills(symbol, order_id, limit)
        except Exception:
            return []

        items: list = []
        if isinstance(raw, dict):
            items = raw.get("data") or raw.get("result") or raw.get("fills") or []
        elif isinstance(raw, (list, tuple)):
            items = list(raw)

        out: list[dict] = []
        for f in items:
            if isinstance(f, dict):
                out.append({
                    "orderId": f.get("orderId") or f.get("order_id") or "",
                    "tradeId": f.get("tradeId") or f.get("trade_id") or "",
                    "price": float(f.get("price", f.get("fillPrice", 0.0)) or 0.0),
                    "qty": float(f.get("qty", f.get("size", f.get("fillQty", 0.0))) or 0.0),
                    "fee": float(f.get("fee", f.get("fillFee", 0.0)) or 0.0),
                })
            else:
                try:
                    seq = list(f)
                    out.append({
                        "orderId": str(seq[0]) if seq else "",
                        "tradeId": str(seq[1]) if len(seq) > 1 else "",
                        "price": float(seq[2]) if len(seq) > 2 else 0.0,
                        "qty": float(seq[3]) if len(seq) > 3 else 0.0,
                        "fee": float(seq[4]) if len(seq) > 4 else 0.0,
                    })
                except Exception:
                    continue
        return out--- [75/191] ./scalper/live/orchestrator.py ---
# scalper/live/orchestrator.py
from __future__ import annotations
import time
from typing import List, Tuple, Dict, Any
from engine.live.fetcher import DataFetcher
from engine.live.runner import JobRunner

class Orchestrator:
    def __init__(
        self,
        *,
        exchange_client: Any,
        strategies_cfg: Dict[str, Any],
        jobs: List[Tuple[str, str]],   # [(symbol, timeframe)]
        interval_sec: int = 60,
        equity: float = 1000.0,
        risk_pct: float = 0.01,
    ) -> None:
        self.fetcher = DataFetcher(exchange_client)
        self.runner = JobRunner(strategies_cfg, equity, risk_pct)
        self.jobs = [(s.upper(), tf) for s, tf in jobs]
        self.interval = max(5, int(interval_sec))

    def _tick(self) -> None:
        for symbol, tf in self.jobs:
            try:
                data = self.fetcher.fetch(symbol, tf)
                data_1h = self.fetcher.try_fetch_1h(symbol)
                sig = self.runner.run_once(symbol=symbol, timeframe=tf, ohlcv=data, ohlcv_1h=data_1h)
                if sig is None:
                    print(f"[{symbol}/{tf}] Aucun signal.")
                else:
                    d = sig.as_dict()
                    print(f"[{symbol}/{tf}] side={d['side']} entry={d['entry']:.6f} "
                          f"sl={d['sl']:.6f} tp1={d['tp1']:.6f} tp2={d['tp2']:.6f} "
                          f"score={d['score']} q={d['quality']:.2f} :: {d.get('reasons','')}")
            except Exception as e:
                print(f"[{symbol}/{tf}] ERREUR: {e}")

    def loop(self) -> None:
        print(f"[Orchestrator] jobs={self.jobs} interval={self.interval}s")
        while True:
            t0 = time.time()
            self._tick()
            dt = time.time() - t0
            time.sleep(max(0.0, self.interval - dt))--- [76/191] ./scalper/live/fetcher.py ---
# scalper/live/fetcher.py
from __future__ import annotations
from typing import Dict, List, Optional, Any

class DataFetcher:
    """
    Récupération OHLCV depuis un client d'exchange.
    Compatible:
      - Wrapper custom: client.get_ohlcv(symbol, timeframe, limit)
      - ccxt direct:    client.fetch_ohlcv(symbol, timeframe=..., limit=...)
    Retour standardisé: dict[str, list[float]] avec clés:
      timestamp, open, high, low, close, volume
    """
    def __init__(self, client: Any) -> None:
        self.client = client
        # Détection des méthodes disponibles
        self._has_get = hasattr(client, "get_ohlcv")
        self._has_fetch = hasattr(client, "fetch_ohlcv")

        if not (self._has_get or self._has_fetch):
            raise AttributeError(
                "Le client exchange doit exposer get_ohlcv(...) ou fetch_ohlcv(...). "
                "Ex: wrapper custom ou objet ccxt.bitget."
            )

    @staticmethod
    def _to_dict(rows: List[List[float]]) -> Dict[str, List[float]]:
        cols = ("timestamp", "open", "high", "low", "close", "volume")
        out = {k: [] for k in cols}
        for r in rows:
            # rows: [ts, open, high, low, close, volume]
            out["timestamp"].append(float(r[0]))
            out["open"].append(float(r[1]))
            out["high"].append(float(r[2]))
            out["low"].append(float(r[3]))
            out["close"].append(float(r[4]))
            out["volume"].append(float(r[5]))
        return out

    def fetch(self, symbol: str, timeframe: str, limit: int = 1500) -> Dict[str, List[float]]:
        if self._has_get:
            rows = self.client.get_ohlcv(symbol=symbol, timeframe=timeframe, limit=limit)
        else:
            # ccxt: fetch_ohlcv(symbol, timeframe=..., limit=...)
            rows = self.client.fetch_ohlcv(symbol, timeframe=timeframe, limit=limit)
        return self._to_dict(rows)

    def try_fetch_1h(self, symbol: str, limit: int = 1500) -> Optional[Dict[str, List[float]]]:
        try:
            if self._has_get:
                rows = self.client.get_ohlcv(symbol=symbol, timeframe="1h", limit=limit)
            else:
                rows = self.client.fetch_ohlcv(symbol, timeframe="1h", limit=limit)
            return self._to_dict(rows)
        except Exception:
            return None--- [77/191] ./scalper/live/.ipynb_checkpoints/notify-checkpoint.py ---
# -*- coding: utf-8 -*-
from __future__ import annotations
import os
import asyncio
from dataclasses import dataclass
from typing import AsyncIterator, Optional


@dataclass
class BaseNotifier:
    async def send(self, text: str) -> None:  # pragma: no cover
        print(text)


class NullNotifier(BaseNotifier):
    pass


class TelegramNotifier(BaseNotifier):
    def __init__(self, token: [REDACTED] chat_id: str, session: Optional[asyncio.AbstractEventLoop]=None):
        import aiohttp  # lazy
        self._token = [REDACTED]
        self._chat = chat_id
        self._session: aiohttp.ClientSession | None = None

    async def _ensure(self):
        import aiohttp
        if self._session is None or self._session.closed:
            self._session = aiohttp.ClientSession()

    async def send(self, text: str) -> None:
        import aiohttp
        await self._ensure()
        # pas de markdown pour éviter les erreurs 400 de parsing
        url = f"https://api.telegram.org/bot{self._token}/sendMessage"
        payload = {"chat_id": self._chat, "text": text, "[REDACTED]": True}
        try:
            async with self._session.post(url, json=payload, timeout=20) as r:
                await r.text()  # on ignore la réponse pour rester simple
        except Exception:
            # on fait un fallback silencieux pour ne pas casser le bot
            print("[notify:telegram] send fail (ignored)")

    async def close(self):
        if self._session and not self._session.closed:
            await self._session.close()


class _NullCommands:
    """Itérateur async vide utilisé quand Telegram n'est pas configuré."""
    def __aiter__(self) -> AsyncIterator[str]:
        return self
    async def __anext__(self) -> str:
        await asyncio.sleep(3600)  # jamais
        raise StopAsyncIteration


async def [REDACTED](config: dict) -> tuple[BaseNotifier, AsyncIterator[str]]:
    """
    Retourne (notifier, command_stream).

    - Si TELEGRAM_BOT_TOKEN=[REDACTED] et TELEGRAM_CHAT_ID sont présents: TelegramNotifier,
      et un flux (vide) – l’orchestreur n’en a besoin que si on implémente des
      commandes interactives plus tard.
    - Sinon: NullNotifier + flux vide.
    """
    token = [REDACTED]"TELEGRAM_BOT_TOKEN=[REDACTED]
    chat = os.getenv("TELEGRAM_CHAT_ID")
    if token and chat:
        print("[notify] TELEGRAM configured.")
        return TelegramNotifier(token, chat), _NullCommands()
    print("[notify] TELEGRAM not configured -> Null notifier will be used.")
    return NullNotifier(), _NullCommands()--- [78/191] ./scalper/live/.ipynb_checkpoints/runner-checkpoint.py ---
# scalper/live/runner.py
from __future__ import annotations
from typing import Dict, List, Optional
from engine.signals.factory import resolve_signal_fn

class JobRunner:
    def __init__(self, strategies_cfg: dict, equity: float, risk_pct: float) -> None:
        self.cfg = strategies_cfg
        self.equity = float(equity)
        self.risk = float(risk_pct)

    def run_once(
        self, *, symbol: str, timeframe: str,
        ohlcv: Dict[str, List[float]],
        ohlcv_1h: Optional[Dict[str, List[float]]] = None
    ):
        fn = resolve_signal_fn(symbol, timeframe, self.cfg)
        return fn(
            symbol=symbol, timeframe=timeframe, ohlcv=ohlcv,
            equity=self.equity, risk_pct=self.risk, ohlcv_1h=ohlcv_1h
        )--- [79/191] ./scalper/live/position_fsm.py ---
# live/position_fsm.py
from __future__ import annotations
from dataclasses import dataclass
from typing import Optional, Dict, Any, List


STATE_FLAT = "FLAT"
STATE_PENDING_ENTRY = "PENDING_ENTRY"
STATE_OPEN = "OPEN"
STATE_PENDING_EXIT = "PENDING_EXIT"


@dataclass
class PositionState:
    symbol: str
    state: str = STATE_FLAT
    order_id: Optional[str] = None
    side: Optional[str] = None   # "long" | "short"
    qty: float = 0.0
    entry: float = 0.0


class PositionFSM:
    """
    FSM ultra-simple par symbole.
    - set_pending_entry(order_id, side)
    - reconcile(open_positions, fills) -> met à jour l'état à partir des données Bitget
    """

    def __init__(self, symbols: List[str]) -> None:
        self._by_symbol: Dict[str, PositionState] = {s: PositionState(s) for s in symbols}

    # -------- API utilisateur --------
    def ensure_symbol(self, symbol: str) -> None:
        if symbol not in self._by_symbol:
            self._by_symbol[symbol] = PositionState(symbol)

    def set_pending_entry(self, symbol: str, order_id: str, side: str) -> None:
        self.ensure_symbol(symbol)
        st = self._by_symbol[symbol]
        st.state = STATE_PENDING_ENTRY
        st.order_id = order_id
        st.side = side

    def mark_pending_exit(self, symbol: str) -> None:
        self.ensure_symbol(symbol)
        st = self._by_symbol[symbol]
        st.state = STATE_PENDING_EXIT

    def force_flat(self, symbol: str) -> None:
        self._by_symbol[symbol] = PositionState(symbol)

    # -------- Lecture --------
    def get(self, symbol: str) -> PositionState:
        self.ensure_symbol(symbol)
        return self._by_symbol[symbol]

    def all(self) -> Dict[str, PositionState]:
        return self._by_symbol

    # -------- Réconciliation --------
    def reconcile(self, open_positions: List[Dict[str, Any]], fills: Dict[str, List[Dict[str, Any]]]) -> None:
        """
        open_positions: liste [{symbol, side, qty, avgEntryPrice}]
        fills: dict symbol -> liste de fills [{orderId, price, qty, ...}]
        """
        # indexer positions ouvertes
        idx_open = {p["symbol"]: p for p in open_positions if float(p.get("qty", 0.0)) > 0.0}

        for sym, st in self._by_symbol.items():
            p = idx_open.get(sym)

            if st.state == STATE_PENDING_ENTRY:
                # si on voit des fills de l'ordre en attente -> OPEN
                f_list = fills.get(sym) or []
                qty_filled = sum(float(f.get("qty", 0.0)) for f in f_list if not st.order_id or str(f.get("orderId")) == str(st.order_id))
                if qty_filled > 0.0 or p:
                    st.state = STATE_OPEN
                    st.qty = float(p.get("qty", qty_filled)) if p else qty_filled
                    st.entry = float(p.get("avgEntryPrice", f_list[0].get("price", 0.0) if f_list else 0.0)) if p else \
                               float(f_list[0].get("price", 0.0)) if f_list else 0.0
            elif st.state == STATE_OPEN:
                # si plus de position ouverte -> FLAT
                if not p:
                    st.state = STATE_FLAT
                    st.order_id = None
                    st.side = None
                    st.qty = 0.0
                    st.entry = 0.0
                else:
                    st.qty = float(p.get("qty", st.qty))
                    st.entry = float(p.get("avgEntryPrice", st.entry))
            elif st.state == STATE_PENDING_EXIT:
                # si plus de position -> FLAT ; sinon reste OPEN
                if not p:
                    st.state = STATE_FLAT
                    st.order_id = None
                    st.side = None
                    st.qty = 0.0
                    st.entry = 0.0
                else:
                    st.state = STATE_OPEN  # pas encore clos
            else:
                # FLAT: si une position apparaît (cas reboot) -> OPEN
                if p:
                    st.state = STATE_OPEN
                    st.qty = float(p.get("qty", 0.0))
                    st.entry = float(p.get("avgEntryPrice", 0.0))--- [80/191] ./scalper/live/__init__.py ---
__all__ = ["orchestrator", "fetcher", "runner"]--- [81/191] ./scalper/live/setup_wizard.py ---
from __future__ import annotations
import asyncio, os
from dataclasses import dataclass
from typing import List, Dict, Optional, Callable
from ..signals.factory import load_signal
from ..backtest.runner import BacktestRunner
from .notify import Notifier, CommandStream

@dataclass
class SetupResult:
    strategy: str
    symbols: List[str]
    timeframes: List[str]
    risk_pct: float
    accepted: bool
    summary_path: str

class SetupWizard:
    """
    Wizard interactif Telegram avant lancement des trades.
    Utilise Notifier (send/send_menu) + CommandStream (async iterator).
    """
    def __init__(self, notifier: Notifier, cmd_stream: CommandStream,
                 ohlcv_loader_sync: Callable, out_dir: str = "out_bt_setup",
                 admin_chat_id: Optional[int]=None):
        self.notifier = notifier
        self.cmd_stream = cmd_stream
        self.loader = ohlcv_loader_sync
        self.out_dir = out_dir
        self.admin_chat_id = admin_chat_id

    async def _ask_list(self, prompt: str, choices: List[str], allow_multi=True) -> List[str]:
        await self.notifier.send_menu(prompt, choices)
        async for msg in self.cmd_stream:
            txt = msg.strip()
            if allow_multi and ("," in txt or " " in txt):
                sel = [t.strip() for t in txt.replace(" ", "").split(",") if t.strip()]
                return sel
            if txt.isdigit():
                i = int(txt)-1
                if 0 <= i < len(choices):
                    return [choices[i]]
            if txt in choices:
                return [txt]
            await self.notifier.send("Entrée invalide. Réessaie.")

    async def _ask_value(self, prompt: str, cast: Callable, default):
        await self.notifier.send(f"{prompt} (défaut: {default})")
        async for msg in self.cmd_stream:
            txt = msg.strip()
            if txt == "" or txt.lower() in ("d","defaut","default"):
                return default
            try:
                return cast(txt)
            except Exception:
                await self.notifier.send("Entrée invalide. Réessaie.")

    async def run(self, default_symbols: List[str], default_timeframes: List[str],
                  default_strategy: str="current") -> SetupResult:
        await self.notifier.send("🧪 Validation avant trading : choix strat/symbols/TF → backtest → validation.")
        # 1) stratégie
        strategies = ["current","ema_cross","vwap_break"]
        [strategy] = await self._ask_list("Choisis la stratégie :", strategies, allow_multi=False)

        # 2) symboles
        symbols = await self._ask_list("Sélectionne les symboles :", default_symbols, allow_multi=True)

        # 3) timeframes
        timeframes = await self._ask_list("Sélectionne les timeframes :", default_timeframes, allow_multi=True)

        # 4) risk %
        risk_pct = await self._ask_value("Risk % du solde (ex: 0.5 = 50%)", float, 0.5)

        # 5) période backtest
        start = await self._ask_value("Date de début (YYYY-MM-DD)", str, "2024-01-01")
        end   = await self._ask_value("Date de fin   (YYYY-MM-DD)", str, "2025-08-01")

        # 6) run backtest
        from ..backtest.cli import parse_ts
        start_ms, end_ms = parse_ts(start), parse_ts(end)
        runner = BacktestRunner(self.loader, self.out_dir, strategy,
                                cfg={}, cash=10_000.0, risk_pct=risk_pct, max_conc=6)
        res = await runner.run_all(symbols, timeframes, start_ms, end_ms)

        # 7) résumé
        sum_path = os.path.join(self.out_dir, "metrics.json")
        prop = res["proposal"]
        lines = ["**Proposition** :"]
        for sym, best in prop["per_symbol_best"].items():
            lines.append(f"• {sym}: {best['timeframe']}  score={best['score']:.3f}  PF={best['pf']:.2f}  WR={best['winrate']:.1%}  DD={best['maxdd']:.1%}")
        await self.notifier.send("\n".join(lines) + f"\nFichier: {sum_path}\n✅ Tape **ACCEPTER** pour lancer\n🔁 **MODIFIER** pour relancer\n❌ **ANNULER** pour quitter.")

        # 8) décision
        async for msg in self.cmd_stream:
            t = msg.strip().lower()
            if t in ("accepter","accept","ok","go","start"):
                await self.notifier.send("✅ Validation reçue — passage en RUNNING.")
                return SetupResult(strategy, symbols, timeframes, risk_pct, True, sum_path)
            if t in ("modifier","again","repeat"):
                return await self.run(default_symbols, default_timeframes, default_strategy=strategy)
            if t in ("annuler","cancel","stop"):
                await self.notifier.send("❌ Annulé.")
                return SetupResult(strategy, symbols, timeframes, risk_pct, False, sum_path)--- [82/191] ./scalper/live/telegram_async.py ---
from __future__ import annotations
import time
import requests
import asyncio
from typing import Optional, Dict, Any, List


class TelegramAsync:
    """
    Client Telegram simple basé sur requests, utilisé de manière non bloquante via asyncio.to_thread.
    Sans nouvelle dépendance.
    """
    def __init__(self, token: [REDACTED] chat_id: Optional[str]) -> None:
        self.token = [REDACTED]
        self.chat_id = chat_id
        self.base = f"https://api.telegram.org/bot{token}" if token else None
        self._offset = 0
        self._enabled = bool(token and chat_id)

    def enabled(self) -> bool:
        return self._enabled

    # ---------- sync I/O (appelées via to_thread) ----------
    def _send_message_sync(self, text: str) -> Dict[str, Any]:
        if not self._enabled:
            return {"ok": False, "reason": "disabled"}
        url = f"{self.base}/sendMessage"
        payload = {"chat_id": self.chat_id, "text": text}
        try:
            r = requests.post(url, json=payload, timeout=10)
            return r.json()
        except Exception as e:
            return {"ok": False, "error": repr(e)}

    def _get_updates_sync(self, timeout_s: int = 30) -> Dict[str, Any]:
        if not self._enabled:
            return {"ok": True, "result": []}
        url = f"{self.base}/getUpdates"
        params = {"timeout": timeout_s, "offset": self._offset}
        try:
            r = requests.get(url, params=params, timeout=timeout_s + 5)
            return r.json()
        except Exception as e:
            return {"ok": False, "error": repr(e), "result": []}

    # ---------- async wrappers ----------
    async def send_message(self, text: str) -> None:
        await asyncio.to_thread(self._send_message_sync, text)

    async def poll_commands(self, timeout_s: int = 30) -> List[Dict[str, Any]]:
        data = await asyncio.to_thread(self._get_updates_sync, timeout_s)
        if not data.get("ok"):
            return []
        out = []
        for upd in data.get("result", []):
            self._offset = max(self._offset, int(upd.get("update_id", 0)) + 1)
            msg = upd.get("message") or {}
            text = (msg.get("text") or "").strip()
            if not text:
                continue
            out.append({
                "date": msg.get("date"),
                "chat": str((msg.get("chat") or {}).get("id")),
                "text": text,
                "from": (msg.get("from") or {}).get("username") or (msg.get("from") or {}).get("first_name") or "unknown",
            })
        return out
--- [83/191] ./scalper/live/notify.py ---
# -*- coding: utf-8 -*-
from __future__ import annotations
import os
import asyncio
from dataclasses import dataclass
from typing import AsyncIterator, Optional


@dataclass
class BaseNotifier:
    async def send(self, text: str) -> None:  # pragma: no cover
        print(text)


class NullNotifier(BaseNotifier):
    pass


class TelegramNotifier(BaseNotifier):
    def __init__(self, token: [REDACTED] chat_id: str, session: Optional[asyncio.AbstractEventLoop]=None):
        import aiohttp  # lazy
        self._token = [REDACTED]
        self._chat = chat_id
        self._session: aiohttp.ClientSession | None = None

    async def _ensure(self):
        import aiohttp
        if self._session is None or self._session.closed:
            self._session = aiohttp.ClientSession()

    async def send(self, text: str) -> None:
        import aiohttp
        await self._ensure()
        # pas de markdown pour éviter les erreurs 400 de parsing
        url = f"https://api.telegram.org/bot{self._token}/sendMessage"
        payload = {"chat_id": self._chat, "text": text, "[REDACTED]": True}
        try:
            async with self._session.post(url, json=payload, timeout=20) as r:
                await r.text()  # on ignore la réponse pour rester simple
        except Exception:
            # on fait un fallback silencieux pour ne pas casser le bot
            print("[notify:telegram] send fail (ignored)")

    async def close(self):
        if self._session and not self._session.closed:
            await self._session.close()


class _NullCommands:
    """Itérateur async vide utilisé quand Telegram n'est pas configuré."""
    def __aiter__(self) -> AsyncIterator[str]:
        return self
    async def __anext__(self) -> str:
        await asyncio.sleep(3600)  # jamais
        raise StopAsyncIteration


async def [REDACTED](config: dict) -> tuple[BaseNotifier, AsyncIterator[str]]:
    """
    Retourne (notifier, command_stream).

    - Si TELEGRAM_BOT_TOKEN=[REDACTED] et TELEGRAM_CHAT_ID sont présents: TelegramNotifier,
      et un flux (vide) – l’orchestreur n’en a besoin que si on implémente des
      commandes interactives plus tard.
    - Sinon: NullNotifier + flux vide.
    """
    token = [REDACTED]"TELEGRAM_BOT_TOKEN=[REDACTED]
    chat = os.getenv("TELEGRAM_CHAT_ID")
    if token and chat:
        print("[notify] TELEGRAM configured.")
        return TelegramNotifier(token, chat), _NullCommands()
    print("[notify] TELEGRAM not configured -> Null notifier will be used.")
    return NullNotifier(), _NullCommands()--- [84/191] ./scalper/exchange/bitget.py ---
# scalper/exchange/bitget.py
from __future__ import annotations
import os
import requests
from typing import List, Dict, Any

BASE_URL = "https://api.bitget.com"

# Spot: period strings
_SPOT_PERIOD = {
    "1m": "1min", "3m": "3min", "5m": "5min", "15m": "15min", "30m": "30min",
    "1h": "1hour", "4h": "4hour", "6h": "6hour", "12h": "12hour",
    "1d": "1day", "3d": "3day", "1w": "1week",
}
# Mix: granularity seconds
_MIX_GRAN = {
    "1m": 60, "3m": 180, "5m": 300, "15m": 900, "30m": 1800,
    "1h": 3600, "4h": 14400, "6h": 21600, "12h": 43200,
    "1d": 86400, "3d": 259200, "1w": 604800,
}

def _market_from_symbol(symbol: str) -> str:
    s = symbol.upper()
    if s.endswith("_SPBL"):
        return "spot"
    if s.endswith("_UMCBL"):
        return "umcbl"
    if s.endswith("_DMCBL"):
        return "dmcbl"
    if s.endswith("_CMCBL"):
        return "cmcbl"
    # fallback env / défaut umcbl
    return os.getenv("BITGET_MARKET", "umcbl").lower()

def _product_type(market: str) -> str:
    # valeur attendue par les endpoints mix (umcbl/dmcbl/cmcbl)
    if market in ("umcbl", "dmcbl", "cmcbl"):
        return market
    return "umcbl"

class BitgetExchange:
    """
    Wrapper simple: get_ohlcv(symbol, timeframe, limit) -> [[ts, o, h, l, c, v], ...]
    symbol spot ex: BTCUSDT_SPBL
    symbol perp ex: BTCUSDT_UMCBL / BTCUSD_DMCBL / BTCUSD_CMCBL
    """
    def __init__(self, api_key: [REDACTED] = "", api_secret: [REDACTED] = "", api_passphrase: [REDACTED] = "", timeout: int = 20) -> None:
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": "scalp-bot/1.0"})
        self.timeout = timeout

    def _get(self, path: str, params: Dict[str, Any]) -> Dict[str, Any]:
        url = BASE_URL + path
        r = self.session.get(url, params=params, timeout=self.timeout)
        r.raise_for_status()
        data = r.json()
        # Bitget: {"code":"00000","msg":"success","requestTime":..., "data":[...]}
        if not isinstance(data, dict) or str(data.get("code")) not in ("00000", "0", "200"):
            raise RuntimeError(f"Bitget error payload: {data}")
        return data

    def get_ohlcv(self, symbol: str, timeframe: str = "5m", limit: int = 500) -> List[List[float]]:
        timeframe = timeframe.lower()
        mkt = _market_from_symbol(symbol)

        if mkt == "spot":
            period = _SPOT_PERIOD.get(timeframe)
            if not period:
                raise ValueError(f"timeframe spot non supporté: {timeframe}")
            # Bitget spot: limit max souvent 1000
            lim = max(1, min(int(limit), 1000))
            params = {"symbol": symbol, "period": period, "limit": lim}
            data = self._get("/api/spot/v1/market/candles", params=params)
            rows = data.get("data") or []
            out: List[List[float]] = []
            # Bitget renvoie décroissant -> on inverse
            for r in reversed(rows):
                ts = int(r[0]); o, h, l, c = map(float, r[1:5]); v = float(r[5])
                out.append([ts, o, h, l, c, v])
            return out

        # MIX (umcbl/dmcbl/cmcbl)
        gran = _MIX_GRAN.get(timeframe)
        if not gran:
            raise ValueError(f"timeframe mix non supporté: {timeframe}")

        # Bitget mix: limit max souvent 200, granularity en secondes, productType parfois requis
        lim = max(1, min(int(limit), 200))
        params = {
            "symbol": symbol,
            "granularity": int(gran),
            "limit": lim,
            "productType": _product_type(mkt),
        }

        # essais: candles -> history-candles (certaines régions)
        try:
            data = self._get("/api/mix/v1/market/candles", params=params)
        except requests.HTTPError:
            data = self._get("/api/mix/v1/market/history-candles", params=params)

        rows = data.get("data") or []
        out: List[List[float]] = []
        for r in reversed(rows):
            ts = int(r[0]); o, h, l, c = map(float, r[1:5]); v = float(r[5])
            out.append([ts, o, h, l, c, v])
        return out--- [85/191] ./scalper/exchange/bitget_ccxt.py ---
# scalper/exchange/bitget_ccxt.py
from __future__ import annotations

import asyncio
import csv
import os
import time
from typing import Any, List, Optional

# CCXT async
try:
    import ccxt.async_support as ccxt
except Exception as e:  # noqa: BLE001
    raise RuntimeError("CCXT n'est pas installé. Fais `pip install ccxt`.") from e


def _now_ms() -> int:
    return int(time.time() * 1000)


class BitgetExchange:
    """
    Échange Bitget via CCXT (async) avec cache CSV local.
    - Orienté SPOT pour simplifier (BTCUSDT, ETHUSDT, ...).
    - fetch_ohlcv(symbol, timeframe, limit) -> list[list] façon CCXT:
        [[ts, open, high, low, close, volume], ...]
    """

    def __init__(
        self,
        *,
        api_key: [REDACTED] = None,
        secret: [REDACTED] = None,
        password: Optional[str] = None,  # Bitget a souvent "password" (API passphrase)
        data_dir: str = "/notebooks/data",
        use_cache: bool = True,
        min_fresh_seconds: int = 0,  # fraicheur minimale requise (0 = on accepte tout)
        spot: bool = True,           # True = SPOT (recommandé ici)
    ) -> None:
        self.data_dir = data_dir
        self.use_cache = use_cache
        self.min_fresh = int(min_fresh_seconds)
        self.spot = spot

        os.makedirs(self.data_dir, exist_ok=True)

        # Instance CCXT (async)
        self.ex = ccxt.bitget({
            "apiKey": api_key or "",
            "secret": secret or "",
            "password": password or "",
            "enableRateLimit": True,
            # CCXT timeframe natif (pas besoin de rajouter des headers…)
        })

        # Pré‑charge les marchés SPOT pour résoudre correctement symboles
        self._markets_task: Optional[asyncio.Task[Any]] = None

    async def _ensure_markets(self) -> None:
        if self._markets_task is None:
            self._markets_task = asyncio.create_task(self.ex.load_markets())
        await self._markets_task

    # ---------- CSV cache ----------
    def _csv_path(self, symbol: str, timeframe: str) -> str:
        safe = symbol.replace("/", "").replace(":", "")
        return os.path.join(self.data_dir, f"{safe}-{timeframe}.csv")

    def _read_cache(self, path: str) -> List[List[float]]:
        if not os.path.exists(path):
            return []
        rows: List[List[float]] = []
        try:
            with open(path, "r", newline="") as f:
                rd = csv.reader(f)
                for r in rd:
                    if not r:
                        continue
                    # ts, o, h, l, c, v
                    try:
                        rows.append([
                            int(r[0]),
                            float(r[1]),
                            float(r[2]),
                            float(r[3]),
                            float(r[4]),
                            float(r[5]),
                        ])
                    except Exception:
                        # on ignore les lignes corrompues
                        continue
        except Exception:
            return []
        return rows

    def _write_cache(self, path: str, data: List[List[float]]) -> None:
        # On ré‑écrit intégralement (simple et sûr)
        tmp = path + ".tmp"
        with open(tmp, "w", newline="") as f:
            wr = csv.writer(f)
            wr.writerows(data)
        os.replace(tmp, path)

    # ---------- API publique pour orchestrateur ----------
    async def fetch_ohlcv(
        self, symbol: str, timeframe: str, limit: int, since: Optional[int] = None
    ) -> List[List[float]]:
        """
        Conformité orchestrateur : signature (symbol, timeframe, limit).
        Retour CCXT OHLCV. Utilise cache si dispo/assez frais, sinon CCXT.
        """
        await self._ensure_markets()

        # Bitget (spot) symbol format CCXT: "BTC/USDT"
        ccxt_symbol = symbol.replace("USDT", "/USDT")
        cache_path = self._csv_path(symbol, timeframe)

        # 1) Cache
        if self.use_cache:
            cached = self._read_cache(cache_path)
            if cached:
                # fraicheur = diff entre maintenant et ts dernière bougie
                last_ts = int(cached[-1][0])
                if self.min_fresh == 0 or (_now_ms() - last_ts) <= self.min_fresh * 1000:
                    # suffisant => on retourne la fin
                    if len(cached) >= limit:
                        return cached[-limit:]
                    # pas assez, on essaiera de compléter via CCXT plus bas
                # sinon: on tentera de rafraîchir plus loin

        # 2) Remote via CCXT
        # CCXT fetch_ohlcv: since=None, limit=…  (since en ms)
        # On demande 'limit' bougies; si cache partiel, on pourra fusionner ensuite.
        params: dict[str, Any] = {}
        if self.spot is True:
            params["type"] = "spot"  # ccxt bitget accepte 'type' pour certain endpoints

        try:
            ohlcv = await self.ex.fetch_ohlcv(ccxt_symbol, timeframe, since=since, limit=limit, params=params)
        except Exception as e:  # noqa: BLE001
            # En cas d’échec remote: si on a du cache, on le renvoie quand même
            cached = self._read_cache(cache_path) if self.use_cache else []
            if cached:
                return cached[-limit:]
            raise RuntimeError(f"Bitget CCXT fetch_ohlcv failed for {symbol} {timeframe}: {e}") from e

        # 3) Merge simple cache + remote et ré‑écrit (sans doublons sur ts)
        if self.use_cache:
            base = self._read_cache(cache_path)
            merged = _merge_ohlcv(base, ohlcv)
            self._write_cache(cache_path, merged)
            # retourne la fin
            return merged[-limit:]

        return ohlcv[-limit:]

    async def close(self) -> None:
        try:
            await self.ex.close()
        except Exception:
            pass


def _merge_ohlcv(a: List[List[float]], b: List[List[float]]) -> List[List[float]]:
    """
    Fusionne deux listes OHLCV par timestamp, en écrasant a par b sur collision.
    """
    if not a:
        return list(b)
    if not b:
        return list(a)

    # index rapide par ts
    by_ts: dict[int, List[float]] = {int(row[0]): row for row in a}
    for row in b:
        by_ts[int(row[0])] = row
    return [by_ts[k] for k in sorted(by_ts)]--- [86/191] ./scalper/exchange/fees.py ---
# scalper/exchange/fees.py
from __future__ import annotations

from typing import Dict, Iterable

# Valeurs par défaut (Bitget spot/futures ~ ordre de grandeur ; sera écrasé quand on charge les frais)
DEFAULT_TAKER_BPS = 6    # 0.06%
DEFAULT_MAKER_BPS = 2    # 0.02%

# Cache local: symbol -> {"taker_bps": int, "maker_bps": int}
_FEES_BY_SYMBOL: Dict[str, Dict[str, float]] = {}


def get_fee(symbol: str, kind: str = "taker") -> float:
    """
    Retourne le fee rate (fraction, ex 0.0006) pour 'symbol' et 'kind' ("taker" ou "maker").
    Utilise le cache alimenté par load_bitget_fees(), sinon valeurs par défaut.
    """
    rec = _FEES_BY_SYMBOL.get(symbol, {"taker_bps": DEFAULT_TAKER_BPS, "maker_bps": DEFAULT_MAKER_BPS})
    bps = rec["taker_bps"] if kind == "taker" else rec["maker_bps"]
    return float(bps) / 10_000.0


async def load_bitget_fees(exchange, symbols: Iterable[str]) -> Dict[str, Dict[str, float]]:
    """
    Tente de charger les frais auprès de l'exchange (type ccxt):
      - fetch_trading_fees(symbols) si dispo
      - sinon fetch_trading_fee(symbol) pour chaque symbole
    Remplit le cache _FEES_BY_SYMBOL avec des BPS (entiers).
    """
    symbols = list(symbols)
    fees: Dict[str, Dict[str, float]] = {}

    try:
        if hasattr(exchange, "fetch_trading_fees"):
            data = await exchange.fetch_trading_fees(symbols)
            for s in symbols:
                d = (data or {}).get(s, {}) or {}
                taker = float(d.get("taker", DEFAULT_TAKER_BPS / 10_000))
                maker = float(d.get("maker", DEFAULT_MAKER_BPS / 10_000))
                fees[s] = {"taker_bps": round(taker * 10_000), "maker_bps": round(maker * 10_000)}
        else:
            for s in symbols:
                try:
                    d = await exchange.fetch_trading_fee(s)
                except Exception:
                    d = {}
                taker = float(d.get("taker", DEFAULT_TAKER_BPS / 10_000))
                maker = float(d.get("maker", DEFAULT_MAKER_BPS / 10_000))
                fees[s] = {"taker_bps": round(taker * 10_000), "maker_bps": round(maker * 10_000)}
    except Exception:
        # fallback: défauts
        for s in symbols:
            fees[s] = {"taker_bps": DEFAULT_TAKER_BPS, "maker_bps": DEFAULT_MAKER_BPS}

    # maj du cache
    _FEES_BY_SYMBOL.update(fees)
    return fees--- [87/191] ./scalper/exchange/__init__.py ---
# Rend le sous-package exchanges importable
__all__ = ["bitget"]--- [88/191] ./scalper/config/strategies.yml ---
# scalper/config/strategies.yml
default: current
by_timeframe:
  "1m": current
  "5m": current
  "15m": current
  "1h": current
by_symbol:
  BTCUSDT:
    "1m": current
    "5m": current
  ETHUSDT:
    "5m": current
--- [89/191] ./scalper/config/.ipynb_checkpoints/__init__-checkpoint.py ---
from .loader import load_settings
__all__ = ['load_settings']
--- [90/191] ./scalper/config/__init__.py ---
from .loader import load_settings
__all__ = ['load_settings']
--- [91/191] ./scalper/config/loader.py ---
# scalp/config/loader.py
from __future__ import annotations
import os, json
from typing import Any, Dict, Tuple

# YAML est recommandé, mais on fallback proprement si PyYAML n'est pas installé
try:
    import yaml  # type: ignore
except Exception:
    yaml = None  # fallback JSON si besoin

# dotenv (facultatif) pour charger un .env automatiquement
try:
    from dotenv import load_dotenv  # type: ignore
except Exception:
    load_dotenv = None

# ---------------- Utils ----------------

def _parse_bool(x: Any, default: bool = False) -> bool:
    if isinstance(x, bool): return x
    s = str(x).strip().lower()
    if s in ("1","true","yes","y","on"): return True
    if s in ("0","false","no","n","off",""): return False
    return default

def _parse_float(x: Any, default: float | None = None) -> float | None:
    try: return float(x)
    except Exception: return default

def _parse_int(x: Any, default: int | None = None) -> int | None:
    try: return int(str(x).strip())
    except Exception: return default

def _parse_csv(x: Any) -> list[str]:
    if x is None: return []
    if isinstance(x, (list, tuple)): return [str(v).strip() for v in x if str(v).strip()]
    return [t.strip() for t in str(x).replace(" ", "").split(",") if t.strip()]

def _read_yaml(path: str) -> Dict[str, Any]:
    if not os.path.exists(path): return {}
    with open(path, "r", encoding="utf-8") as f:
        if yaml:
            return yaml.safe_load(f) or {}
        # fallback JSON si quelqu’un met du JSON dans config.yml (rare mais safe)
        try:
            return json.load(f)
        except Exception:
            raise RuntimeError(f"Impossible de lire {path}: installe PyYAML (`pip install pyyaml`) ou fournis du JSON valide.")

def _merge_dict(a: Dict[str, Any], b: Dict[str, Any]) -> Dict[str, Any]:
    # shallow merge suffisant ici (structure plate)
    out = dict(a)
    out.update({k: v for k, v in b.items() if v is not None})
    return out

# ---------------- Public API ----------------

def load_settings(
    config_path: str = "config.yml",
    config_local_path: str = "config.local.yml",
) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """
    Retourne (config_runtime, secrets) :
      - config_runtime : paramètres de stratégie / exécution (OK pour versionner)
      - secrets        : clés API & tokens (NE PAS versionner)
    Priorité : config.yml < config.local.yml < ENV (non sensibles)
    Secrets proviennent EXCLUSIVEMENT de l'ENV (.env)
    """
    # 1) .env (pour secrets & env non sensibles). Faculatif.
    if load_dotenv is not None:
        load_dotenv(override=False)

    # 2) Charge YAML (config.yml + override local)
    base = _read_yaml(config_path)
    local = _read_yaml(config_local_path)
    cfg = _merge_dict(base, local)

    # 3) Overlay ENV **non sensibles** (permet de surcharger sans toucher au YAML)
    env_overlay: Dict[str, Any] = {}
    # Verbosité
    env_overlay["QUIET"] = _parse_bool(os.getenv("QUIET", cfg.get("QUIET", 0)), bool(cfg.get("QUIET", 0)))
    env_overlay["PRINT_OHLCV_SAMPLE"] = _parse_bool(os.getenv("PRINT_OHLCV_SAMPLE", cfg.get("PRINT_OHLCV_SAMPLE", 0)),
                                                    bool(cfg.get("PRINT_OHLCV_SAMPLE", 0)))
    # Runtime / Stratégie
    env_overlay["TIMEFRAME"] = os.getenv("TIMEFRAME", cfg.get("TIMEFRAME", "5m"))
    env_overlay["CASH"] = _parse_float(os.getenv("CASH", cfg.get("CASH", 10000)), cfg.get("CASH", 10000))
    env_overlay["RISK_PCT"] = _parse_float(os.getenv("RISK_PCT", cfg.get("RISK_PCT", 0.5)), cfg.get("RISK_PCT", 0.5))
    env_overlay["SLIPPAGE_BPS"] = _parse_float(os.getenv("SLIPPAGE_BPS", cfg.get("SLIPPAGE_BPS", 0)), cfg.get("SLIPPAGE_BPS", 0))
    # Watchlist
    env_overlay["WATCHLIST_MODE"] = os.getenv("WATCHLIST_MODE", cfg.get("WATCHLIST_MODE", "static"))
    env_overlay["[REDACTED]"] = _parse_int(
        os.getenv("[REDACTED]", cfg.get("[REDACTED]", 4)), cfg.get("[REDACTED]", 4)
    )
    env_overlay["TOP_SYMBOLS"] = _parse_csv(os.getenv("TOP_SYMBOLS", cfg.get("TOP_SYMBOLS")))
    env_overlay["TOP_CANDIDATES"] = _parse_csv(os.getenv("TOP_CANDIDATES", cfg.get("TOP_CANDIDATES")))
    # Caps (optionnel) : on accepte YAML (dict) ou ENV JSON
    caps_env = os.getenv("CAPS_JSON")
    if caps_env:
        try:
            env_overlay["CAPS"] = json.loads(caps_env)
        except Exception:
            env_overlay["CAPS"] = cfg.get("CAPS", {})
    else:
        env_overlay["CAPS"] = cfg.get("CAPS", {})

    # 4) Secrets UNIQUEMENT via ENV (jamais via YAML)
    secrets = {
        "BITGET_API_KEY=[REDACTED] os.getenv("BITGET_API_KEY=[REDACTED] ""),
        "BITGET_API_SECRET=[REDACTED] os.getenv("BITGET_API_SECRET=[REDACTED] ""),
        "BITGET_API_PASS=[REDACTED] os.getenv("BITGET_API_PASS=[REDACTED] ""),
        "BITGET_USE_TESTNET": _parse_bool(os.getenv("BITGET_USE_TESTNET", os.getenv("BITGET_TESTNET", "1")), True),
        "BITGET_PRODUCT": os.getenv("BITGET_PRODUCT", "umcbl"),
        "TELEGRAM_BOT_TOKEN=[REDACTED] os.getenv("TELEGRAM_BOT_TOKEN=[REDACTED] ""),
        "TELEGRAM_CHAT_ID": os.getenv("TELEGRAM_CHAT_ID", ""),
    }

    # 5) Runtime normalisé pour l’orchestrateur
    runtime = {
        "quiet": bool(env_overlay["QUIET"]),
        "print_sample": bool(env_overlay["PRINT_OHLCV_SAMPLE"]),
        "timeframe": str(env_overlay["TIMEFRAME"]),
        "cash": float(env_overlay["CASH"]),
        "risk_pct": float(env_overlay["RISK_PCT"]),
        "slippage_bps": float(env_overlay["SLIPPAGE_BPS"]),
        "watchlist_mode": str(env_overlay["WATCHLIST_MODE"]),
        "[REDACTED]": int(env_overlay["[REDACTED]"]),
        "top_symbols": env_overlay["TOP_SYMBOLS"],          # list[str]
        "top_candidates": env_overlay["TOP_CANDIDATES"],    # list[str]
        "caps": env_overlay["CAPS"],                        # dict
        # rempli au boot par les frais Bitget
        "fees_by_symbol": {}, 
    }

    return runtime, secrets
    --- [92/191] ./tg_diag.py ---
# tg_diag.py
import asyncio, os, aiohttp

TOKEN=[REDACTED] = os.getenv("TELEGRAM_TOKEN=[REDACTED] "")
CHAT  = os.getenv("TELEGRAM_CHAT_ID", "")

async def main():
    if not TOKEN=[REDACTED] or not CHAT:
        print("❌ Manque TELEGRAM_TOKEN=[REDACTED] ou TELEGRAM_CHAT_ID dans l'env.")
        return
    url = f"https://api.telegram.org/bot{TOKEN=[REDACTED]
    payload = {"chat_id": CHAT, "text": "🔎 Test Telegram OK ?"}
    try:
        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=15)) as s:
            async with s.post(url, json=payload) as r:
                body = await r.text()
                print("HTTP:", r.status)
                print("Body:", body[:500])
    except Exception as e:
        print("❌ Exception:", repr(e))

if __name__ == "__main__":
    asyncio.run(main())--- [93/191] ./README.md ---
SCALP — Backtest → Promotion → Exécution (Notebook unique)

Mode recommandé : une seule machine Notebook (Paperspace Gradient ou équivalent).
Visu : tableau de bord HTML unique généré à la racine du repo (./dashboard.html).
Accès externe : via ngrok démarré automatiquement par bot.py.
Aucune URL/port manuel à gérer.

============================================================
TL;DR
============================================================

1. Clone le repo dans ton Notebook.
2. Lance :
   python bot.py

   - Auto-install des deps (via sitecustomize.py + fallback).
   - Démarrage d’un mini-serveur http.server (port configurable, défaut 8888).
   - ngrok auto (si authtoken dispo) → écrit ./ngrok_url.txt.
   - Génération du dashboard ./dashboard.html + ./dashboard_url.txt (URLs locales + ngrok).
3. Ouvre dashboard_url.txt (copier/coller le lien dans Safari, iPhone ok).

============================================================
Arborescence
============================================================

.
├── bot.py                           # point d’entrée unique
├── engine/
│   └── config/
│       ├── config.yaml              # chemins, TF, risk_mode, html_port, etc.
│       └── strategies.yml           # stratégies promues "actives"
├── jobs/
│   ├── maintainer.py                # boucle orchestrateur
│   ├── backtest.py                  # génère summary.json + strategies.yml.next
│   └── promote.py                   # filtre/promote + rendus (appelle tools/render_report.py)
├── tools/
│   ├── render_report.py             # génère ./dashboard.html + dashboard_url.txt
│   └── start_ngrok.py               # démarre ngrok, écrit ./ngrok_url.txt
├── notebooks/scalp_data/
│   ├── data/ohlcv/<PAIRUSDT>/<TF>.csv
│   ├── logs/
│   └── reports/
│       ├── summary.json             # résultats backtest agrégés
│       └── strategies.yml.next      # candidats à la promotion
├── backtest_config.json             # JSON de backtest (grid/optuna/walk-forward)
├── entries_config.json              # JSON des sets d’entrées/signaux
├── requirements.txt
├── sitecustomize.py
├── dashboard.html                   # (généré) Dashboard à la racine
├── dashboard_url.txt                # (généré) URLs utiles (localhost + ngrok)
└── ngrok_url.txt                    # (généré) URL publique courante

============================================================
Pipeline (automate)
============================================================

1. Refresh watchlist + backfill OHLCV (TF = runtime.tf_list).
2. Backtest multi-paires × multi-TF (sur les CSV frais).
3. Ecriture des résultats : reports/summary.json + reports/strategies.yml.next.
4. Promotion (règles via risk_mode) → mise à jour engine/config/strategies.yml.
5. Rendu du dashboard (HTML à la racine) + URLs (local + ngrok).
6. Termboard : désactivé par défaut (tout se lit dans le HTML).

============================================================
Démarrage et URLs
============================================================

Lancer :
  python bot.py

Ouvrir :
  - Fichier texte ./dashboard_url.txt → contient :
    - http://localhost:<port>/dashboard.html
    - et si ngrok actif : https://<id>.ngrok.io/dashboard.html

Ngrok token : [REDACTED] nécessaire, poser NGROK_AUTHTOKEN=[REDACTED] dans l’environnement ou exécuter une fois dans le Notebook :
  ngrok config add-authtoken VOTRE_TOKEN=[REDACTED]

============================================================
Config minimale (extrait engine/config/config.yaml)
============================================================

runtime:
  data_dir: /notebooks/scalp_data/data
  reports_dir: /notebooks/scalp_data/reports
  tf_list: ["1m","5m","15m"]
  age_mult: 5
  topN: 10
  backfill_limit: 5000
  risk_mode: aggressive
  exec_enabled: false
  html_port: 8888

============================================================
Standards d’entrée/sortie
============================================================

Entrées (backtest) :
  CSV OHLCV data/ohlcv/<PAIRUSDT>/<TF>.csv

Sorties :
  reports/summary.json (tableau rows avec PF, MDD, Sharpe, WR, trades, etc.)
  reports/strategies.yml.next (candidats formatés pour promotion)
  engine/config/strategies.yml (actifs promus)
  ./dashboard.html + ./dashboard_url.txt

Split JSON prévu :
  backtest_config.json (grids optuna, walk-forward, contraintes)
  entries_config.json (signaux d’entrée, règles de risk, etc.)

============================================================
Commandes utiles
============================================================

python bot.py
python jobs/maintainer.py
python jobs/backtest.py --from-watchlist --tfs 1m,5m,15m
python jobs/promote.py --source notebooks/scalp_data/reports/strategies.yml.next

============================================================
Dépannage rapide
============================================================

Pas d’affichage dans le dashboard :
  Vérifier que reports/summary.json n’est pas vide.
  Assouplir risk_mode (aggressive) ou augmenter backfill_limit.

URL ngrok manquante :
  Vérifier ngrok_url.txt et NGROK_AUTHTOKEN=[REDACTED]

Module introuvable :
  sitecustomize.py auto-installe les libs (plotly, pyngrok…).
  Relancer python bot.py.

============================================================
Architecture (rappel)
============================================================

- jobs/maintainer.py : boucle 60s (refresh, backfill, backtest, promote, expiry).
- jobs/backtest.py : calcule métriques et écrit summary.json + strategies.yml.next.
- jobs/promote.py : filtre par risk_mode, maj strategies.yml, génère dashboard.
- bot.py : point d’entrée unique (http.server, ngrok auto, rendu initial, maintainer).

Split JSON :
  backtest_config.json → règles de backtest, optuna, walk-forward
  entries_config.json → sets de signaux et risk management

Etats orchestrateur :
  MIS = CSV absent
  OLD = CSV vieux
  DAT = CSV frais mais pas de stratégie
  OK  = CSV frais + stratégie promue non expirée

============================================================
Fin doc
============================================================--- [94/191] ./STRATEGY.md ---
SCALP — Spécification Stratégies
================================

1) Critères de sélection selon risk_mode
----------------------------------------

- conservative:
  PF ≥ 1.4
  MDD ≤ 15%
  trades ≥ 35

- normal:
  PF ≥ 1.3
  MDD ≤ 20%
  trades ≥ 30

- aggressive:
  PF ≥ 1.2
  MDD ≤ 30%
  trades ≥ 25

Notes :
- PF = Profit Factor (gain/perte)
- MDD = Max Drawdown (drawdown max)
- trades = nombre de trades réalisés pendant le backtest

2) Paramètres mesurés (summary.json)
------------------------------------
Chaque backtest écrit dans reports/summary.json une liste de lignes (rows) contenant :
- pair : symbole (ex: BTCUSDT)
- tf : timeframe (1m, 5m, 15m…)
- pf : Profit Factor
- mdd : Max Drawdown (0.18 = 18%)
- trades : nombre de trades
- wr : Win rate (0.55 = 55%)
- sharpe : ratio Sharpe
- note : score interne utilisé pour trier les stratégies

3) Format strategies.yml.next
-----------------------------

/notebooks/scalp_data/reports/strategies.yml.next

strategies:
  "<PAIRUSDT>:<TF>":
    name: "ema_atr_v1"
    ema_fast: 12
    ema_slow: 34
    atr_period: 14
    trail_atr_mult: 2.0
    risk_pct_equity: 0.5
    created_at: <timestamp>
    expires_at: <timestamp>
    expired: false
    metrics:
      pf: 1.34
      mdd: 0.18
      trades: 42
      wr: 0.55
      sharpe: 1.10

4) Format strategies.yml (promu)
--------------------------------
engine/config/strategies.yml

- Reprend le format ci-dessus.
- Ne garde que les stratégies qui passent les critères du risk_mode.
- Met à jour si une meilleure stratégie ou plus récente est trouvée.
- Marque expired=true si dépassée.

5) Lifetime (expiry)
--------------------

Durée de vie = age_mult × TF
Exemple avec age_mult=5 :
- 1m → 5 minutes
- 5m → 25 minutes
- 15m → 75 minutes

Après ce délai, expired=true et la stratégie doit être remplacée par une nouvelle.

6) Split JSON
-------------

- backtest_config.json
  Paramètres pour recherche, optimisation et walk-forward :
  - Grilles EMA/ATR/MACD/RSI
  - Coûts (fees, slippage)
  - Contraintes globales (min trades, min PF)
  - Méthode optimisation (optuna, grid)

- entries_config.json
  Paramètres pour les sets d’entrées (signaux) :
  - pullback_trend, breakout, mean_reversion
  - context (probabilités min, ADX, volume, ATR)
  - signaux (RSI, MACD, VWAP, BB, candles…)
  - risk (SL, TP, trail, timeout_bars)

Ces 2 fichiers permettent de séparer :
- la recherche et validation (backtest_config.json)
- la logique de déclenchement opérationnelle (entries_config.json)

7) Promotion
------------

- Source : strategies.yml.next
- Filtrage : appliquer critères risk_mode
- Fusion : engine/config/strategies.yml
- Logs : scalp_data/logs/promote.log
- Rendu : dashboard.html mis à jour automatiquement--- [95/191] ./.ipynb_checkpoints/CHANGELOG-checkpoint.md ---
# Changelog

## Unreleased

- Trigger trade entries via `strategy.generate_signal` with weighted scoring and
  signal levels.
- Dynamic risk management adapting `risk_pct` and leverage based on signal and
  user risk level.
- Notional and margin caps with available balance check to avoid Bitget error
  `40762`.
- Risk notifications with green/yellow/red indicators for terminal and
  Telegram.
--- [96/191] ./.ipynb_checkpoints/bot-checkpoint.py ---
# bot.py  (mise à jour pour brancher les commandes Telegram à part)
#!/usr/bin/env python3
from __future__ import annotations

import asyncio
import logging
import os
import sys
from typing import Any, Dict, Iterable, Optional, Sequence

from engine.config.loader import load_config
from engine.live.orchestrator import RunConfig, run_orchestrator
from engine.live.notify import build_notifier
from engine.live.commands import [REDACTED]

LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=getattr(logging, LOG_LEVEL, logging.INFO),
                    format="%(asctime)s %(levelname)s %(name)s: %(message)s")
log = logging.getLogger("bot")


def _build_exchange(cfg: Dict[str, Any]):
    try:
        from engine.exchange.bitget_ccxt import BitgetExchange
        ex = BitgetExchange(
            api_key=[REDACTED]"secrets"]["bitget"]["access"],
            secret=[REDACTED]"secrets"]["bitget"]["secret"],
            password=cfg["secrets"]["bitget"]["passphrase"],
            data_dir=cfg["runtime"]["data_dir"],
        )
        log.info("Exchange CCXT initialisé")
        return ex
    except Exception as exc:
        log.warning("CCXT indisponible (%s) — fallback REST", exc)
        from engine.exchange.bitget_rest import BitgetFuturesClient
        return BitgetFuturesClient(
            access_key=[REDACTED]"secrets"]["bitget"]["access"],
            secret_key=[REDACTED]"secrets"]["bitget"]["secret"],
            passphrase=[REDACTED]"secrets"]["bitget"]["passphrase"],
            base_url=os.getenv("BITGET_BASE_URL", "https://api.bitget.com"),
            paper_trade=cfg["runtime"].get("paper_trade", True),
        )


async def _run() -> int:
    cfg = load_config()
    runtime, strategy = cfg.get("runtime", {}), cfg.get("strategy", {})
    symbols: Sequence[str] = runtime.get("allowed_symbols") or []
    run_cfg = RunConfig(
        symbols=symbols or ["BTCUSDT", "ETHUSDT", "SOLUSDT"],
        timeframe=strategy.get("live_timeframe", "1m"),
        refresh_secs=int(runtime.get("refresh_secs", 5)),
        cache_dir=str(runtime.get("data_dir")),
    )
    ex = _build_exchange(cfg)
    notifier = build_notifier(cfg)
    cmd_stream = [REDACTED](cfg)
    await run_orchestrator(ex, run_cfg, notifier, cmd_stream)
    return 0


def main(argv: Optional[Iterable[str]] = None) -> int:
    try:
        return asyncio.run(_run())
    except KeyboardInterrupt:
        log.info("Arrêt demandé (Ctrl+C)")
        return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))--- [97/191] ./.ipynb_checkpoints/rr-checkpoint.py ---
--- [98/191] ./.ipynb_checkpoints/pytest-checkpoint.ini ---
[pytest]
addopts = -q
--- [99/191] ./.ipynb_checkpoints/requirements-checkpoint.txt ---
# Dépendances principales du projet scalp

# HTTP / API
requests>=2.31

# Data science
pandas>=2.1
numpy>=1.26

# Config et environnements
PyYAML>=6.0
python-dotenv>=1.0

# Dashboard (inclus par défaut pour éviter de multiplier les fichiers)
streamlit>=1.33

# Exchange (optionnel mais utile si activé)
ccxt>=4.0.0--- [100/191] ./.ipynb_checkpoints/[REDACTED]-checkpoint.py ---
#!/usr/bin/env python3
"""Fetch the list of Bitget futures contracts.

This helper script queries the public Bitget REST API to retrieve futures
trading pairs for the specified product types and saves them to CSV and JSON
files. It mirrors the standalone example provided by the user but integrates
with the repository's configuration system.

Usage examples::

    python [REDACTED].py
    python [REDACTED].py --types USDT-FUTURES COIN-FUTURES
    python [REDACTED].py --out pairs.csv --json-out pairs.json
"""
from __future__ import annotations

import argparse
import csv
import json
import sys
import time
from typing import Any, Dict, List

from engine.bot_config import CONFIG

try:  # pragma: no cover - import guard
    import requests
except ModuleNotFoundError as exc:  # pragma: no cover - handled at runtime
    sys.stderr.write(
        "This script requires the 'requests' package. Install it with:\n  pip install requests\n"
    )
    raise

BASE_URL = CONFIG.get("BASE_URL", "https://api.bitget.com")
CONTRACTS_ENDPOINT = "/api/v2/mix/market/contracts"
[REDACTED] = ["USDT-FUTURES", "USDC-FUTURES", "COIN-FUTURES"]


def fetch_contracts(product_type: str, timeout: float = 10.0) -> List[Dict[str, Any]]:
    """Return contract metadata for ``product_type``."""
    url = f"{BASE_URL}{CONTRACTS_ENDPOINT}"
    params = {"productType": product_type}
    resp = requests.get(url, params=params, timeout=timeout)
    try:
        data = resp.json()
    except json.JSONDecodeError as exc:  # pragma: no cover - network failure
        raise RuntimeError(
            f"Non-JSON response from Bitget API for {product_type}: {resp.text[:200]}"
        ) from exc
    if resp.status_code != 200 or data.get("code") != "00000":
        raise RuntimeError(f"Bitget API error for {product_type}: HTTP {resp.status_code} body={data}")
    return data.get("data", [])


def normalize_rows(product_type: str, contracts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Select and rename key fields for CSV/JSON output."""
    rows: List[Dict[str, Any]] = []
    for c in contracts:
        row = {
            "productType": product_type,
            "symbol": c.get("symbol"),
            "baseCoin": c.get("baseCoin"),
            "quoteCoin": c.get("quoteCoin"),
            "symbolType": c.get("symbolType"),
            "symbolStatus": c.get("symbolStatus"),
            "maxLever": c.get("maxLever"),
            "minLever": c.get("minLever"),
            "minTradeNum": c.get("minTradeNum"),
            "sizeMultiplier": c.get("sizeMultiplier"),
            "pricePlace": c.get("pricePlace"),
            "volumePlace": c.get("volumePlace"),
            "launchTime": c.get("launchTime"),
            "deliveryTime": c.get("deliveryTime"),
        }
        rows.append(row)
    return rows


def write_csv(rows: List[Dict[str, Any]], path: str) -> None:
    """Write ``rows`` to ``path`` in CSV format."""
    headers = [
        "productType",
        "symbol",
        "baseCoin",
        "quoteCoin",
        "symbolType",
        "symbolStatus",
        "maxLever",
        "minLever",
        "minTradeNum",
        "sizeMultiplier",
        "pricePlace",
        "volumePlace",
        "launchTime",
        "deliveryTime",
    ]
    with open(path, "w", newline="", encoding="utf-8") as fh:
        writer = csv.DictWriter(fh, fieldnames=headers)
        writer.writeheader()
        if rows:
            writer.writerows(rows)


def main(argv: List[str] | None = None) -> int:
    parser = argparse.ArgumentParser(
        description="Fetch Bitget futures pairs (contracts) and save to CSV/JSON."
    )
    parser.add_argument(
        "--types",
        nargs="+",
        default=[REDACTED],
        help="Product types to fetch. Choices: USDT-FUTURES, USDC-FUTURES, COIN-FUTURES",
    )
    parser.add_argument("--out", default="[REDACTED].csv", help="CSV output file path")
    parser.add_argument(
        "--json-out", default="[REDACTED].json", help="JSON output file path"
    )
    parser.add_argument("--sleep", type=float, default=0.2, help="Seconds to sleep between requests")
    args = parser.parse_args(argv)

    all_rows: List[Dict[str, Any]] = []
    merged_json: Dict[str, List[Dict[str, Any]]] = {}

    for i, pt in enumerate(args.types):
        try:
            contracts = fetch_contracts(pt)
        except Exception as exc:  # pragma: no cover - network/runtime error
            sys.stderr.write(f"[!] Failed to fetch {pt}: {exc}\n")
            continue
        rows = normalize_rows(pt, contracts)
        all_rows.extend(rows)
        merged_json[pt] = contracts
        if i < len(args.types) - 1 and args.sleep > 0:
            time.sleep(args.sleep)

    all_rows.sort(key=[REDACTED] r: (r.get("productType") or "", r.get("symbol") or ""))

    write_csv(all_rows, args.out)
    with open(args.json_out, "w", encoding="utf-8") as fh:
        json.dump(merged_json, fh, ensure_ascii=False, indent=2)

    counts = {pt: len(merged_json.get(pt, [])) for pt in args.types}
    total = sum(counts.values())
    print(
        f"Saved {total} futures pairs across {len(args.types)} product types to '{args.out}' and '{args.json_out}'."
    )
    for pt, n in counts.items():
        print(f"  - {pt}: {n} pairs")
    return 0


if __name__ == "__main__":  # pragma: no cover - CLI execution
    raise SystemExit(main())
--- [101/191] ./.ipynb_checkpoints/dump-checkpoint.txt ---
Dump created: 2025-08-23 08:28:27
Repository tree:
Scalp/
    CHANGELOG.md
    Makefile
    PROMPT.md
    README.md
    STRATEGY.md
    [REDACTED].py
    bot.py
    cli.py
    dashboard.py
    dump_repo.py
    init.py
    pytest.ini
    quick_order.py
    requirements-dev.txt
    requirements.txt
    run_backtest.py
    short_one_way.py
    notebooks/
        spot/
            bitget_bot.py
    backtest/
        __init__.py
        engine.py
        grid_search.py
        optimize.py
        run_multi.py
    scalp/
        VERSION
        __init__.py
        bitget_client.py
        bot_config.py
        client.py
        config.py
        logging_utils.py
        metrics.py
        notifier.py
        pairs.py
        strategy.py
        telegram_bot.py
        trade_utils.py
        version.py
        ws.py
        risk/
            __init__.py
            manager.py
        services/
            __init__.py
            order_service.py
            utils.py
        backtest/
            __init__.py
            walkforward.py
        positions/
            __init__.py
            state.py
        selection/
            __init__.py
            momentum.py
            scanner.py
        adapters/
            __init__.py
            bitget.py
            market_data.py
    result/
    signals/
        __init__.py
        generator.py
    data/
        BTCUSDT-1m.csv
        __init__.py
        indicators.py
    tests/
        conftest.py
        test_analyse_risque.py
        test_backtest.py
        test_backtest_multi.py
        [REDACTED].py
        [REDACTED].py
        [REDACTED].py
        test_bot_update.py
        [REDACTED].py
        test_calc_pnl_pct.py
        test_check_config.py
        test_cli.py
        test_client.py
        [REDACTED].py
        [REDACTED].py
        [REDACTED].py
        [REDACTED].py
        test_env_loading.py
        test_grid_search.py
        test_heat_score.py
        test_indicators.py
        test_min_qty_rules.py
        test_notifier.py
        [REDACTED].py
        test_pair_selection.py
        test_pairs.py
        test_risk_manager.py
        test_risk_utils.py
        test_signal_risk.py
        test_slippage.py
        test_strategy_v2.py
        test_telegram_bot.py
        test_utils.py
        test_version.py
        test_walk_forward.py
        test_ws.py
    live/
        __init__.py
        journal.py
        notify.py
        ohlcv_service.py
        orchestrator.py
        orders.py
        position_fsm.py
        state_store.py
        telegram_async.py
        watchlist.py

## CHANGELOG.md (last modified: 2025-08-23 08:27:41)
     1: # Changelog
     2: 
     3: ## Unreleased
     4: 
     5: - Trigger trade entries via `strategy.generate_signal` with weighted scoring and
     6:   signal levels.
     7: - Dynamic risk management adapting `risk_pct` and leverage based on signal and
     8:   user risk level.
     9: - Notional and margin caps with available balance check to avoid Bitget error
    10:   `40762`.
    11: - Risk notifications with green/yellow/red indicators for terminal and
    12:   Telegram.


## Makefile (last modified: 2025-08-23 08:27:41)
     1: .PHONY: test
     2: 
     3: test:
     4: 	pytest


## PROMPT.md (last modified: 2025-08-23 08:27:41)
     1: # Prompt de re-création du bot Scalp (version spot)
     2: 
     3: Ce fichier résume les modules et fonctions essentiels afin de recréer le bot de trading **spot** Bitget (paires USDT) à partir de zéro. Chaque fonction liste son rôle principal et les paramètres indispensables. Le fichier `.env` contenant les clés API se trouve dans le dossier parent du bot.
     4: 
     5: ## Structure principale
     6: 
     7: ### bot.py
     8: - `_noop_event(*args, **kwargs)` : fonction vide pour le logging d'événements.
     9: - `check_config()` : vérifie la présence des clés API Bitget et journalise un avertissement si elles manquent.
    10: - `BitgetSpotClient` : sous-classe du client spot Bitget qui injecte `requests` et la fonction `log_event`.
    11: - `[REDACTED](client, pairs, interval="1m", ema_fast_n=None, ema_slow_n=None)` : applique la stratégie EMA sur une liste de paires et renvoie les signaux.
    12: - `send_selected_pairs(client, top_n=40, tg_bot=None)` : sélectionne et notifie les paires les plus actives.
    13: - `update(client, top_n=40, tg_bot=None)` : rafraîchit la liste des paires et renvoie la charge utile envoyée.
    14: - `main(argv=None)` : initialise la configuration, le client, le `RiskManager`, le bot Telegram et exécute la boucle de trading.
    15: 
    16: ### cli.py
    17: - `[REDACTED](pairs, timeframe, jobs)` : lance une optimisation paramétrique (exemple minimal).
    18: - `[REDACTED](pair, timeframe, splits, train_ratio)` : exécute une analyse walk-forward.
    19: - `run_live_pipeline(pairs, tfs)` : exécute la pipeline live asynchrone.
    20: - `create_parser()` : construit l’analyseur d’arguments avec les sous-commandes `opt`, `walkforward`, `live` et `bump-version`.
    21: - `main(argv=None)` : point d'entrée qui déclenche la commande choisie.
    22: 
    23: ### init.py
    24: - `install_packages(*args)` : installe des paquets via `pip`.
    25: - `main()` : installe tous les fichiers `requirements*.txt` et `pytest`.
    26: 
    27: ## Modules `scalp`
    28: 
    29: ### bot_config.py
    30: - `CONFIG` : dictionnaire global des paramètres (clés API, symbole, EMA, ATR, risques, etc.).
    31: 
    32: ### metrics.py
    33: - `calc_pnl_pct(entry_price, exit_price, side, fee_rate=0)` : pourcentage de PnL net des frais.
    34: - `calc_rsi(prices, period=14)` : calcul du RSI (Wilder).
    35: - `calc_atr(highs, lows, closes, period=14)` : ATR avec lissage de Wilder.
    36: - `calc_macd(prices, fast=12, slow=26, signal=9)` : renvoie MACD, ligne signal et histogramme.
    37: - `backtest_position(prices, entry_idx, exit_idx, side)` : valide qu’une position est cohérente avec le mouvement des prix.
    38: 
    39: ### strategy.py
    40: - `ema(series, window)` : moyenne mobile exponentielle.
    41: - `vwap(highs, lows, closes, volumes)` : prix moyen pondéré par le volume.
    42: - `obv(closes, volumes)` : série On Balance Volume.
    43: - `cross(last_fast, last_slow, prev_fast, prev_slow)` : détecte les croisements EMA.
    44: - `[REDACTED](bid_vol, ask_vol)` : mesure le déséquilibre du carnet d'ordres.
    45: - `swing_levels(highs, lows, lookback)` : renvoie le dernier plus haut et plus bas.
    46: - `Signal` : dataclass contenant `symbol`, `side`, `price`, `sl`, `tp1`, `tp2`, `qty`.
    47: - `generate_signal(symbol, ohlcv, equity, risk_pct, ...)` : produit un `Signal` si toutes les conditions de stratégie sont réunies.
    48: - `scan_pairs` et `select_active_pairs` sont re-exportés pour la sélection des paires.
    49: 
    50: ### trade_utils.py
    51: - `[REDACTED](equity_usdt, price, risk_pct, symbol=None)` : calcule la quantité à acheter/vendre en fonction du risque et du prix.
    52: - `analyse_risque(open_positions, equity_usdt, price, risk_pct, symbol=None, side="long", risk_level=2)` : renvoie la taille de position conseillée selon l’exposition actuelle (sans effet de levier).
    53: - `trailing_stop(side, current_price, atr, sl, mult=0.75)` : met à jour le stop loss en fonction de l'ATR.
    54: - `break_even_stop(side, entry_price, current_price, atr, sl, mult=1.0)` : déplace le stop loss à break-even après un mouvement favorable.
    55: - `should_scale_in(entry_price, current_price, last_entry, atr, side, distance_mult=0.5)` : indique si la position doit être renforcée.
    56: - `timeout_exit(entry_time, now, entry_price, current_price, side, progress_min=15, timeout_min=30)` : ferme une position si aucune progression n’est constatée.
    57: - `[REDACTED](side, best_bid, best_ask, slippage=0.001)` : calcule un prix limite pour une exécution quasi immédiate.
    58: 
    59: ### risk
    60: - `calc_risk_amount(equity, risk_pct)` : montant d'argent risqué sur un trade.
    61: - `calc_position_size(equity, risk_pct, stop_distance)` : taille de position selon le stop.
    62: - `adjust_risk_pct(risk_pct, win_streak, loss_streak, increase=0.12, decrease=0.25, min_pct=0.001, max_pct=0.05)` : ajuste le pourcentage de risque selon les séries de gains/pertes.
    63: - `RiskManager` : classe gérant limites journalières, kill switch et ajustement de risque.
    64:   - `reset_day()`, `register_trade(pnl_pct)`/`record_trade`, `dynamic_risk_pct(signal_quality, score)`, `apply_trailing(direction, price, sl, atr, params)`, `pause_duration()`, `can_open(current_positions)`.
    65: 
    66: ### notifier.py
    67: - `_pair_name(symbol)` : formatte le nom d’une paire.
    68: - `_format_text(event, payload=None)` : construit un message lisible.
    69: - `notify(event, payload=None)` : envoie des notifications via webhook HTTP et/ou Telegram.
    70: 
    71: ### logging_utils.py
    72: - `get_jsonl_logger(path, max_bytes=0, backup_count=0)` : renvoie une fonction de logging JSONL avec rotation optionnelle.
    73: - `TradeLogger(csv_path, sqlite_path)` : enregistre chaque trade dans un CSV et une base SQLite (`log(data)`).
    74: 
    75: ### bitget_client.py
    76: - `BitgetSpotClient(access_key, secret_key, base_url, recv_window=30, paper_trade=True, requests_module=requests, log_event=None)` : client REST léger pour le marché spot.
    77:   - `get_symbol_info(symbol=None)`, `get_kline(symbol, interval="1m", start=None, end=None)`, `get_ticker(symbol=None)`.
    78:   - `_private_request(method, path, params=None, body=None)` : signe et exécute les requêtes privées.
    79:   - `get_account()`, `get_open_orders(symbol=None)`.
    80:   - `place_order(symbol, side, quantity, order_type, price=None, stop_loss=None, take_profit=None)`.
    81:   - `cancel_order(symbol, order_id)`, `cancel_all(symbol)`.
    82: 
    83: ### pairs.py
    84: - `get_trade_pairs(client)` : récupère toutes les paires via `get_ticker`.
    85: - `filter_trade_pairs(client, volume_min=5_000_000, max_spread_bps=5, top_n=40)` : filtre par volume/spread.
    86: - `select_top_pairs(client, top_n=40, key="volume")` : trie par volume ou autre clé.
    87: - `[REDACTED](client, pairs, interval="1m", ema_fast_n=None, ema_slow_n=None, ema_func=ema, cross_func=cross)` : signaux EMA croisement.
    88: - `send_selected_pairs(client, top_n=40, select_fn=select_top_pairs, notify_fn=notify)` : déduplique USD/USDT/USDC et notifie la liste.
    89: - `heat_score(volatility, volume, news=False)` : score combinant volatilite et volume.
    90: - `[REDACTED](pairs, top_n=3)` : sélection des paires les plus "chaudes".
    91: - `decorrelate_pairs(pairs, corr, threshold=0.8, top_n=3)` : choisit des paires peu corrélées.
    92: 
    93: ### telegram_bot.py
    94: - `TelegramBot(token, chat_id, client, config, risk_mgr, requests_module=requests)` : mini bot Telegram.
    95:   - `send_main_menu(session_pnl)`, `update_pairs()`, `send(text, keyboard=None)`, `answer_callback(cb_id)`,
    96:   - `fetch_updates()`, `handle_updates(session_pnl)`, `handle_callback(data, session_pnl)`.
    97:   - Helpers privés `_base_symbol`, `[REDACTED]`, `_menu_text`.
    98: - `init_telegram_bot(client, config, risk_mgr)` : instancie un `TelegramBot` si les variables d’environnement `TELEGRAM_BOT_TOKEN=[REDACTED] et `TELEGRAM_CHAT_ID` sont définies.
    99: 
   100: ## Utilisation
   101: 1. Définir les variables d’environnement (clés Bitget, token Telegram, etc.).
   102: 2. Exécuter `init.py` pour installer les dépendances.
   103: 3. Lancer `bot.py` pour démarrer le trading.
   104: 4. Utiliser `cli.py` pour les outils d’optimisation ou de tests.
   105: 
   106: Ce résumé fournit les éléments nécessaires à la reconstruction du bot et à la compréhension de chaque fonction essentielle.
   107: 


## README.md (last modified: 2025-08-23 08:27:41)
     1: # Scalp
     2: 
     3: Bot de trading pour les futures USDT-M de Bitget. Ce projet est **expérimental** et fourni à des fins éducatives.
     4: 
     5: ## Installation
     6: 
     7: Assurez-vous d'avoir Python 3.8 ou supérieur puis installez les dépendances :
     8: 
     9: ```bash
    10: pip install -r requirements.txt
    11: ```
    12: 
    13: Pour développer ou exécuter les tests :
    14: 
    15: ```bash
    16: pip install -r requirements-dev.txt
    17: pytest  # ou make test
    18: ```
    19: 
    20: ## Configuration
    21: 
    22: Le bot lit sa configuration via des variables d'environnement :
    23: 
    24: - `BITGET_ACCESS_KEY=[REDACTED] `BITGET_SECRET=[REDACTED] : clés API Bitget (laisser les valeurs par défaut pour rester en mode papier).
    25: - `PAPER_TRADE` (`true`/`false`) : par défaut `true`, n'envoie aucun ordre réel.
    26: - `SYMBOL` : symbole du contrat futures (par défaut, `BTCUSDT`).
    27: - `INTERVAL` : intervalle des chandeliers, ex. `1m`, `5m`.
    28: - `EMA_FAST`, `EMA_SLOW` : périodes des EMA utilisées par la stratégie.
    29: - `MACD_FAST`, `MACD_SLOW`, `MACD_SIGNAL` : paramètres du filtre de tendance MACD.
    30: - `EMA_TREND_PERIOD` : période de l'EMA longue utilisée comme filtre de tendance général.
    31: - `RISK_PCT_EQUITY`, `LEVERAGE`, `STOP_LOSS_PCT`, `TAKE_PROFIT_PCT` : paramètres de gestion du risque.
    32: - `ATR_PERIOD`, `TRAIL_ATR_MULT`, `SCALE_IN_ATR_MULT`, `PROGRESS_MIN`, `TIMEOUT_MIN` : réglages pour l'ATR, l'ajout à la position, le trailing stop et la sortie par timeout.
    33: - `MAX_DAILY_LOSS_PCT`, `[REDACTED]`, `MAX_POSITIONS` (par défaut 3) : limites globales (kill switch après perte ou gain, nombre maximal de positions).
    34: - `LOG_DIR` : dossier où seront écrits les fichiers de log.
    35: - `ALLOWED_SYMBOLS` : liste de paires autorisées séparées par des virgules. Vide par défaut pour autoriser toutes les paires.
    36: 
    37: - `NOTIFY_URL` : URL d'un webhook HTTP pour recevoir les événements (optionnel, peut être utilisé en plus de Telegram).
    38: - `TELEGRAM_BOT_TOKEN=[REDACTED] `TELEGRAM_CHAT_ID` : pour envoyer les notifications sur Telegram (optionnel, peut être combiné avec le webhook).
    39: 
    40: Pour éviter de versionner vos clés sensibles, vous pouvez créer un fichier
    41: `.env` dans le dossier parent du dépôt (par exemple `Notebooks/.env` si le
    42: code se trouve dans `Notebooks/scalp`).  Ce fichier est automatiquement chargé
    43: au démarrage et toutes les variables qu'il contient seront disponibles pour le
    44: bot.
    45: 
    46: 
    47: Exemple :
    48: 
    49: ```bash
    50: export BITGET_ACCESS_KEY=[REDACTED]
    51: export BITGET_SECRET=[REDACTED]
    52: export PAPER_TRADE=true
    53: export TELEGRAM_BOT_TOKEN=[REDACTED]
    54: export TELEGRAM_CHAT_ID="123456789"
    55: python bot.py
    56: ```
    57: 
    58: ## Lancement
    59: 
    60: Après configuration, lancez simplement :
    61: 
    62: ```bash
    63: python bot.py
    64: ```
    65: 
    66: Le terminal reste silencieux au démarrage sauf en cas d'absence de variables critiques (`BITGET_ACCESS_KEY=[REDACTED] `BITGET_SECRET=[REDACTED] Les journaux sont écrits dans `logs/` et affichés sur la console. Le bot tourne jusqu'à `Ctrl+C`. Les ouvertures et fermetures de positions sont consignées dans `bot_events.jsonl`.
    67: 
    68: Lors du démarrage, deux notifications Telegram sont émises : la première affiche « Bot démarré » avec un logo, la seconde « Listing ok » sans détailler les paires sélectionnées.
    69: 
    70: Ensuite, un rappel du marché est envoyé chaque minute et l'interface Telegram propose un bouton « Fermer Bot » pour arrêter proprement l'exécution.
    71: 
    72: 
    73: ## Stratégie
    74: 
    75: Scalp cherche à capter de courts mouvements de tendance tout en coupant
    76: rapidement les pertes.
    77: 
    78: Principes généraux :
    79: 
    80: - sélection de paires liquides au fort momentum ;
    81: - trade uniquement dans le sens de la tendance dominante (MACD + EMA longue) ;
    82: - confirmation multi‑indicateurs (VWAP, volume/OBV, RSI multi‑UT) ;
    83: - stop‑loss et take‑profit dynamiques basés sur l’ATR avec taille de position
    84:   calculée selon le risque ;
    85: - limites quotidiennes pour protéger le capital.
    86: 
    87: Les règles détaillées et l’algorithme complet sont décrits dans
    88: `STRATEGY.md`.
    89: 
    90: ## Version
    91: 
    92: La version du bot est stockée dans le fichier `scalp/VERSION` et exposée dans
    93: le code via la variable `scalp.__version__` :
    94: 
    95: ```python
    96: from scalp import __version__
    97: print(__version__)
    98: ```
    99: 
   100: Pour incrémenter la version, utilisez `scalp.version.bump_version` avec
   101: 
   102: `"major"`, `"minor"` ou `"patch"` comme argument. La fonction
   103: `scalp.version.[REDACTED]` permet également de déterminer
   104: automatiquement l'incrément à appliquer à partir d'un message de commit
   105: suivant la convention [Conventional Commits](https://www.conventionalcommits.org).
   106: 
   107: Exemple d'incrément basé sur un message :
   108: 
   109: ```python
   110: from scalp.version import [REDACTED]
   111: [REDACTED]("feat: add new strategy")
   112: ```
   113: 
   114: Exécuté en tant que script, `python -m scalp.version` lit le dernier
   115: message de commit `git` et met à jour le fichier `VERSION` en
   116: conséquence.
   117: 
   118: La même opération peut être déclenchée depuis la ligne de commande via
   119: `cli.py` :
   120: 
   121: ```bash
   122: python cli.py bump-version
   123: ```
   124: 
   125: 
   126: ## Changelog
   127: 
   128: - Ajout d'un contrôle de marge disponible avant chaque ordre afin d'éviter l'erreur Bitget « The order amount exceeds the balance » (code 40762).
   129: 
   130: ## Avertissement
   131: 
   132: © 2025 — Usage à vos risques. Ceci n'est pas un conseil financier.


## STRATEGY.md (last modified: 2025-08-23 08:27:41)
     1: # Stratégie de trading
     2: 
     3: Ce document décrit la logique de trading utilisée par le bot **Scalp**. Elle vise un scalping court terme sur les futures USDT‑M de Bitget.
     4: 
     5: ## Principes généraux
     6: 
     7: - ne traiter que des actifs liquides à fort momentum ;
     8: - suivre la tendance dominante et éviter les marchés plats ;
     9: - utiliser des confirmations multi‑unités de temps pour limiter les faux signaux ;
--- [102/191] ./.ipynb_checkpoints/dump_repo-checkpoint.py ---
import os
from datetime import datetime
from pathlib import Path

IGNORE_EXTENSIONS = {'.log', '.pyc'}
IGNORE_DIRS = {'__pycache__'}


def _is_ignored(path: Path) -> bool:
    """Return True if the path should be ignored."""
    if any(part.startswith('.') for part in path.parts):
        return True
    if path.suffix in IGNORE_EXTENSIONS:
        return True
    if any(part in IGNORE_DIRS for part in path.parts):
        return True
    return False


def _build_tree(root: Path, ignore_path: Path) -> str:
    lines = []
    for dirpath, dirnames, filenames in os.walk(root):
        dirpath = Path(dirpath)
        dirnames[:] = [d for d in dirnames if not d.startswith('.') and d not in IGNORE_DIRS]
        depth = len(dirpath.relative_to(root).parts)
        indent = '    ' * depth
        lines.append(f"{indent}{dirpath.name}/")
        for fname in sorted(filenames):
            fpath = dirpath / fname
            if fpath == ignore_path or _is_ignored(fpath):
                continue
            lines.append(f"{indent}    {fname}")
    return '\n'.join(lines)


def _iter_files(root: Path):
    for path in sorted(root.rglob('*')):
        if path.is_file() and not _is_ignored(path):
            yield path


def create_dump_file(output_path: str = 'dump.txt', root: str = '.') -> None:
    """Create a text dump of the repository tree and file contents."""
    root_path = Path(root).resolve()
    output_path = root_path / output_path
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    with output_path.open('w', encoding='utf-8') as dump:
        dump.write(f"Dump created: {now}\n")
        dump.write('Repository tree:\n')
        dump.write(_build_tree(root_path, output_path))
        dump.write('\n\n')
        for file_path in _iter_files(root_path):
            rel_path = file_path.relative_to(root_path)
            if file_path == output_path:
                continue
            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime).strftime('%Y-%m-%d %H:%M:%S')
            dump.write(f"## {rel_path} (last modified: {mod_time})\n")
            try:
                with file_path.open('r', encoding='utf-8') as f:
                    for i, line in enumerate(f, 1):
                        dump.write(f"{i:6}: {line}")
            except Exception:
                dump.write('[unreadable file]\n')
            dump.write('\n\n')


if __name__ == '__main__':
    create_dump_file()
--- [103/191] ./.ipynb_checkpoints/sitecustomize-checkpoint.py ---
# sitecustomize.py
# Chargé automatiquement par Python s'il est sur le PYTHONPATH (racine du repo).
# - charge /notebooks/.env
# - AUTO-INSTALL des dépendances manquantes (core + dash + ccxt)
# - affiche un récap clair à la console (ok/installed/failed)
# - prépare DATA_ROOT (data/logs/reports)
# - écrit un green-flag JSON avec l'état des checks

from __future__ import annotations
import json
import os
from pathlib import Path

READY_PATH = Path("/notebooks/.scalp/READY.json")


def _load_dotenv_parent() -> None:
    try:
        from dotenv import load_dotenv  # type: ignore
        load_dotenv("/notebooks/.env")
        print("[i] .env chargé depuis /notebooks/.env")
    except Exception:
        print("[-] Impossible de charger /notebooks/.env (dotenv indisponible ?)")


def [REDACTED]() -> dict:
    """
    Installation auto, toujours activée (idempotente).
    Tu peux limiter via variables d'env si besoin:
      DISABLE_DASH=1  -> n'installe pas streamlit
      DISABLE_CCXT=1  -> n'installe pas ccxt
    """
    try:
        from engine.utils.bootstrap import ensure_dependencies  # type: ignore
    except Exception as e:
        print(f"[!] Bootstrap manquant: {e}")
        return {"bootstrap": "missing"}

    with_dash = os.getenv("DISABLE_DASH", "").lower() not in {"1", "true", "yes"}
    with_ccxt = os.getenv("DISABLE_CCXT", "").lower() not in {"1", "true", "yes"}

    print("[*] Vérification des dépendances (auto-install si nécessaire)...")
    deps = ensure_dependencies(with_dash=with_dash, with_ccxt=with_ccxt)

    # Affichage lisible
    ok, inst, fail = [], [], []
    for spec, status in deps.items():
        if status == "ok":
            ok.append(spec)
        elif status == "installed":
            inst.append(spec)
        else:
            fail.append((spec, status))

    if ok:
        print("    [OK]     " + ", ".join(ok))
    if inst:
        print("    [INST]   " + ", ".join(inst))
    if fail:
        print("    [FAILED] " + ", ".join(f"{s} -> {st}" for s, st in fail))

    return deps


def _paths_from_env() -> dict:
    data_root = os.getenv("DATA_ROOT", "/notebooks/scalp_data")
    d = Path(data_root)
    for sub in ("data", "logs", "reports"):
        try:
            (d / sub).mkdir(parents=True, exist_ok=True)
        except Exception as e:
            print(f"[!] Impossible de créer {d/sub}: {e}")
    print(f"[i] DATA_ROOT: {d}")
    return {
        "DATA_ROOT": str(d),
        "data_dir": str(d / "data"),
        "log_dir": str(d / "logs"),
        "reports_dir": str(d / "reports"),
    }


def _write_ready(payload: dict) -> None:
    try:
        READY_PATH.parent.mkdir(parents=True, exist_ok=True)
        READY_PATH.write_text(json.dumps(payload, indent=2), encoding="utf-8")
        print(f"[✓] Préflight OK — green-flag écrit: {READY_PATH}")
    except Exception as e:
        print(f"[!] Impossible d'écrire {READY_PATH}: {e}")


def _apply_env_aliases() -> None:
    try:
        from engine.config.loader import apply_env_aliases  # type: ignore
        apply_env_aliases()
    except Exception:
        pass


try:
    _load_dotenv_parent()
    _apply_env_aliases()
    deps = [REDACTED]()
    paths = _paths_from_env()

    # mini check secrets (non bloquant)
    miss = []
    if not (os.getenv("BITGET_ACCESS_KEY=[REDACTED] and os.getenv("BITGET_SECRET=[REDACTED] and os.getenv("BITGET_PASS=[REDACTED]
        miss.append("BITGET_*")
    if not (os.getenv("TELEGRAM_BOT_TOKEN=[REDACTED] and os.getenv("TELEGRAM_CHAT_ID")):
        miss.append("TELEGRAM_*")
    if miss:
        print("[-] Secrets manquants:", ", ".join(miss))

    _write_ready({"status": "ok", "deps": deps, "paths": paths, "missing": miss})
except Exception as e:
    print(f"[!] Préflight partiel: {e}")
    _write_ready({"status": "partial", "error": str(e)})--- [104/191] ./PROMPT.md ---
# Prompt de re-création du bot Scalp (version spot)

Ce fichier résume les modules et fonctions essentiels afin de recréer le bot de trading **spot** Bitget (paires USDT) à partir de zéro. Chaque fonction liste son rôle principal et les paramètres indispensables. Le fichier `.env` contenant les clés API se trouve dans le dossier parent du bot.

## Structure principale

### bot.py
- `_noop_event(*args, **kwargs)` : fonction vide pour le logging d'événements.
- `check_config()` : vérifie la présence des clés API Bitget et journalise un avertissement si elles manquent.
- `BitgetSpotClient` : sous-classe du client spot Bitget qui injecte `requests` et la fonction `log_event`.
- `[REDACTED](client, pairs, interval="1m", ema_fast_n=None, ema_slow_n=None)` : applique la stratégie EMA sur une liste de paires et renvoie les signaux.
- `send_selected_pairs(client, top_n=40, tg_bot=None)` : sélectionne et notifie les paires les plus actives.
- `update(client, top_n=40, tg_bot=None)` : rafraîchit la liste des paires et renvoie la charge utile envoyée.
- `main(argv=None)` : initialise la configuration, le client, le `RiskManager`, le bot Telegram et exécute la boucle de trading.

### cli.py
- `[REDACTED](pairs, timeframe, jobs)` : lance une optimisation paramétrique (exemple minimal).
- `[REDACTED](pair, timeframe, splits, train_ratio)` : exécute une analyse walk-forward.
- `run_live_pipeline(pairs, tfs)` : exécute la pipeline live asynchrone.
- `create_parser()` : construit l’analyseur d’arguments avec les sous-commandes `opt`, `walkforward`, `live` et `bump-version`.
- `main(argv=None)` : point d'entrée qui déclenche la commande choisie.

### init.py
- `install_packages(*args)` : installe des paquets via `pip`.
- `main()` : installe tous les fichiers `requirements*.txt` et `pytest`.

## Modules `scalp`

### bot_config.py
- `CONFIG` : dictionnaire global des paramètres (clés API, symbole, EMA, ATR, risques, etc.).

### metrics.py
- `calc_pnl_pct(entry_price, exit_price, side, fee_rate=0)` : pourcentage de PnL net des frais.
- `calc_rsi(prices, period=14)` : calcul du RSI (Wilder).
- `calc_atr(highs, lows, closes, period=14)` : ATR avec lissage de Wilder.
- `calc_macd(prices, fast=12, slow=26, signal=9)` : renvoie MACD, ligne signal et histogramme.
- `backtest_position(prices, entry_idx, exit_idx, side)` : valide qu’une position est cohérente avec le mouvement des prix.

### strategy.py
- `ema(series, window)` : moyenne mobile exponentielle.
- `vwap(highs, lows, closes, volumes)` : prix moyen pondéré par le volume.
- `obv(closes, volumes)` : série On Balance Volume.
- `cross(last_fast, last_slow, prev_fast, prev_slow)` : détecte les croisements EMA.
- `[REDACTED](bid_vol, ask_vol)` : mesure le déséquilibre du carnet d'ordres.
- `swing_levels(highs, lows, lookback)` : renvoie le dernier plus haut et plus bas.
- `Signal` : dataclass contenant `symbol`, `side`, `price`, `sl`, `tp1`, `tp2`, `qty`.
- `generate_signal(symbol, ohlcv, equity, risk_pct, ...)` : produit un `Signal` si toutes les conditions de stratégie sont réunies.
- `scan_pairs` et `select_active_pairs` sont re-exportés pour la sélection des paires.

### trade_utils.py
- `[REDACTED](equity_usdt, price, risk_pct, symbol=None)` : calcule la quantité à acheter/vendre en fonction du risque et du prix.
- `analyse_risque(open_positions, equity_usdt, price, risk_pct, symbol=None, side="long", risk_level=2)` : renvoie la taille de position conseillée selon l’exposition actuelle (sans effet de levier).
- `trailing_stop(side, current_price, atr, sl, mult=0.75)` : met à jour le stop loss en fonction de l'ATR.
- `break_even_stop(side, entry_price, current_price, atr, sl, mult=1.0)` : déplace le stop loss à break-even après un mouvement favorable.
- `should_scale_in(entry_price, current_price, last_entry, atr, side, distance_mult=0.5)` : indique si la position doit être renforcée.
- `timeout_exit(entry_time, now, entry_price, current_price, side, progress_min=15, timeout_min=30)` : ferme une position si aucune progression n’est constatée.
- `[REDACTED](side, best_bid, best_ask, slippage=0.001)` : calcule un prix limite pour une exécution quasi immédiate.

### risk
- `calc_risk_amount(equity, risk_pct)` : montant d'argent risqué sur un trade.
- `calc_position_size(equity, risk_pct, stop_distance)` : taille de position selon le stop.
- `adjust_risk_pct(risk_pct, win_streak, loss_streak, increase=0.12, decrease=0.25, min_pct=0.001, max_pct=0.05)` : ajuste le pourcentage de risque selon les séries de gains/pertes.
- `RiskManager` : classe gérant limites journalières, kill switch et ajustement de risque.
  - `reset_day()`, `register_trade(pnl_pct)`/`record_trade`, `dynamic_risk_pct(signal_quality, score)`, `apply_trailing(direction, price, sl, atr, params)`, `pause_duration()`, `can_open(current_positions)`.

### notifier.py
- `_pair_name(symbol)` : formatte le nom d’une paire.
- `_format_text(event, payload=None)` : construit un message lisible.
- `notify(event, payload=None)` : envoie des notifications via webhook HTTP et/ou Telegram.

### logging_utils.py
- `get_jsonl_logger(path, max_bytes=0, backup_count=0)` : renvoie une fonction de logging JSONL avec rotation optionnelle.
- `TradeLogger(csv_path, sqlite_path)` : enregistre chaque trade dans un CSV et une base SQLite (`log(data)`).

### bitget_client.py
- `BitgetSpotClient(access_key, secret_key, base_url, recv_window=30, paper_trade=True, requests_module=requests, log_event=None)` : client REST léger pour le marché spot.
  - `get_symbol_info(symbol=None)`, `get_kline(symbol, interval="1m", start=None, end=None)`, `get_ticker(symbol=None)`.
  - `_private_request(method, path, params=None, body=None)` : signe et exécute les requêtes privées.
  - `get_account()`, `get_open_orders(symbol=None)`.
  - `place_order(symbol, side, quantity, order_type, price=None, stop_loss=None, take_profit=None)`.
  - `cancel_order(symbol, order_id)`, `cancel_all(symbol)`.

### pairs.py
- `get_trade_pairs(client)` : récupère toutes les paires via `get_ticker`.
- `filter_trade_pairs(client, volume_min=5_000_000, max_spread_bps=5, top_n=40)` : filtre par volume/spread.
- `select_top_pairs(client, top_n=40, key="volume")` : trie par volume ou autre clé.
- `[REDACTED](client, pairs, interval="1m", ema_fast_n=None, ema_slow_n=None, ema_func=ema, cross_func=cross)` : signaux EMA croisement.
- `send_selected_pairs(client, top_n=40, select_fn=select_top_pairs, notify_fn=notify)` : déduplique USD/USDT/USDC et notifie la liste.
- `heat_score(volatility, volume, news=False)` : score combinant volatilite et volume.
- `[REDACTED](pairs, top_n=3)` : sélection des paires les plus "chaudes".
- `decorrelate_pairs(pairs, corr, threshold=0.8, top_n=3)` : choisit des paires peu corrélées.

### telegram_bot.py
- `TelegramBot(token, chat_id, client, config, risk_mgr, requests_module=requests)` : mini bot Telegram.
  - `send_main_menu(session_pnl)`, `update_pairs()`, `send(text, keyboard=None)`, `answer_callback(cb_id)`,
  - `fetch_updates()`, `handle_updates(session_pnl)`, `handle_callback(data, session_pnl)`.
  - Helpers privés `_base_symbol`, `[REDACTED]`, `_menu_text`.
- `init_telegram_bot(client, config, risk_mgr)` : instancie un `TelegramBot` si les variables d’environnement `TELEGRAM_BOT_TOKEN=[REDACTED] et `TELEGRAM_CHAT_ID` sont définies.

## Utilisation
1. Définir les variables d’environnement (clés Bitget, token Telegram, etc.).
2. Exécuter `init.py` pour installer les dépendances.
3. Lancer `bot.py` pour démarrer le trading.
4. Utiliser `cli.py` pour les outils d’optimisation ou de tests.

Ce résumé fournit les éléments nécessaires à la reconstruction du bot et à la compréhension de chaque fonction essentielle.

--- [105/191] ./engine/utils/ensure_deps.py ---
# engine/utils/ensure_deps.py
from __future__ import annotations
import importlib, subprocess, sys
from pathlib import Path
from typing import Iterable

DEFAULT_PKGS = [
    # ce set couvre l'app + termboard + dash
    "pandas", "numpy", "pyyaml", "requests", "tqdm",
    "aiohttp", "websockets",
    "rich", "streamlit", "plotly", "matplotlib",
    "python-telegram-bot",
]

def _installed(mod: str) -> bool:
    try:
        importlib.import_module(mod)
        return True
    except Exception:
        return False

def _pip_install(args: list[str]) -> int:
    return subprocess.call([sys.executable, "-m", "pip", "install", *args])

def ensure_minimal(extra: Iterable[str] = ()) -> None:
    """Installe ce qui manque (module par module) sans planter le boot."""
    missing = []
    # mapping module->pip-name si différent
    name_map = {
        "python-telegram-bot": "python-telegram-bot",
        "pyyaml": "pyyaml",
        "matplotlib": "matplotlib",
        "streamlit": "streamlit",
        "plotly": "plotly",
        "rich": "rich",
        "tqdm": "tqdm",
        "aiohttp": "aiohttp",
        "websockets": "websockets",
        "pandas": "pandas",
        "numpy": "numpy",
        "requests": "requests",
    }
    modules = list(DEFAULT_PKGS) + list(extra or [])
    for pkg in modules:
        mod = pkg
        # pour certains noms pip = module
        if pkg == "python-telegram-bot":
            mod = "telegram"
        if pkg == "pyyaml":
            mod = "yaml"
        if not _installed(mod):
            missing.append(name_map.get(pkg, pkg))
    if missing:
        try:
            print(f"[deps] installation manquante: {', '.join(missing)}")
            _pip_install(missing)
        except Exception as e:
            print(f"[deps] avertissement: installation partielle: {e}")

def [REDACTED]() -> None:
    """Si requirements.txt existe, tente un install (idempotent)."""
    req = Path(__file__).resolve().parents[2] / "requirements.txt"
    if req.exists():
        try:
            print("[deps] pip install -r requirements.txt (auto)")
            _pip_install(["-r", str(req)])
        except Exception as e:
            print(f"[deps] échec install -r requirements.txt: {e}")--- [106/191] ./engine/utils/io_safe.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
I/O sûres : écriture atomique, verrous, backups last-good.
"""

from __future__ import annotations
import os, json, time, tempfile, shutil

try:
    import fcntl  # POSIX only
    HAS_FCNTL = True
except Exception:
    HAS_FCNTL = False

class file_lock:
    def __init__(self, path: str):
        self.lock_path = path + ".lock"
        self.fd = None
    def __enter__(self):
        os.makedirs(os.path.dirname(self.lock_path), exist_ok=True)
        self.fd = os.open(self.lock_path, os.O_CREAT | os.O_RDWR)
        if HAS_FCNTL:
            fcntl.flock(self.fd, fcntl.LOCK_EX)
        return self
    def __exit__(self, exc_type, exc, tb):
        try:
            if HAS_FCNTL and self.fd is not None:
                fcntl.flock(self.fd, fcntl.LOCK_UN)
        finally:
            if self.fd is not None:
                os.close(self.fd)

def _atomic_write_bytes(data: bytes, path: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    dir_ = os.path.dirname(path)
    with tempfile.NamedTemporaryFile(dir=dir_, prefix=".tmp_", delete=False) as tf:
        tf.write(data)
        tmp = tf.name
    os.replace(tmp, path)

def atomic_write_text(text: str, path: str, encoding="utf-8"):
    _atomic_write_bytes(text.encode(encoding), path)

def atomic_write_json(obj, path: str, ensure_ascii=False):
    data = json.dumps(obj, ensure_ascii=ensure_ascii).encode("utf-8")
    _atomic_write_bytes(data, path)

def backup_last_good(path: str):
    if not os.path.isfile(path):
        return None
    ts = int(time.time())
    bdir = os.path.join(os.path.dirname(path), "last_good")
    os.makedirs(bdir, exist_ok=True)
    bname = os.path.basename(path) + f".bak_{ts}"
    bpath = os.path.join(bdir, bname)
    shutil.copy2(path, bpath)
    return bpath--- [107/191] ./engine/utils/walkforward.py ---
from __future__ import annotations
from dataclasses import dataclass
from typing import List
import pandas as pd

@dataclass
class Segment:
    train_start: pd.Timestamp
    train_end: pd.Timestamp
    test_start: pd.Timestamp
    test_end: pd.Timestamp

def make_segments(index: pd.DatetimeIndex, train_days: int, test_days: int, segments: int) -> List[Segment]:
    if index.tz is None: index = index.tz_localize("UTC")
    start = index.min(); end = index.max()
    segs: List[Segment] = []
    cur_train_start = start
    for _ in range(segments):
        train_end = cur_train_start + pd.Timedelta(days=train_days)
        test_end = train_end + pd.Timedelta(days=test_days)
        if test_end > end: break
        segs.append(Segment(cur_train_start, train_end, train_end, test_end))
        cur_train_start = cur_train_start + pd.Timedelta(days=test_days)
    return segs--- [108/191] ./engine/utils/params.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations
from typing import Dict, Any

def [REDACTED](rt: Dict[str, Any]) -> Dict[str, Any]:
    """
    Params finaux = [REDACTED] ∪ strategy_profiles[risk_mode]
    """
    base = dict(rt.get("[REDACTED]") or {})
    profiles = rt.get("strategy_profiles") or {}
    mode = (rt.get("risk_mode") or "normal").strip().lower()
    prof = dict(profiles.get(mode) or {})
    return {**base, **prof}--- [109/191] ./engine/utils/retry.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations
import time
from typing import Callable, Any, Type, Iterable

def retry(fn: Callable[..., Any],
          tries: int = 3,
          delay: float = 1.0,
          backoff: float = 1.5,
          exceptions: Iterable[Type[BaseException]] = (Exception,),
          on_retry: Callable[[int, BaseException], None] | None = None,
          *args, **kwargs) -> Any:
    att = 0
    cur_delay = delay
    while True:
        try:
            return fn(*args, **kwargs)
        except exceptions as e:
            att += 1
            if att >= tries:
                raise
            if on_retry:
                on_retry(att, e)
            time.sleep(cur_delay)
            cur_delay *= backoff--- [110/191] ./engine/utils/.ipynb_checkpoints/bootstrap-checkpoint.py ---
# engine/utils/bootstrap.py
from __future__ import annotations
import importlib
import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Dict, Tuple

_MARK_DIR = Path("/notebooks/.scalp")
_MARK_DIR.mkdir(parents=True, exist_ok=True)
STATE = _MARK_DIR / "DEPS.json"

# import_name -> pip_spec
CORE_REQS: Dict[str, str] = {
    "requests": "requests>=2.31",
    "pandas": "pandas>=2.1",
    "numpy": "numpy>=1.26",
    "yaml": "PyYAML>=6.0",
    "dotenv": "python-dotenv>=1.0",
}
DASH_REQS: Dict[str, str] = {"streamlit": "streamlit>=1.33"}
CCXT_REQS: Dict[str, str] = {"ccxt": "ccxt>=4.0.0"}


def _need(import_name: str) -> bool:
    try:
        importlib.import_module(import_name)
        return False
    except Exception:
        return True


def _pip(spec: str) -> Tuple[bool, str]:
    try:
        # upgrade pip une seule fois par session (léger)
        if not getattr(_pip, "_upgraded", False):
            subprocess.run([sys.executable, "-m", "pip", "install", "--upgrade", "pip"],
                           check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            _pip._upgraded = True  # type: ignore[attr-defined]
        proc = subprocess.run([sys.executable, "-m", "pip", "install", spec],
                              capture_output=True, text=True, check=False)
        ok = (proc.returncode == 0)
        return ok, (proc.stdout + proc.stderr)[-3000:]
    except Exception as e:
        return False, f"pip failed: {e}"


def ensure_dependencies(*, with_dash: bool = True, with_ccxt: bool = True) -> Dict[str, str]:
    """
    Idempotent: installe seulement ce qui manque.
    Écrit l'état dans /notebooks/.scalp/DEPS.json
    """
    plan: Dict[str, str] = {}
    reqs = dict(CORE_REQS)
    if with_dash:
        reqs.update(DASH_REQS)
    if with_ccxt:
        reqs.update(CCXT_REQS)

    for import_name, spec in reqs.items():
        if _need(import_name):
            ok, log_tail = _pip(spec)
            plan[spec] = "installed" if ok else f"failed: {log_tail}"
        else:
            plan[spec] = "ok"

    try:
        STATE.write_text(json.dumps(plan, indent=2), encoding="utf-8")
    except Exception:
        pass
    return plan--- [111/191] ./engine/utils/__init__.py ---
--- [112/191] ./engine/utils/bootstrap.py ---
# engine/utils/bootstrap.py
from __future__ import annotations
import importlib
import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Dict, Tuple

_MARK_DIR = Path("/notebooks/.scalp")
_MARK_DIR.mkdir(parents=True, exist_ok=True)
STATE = _MARK_DIR / "DEPS.json"

# import_name -> pip_spec
CORE_REQS: Dict[str, str] = {
    "requests": "requests>=2.31",
    "pandas": "pandas>=2.1",
    "numpy": "numpy>=1.26",
    "yaml": "PyYAML>=6.0",
    "dotenv": "python-dotenv>=1.0",
}
DASH_REQS: Dict[str, str] = {"streamlit": "streamlit>=1.33"}
CCXT_REQS: Dict[str, str] = {"ccxt": "ccxt>=4.0.0"}


def _need(import_name: str) -> bool:
    try:
        importlib.import_module(import_name)
        return False
    except Exception:
        return True


def _pip(spec: str) -> Tuple[bool, str]:
    try:
        # upgrade pip une seule fois par session (léger)
        if not getattr(_pip, "_upgraded", False):
            subprocess.run([sys.executable, "-m", "pip", "install", "--upgrade", "pip"],
                           check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            _pip._upgraded = True  # type: ignore[attr-defined]
        proc = subprocess.run([sys.executable, "-m", "pip", "install", spec],
                              capture_output=True, text=True, check=False)
        ok = (proc.returncode == 0)
        return ok, (proc.stdout + proc.stderr)[-3000:]
    except Exception as e:
        return False, f"pip failed: {e}"


def ensure_dependencies(*, with_dash: bool = True, with_ccxt: bool = True) -> Dict[str, str]:
    """
    Idempotent: installe seulement ce qui manque.
    Écrit l'état dans /notebooks/.scalp/DEPS.json
    """
    plan: Dict[str, str] = {}
    reqs = dict(CORE_REQS)
    if with_dash:
        reqs.update(DASH_REQS)
    if with_ccxt:
        reqs.update(CCXT_REQS)

    for import_name, spec in reqs.items():
        if _need(import_name):
            ok, log_tail = _pip(spec)
            plan[spec] = "installed" if ok else f"failed: {log_tail}"
        else:
            plan[spec] = "ok"

    try:
        STATE.write_text(json.dumps(plan, indent=2), encoding="utf-8")
    except Exception:
        pass
    return plan--- [113/191] ./engine/utils/logging_setup.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations
import os, sys, json, logging, time

class JsonFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        payload = {
            "ts": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(record.created)),
            "lvl": record.levelname,
            "name": record.name,
            "msg": record.getMessage(),
        }
        if record.exc_info:
            payload["exc"] = self.formatException(record.exc_info)
        return json.dumps(payload, ensure_ascii=False)

def setup_logger(name: str, log_path: str) -> logging.Logger:
    os.makedirs(os.path.dirname(log_path), exist_ok=True)
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    fh = logging.FileHandler(log_path, encoding="utf-8")
    sh = logging.StreamHandler(sys.stdout)
    jf = JsonFormatter()
    fh.setFormatter(jf); sh.setFormatter(jf)
    logger.addHandler(fh); logger.addHandler(sh)
    return logger--- [114/191] ./engine/strategy.py ---
"""Core trading strategy components for scalping EMA/VWAP/RSI/ATR.

This module implements a minimal but functional version of the strategy
outlined in the project specification.  The focus is on pure Python
implementations so the logic can easily be unit tested without requiring
external services or heavy third‑party dependencies.

The strategy is deliberately stateless; functions operate on passed data and
return simple data structures.  This makes it easy to plug the logic into
real‑time trading loops or backtest engines.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Sequence, List, Dict, Optional, Tuple, Any

from .metrics import calc_rsi, calc_atr, calc_pnl_pct, calc_macd
from .risk import calc_position_size

# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def ema(series: Sequence[float], window: int) -> List[float]:
    """Return the exponential moving average of *series*.

    The first value is the raw input to remain consistent with most trading
    platforms.  ``window`` must be positive; when it equals ``1`` the input is
    returned unchanged.
    """

    if window <= 1 or not series:
        return list(series)
    k = 2.0 / (window + 1.0)
    out: List[float] = [float(series[0])]
    prev = out[0]
    for x in series[1:]:
        prev = float(x) * k + prev * (1.0 - k)
        out.append(prev)
    return out

def vwap(highs: Sequence[float], lows: Sequence[float],
         closes: Sequence[float], volumes: Sequence[float]) -> float:
    """Compute the volume weighted average price (VWAP).

    Parameters
    ----------
    highs, lows, closes, volumes: Sequence[float]
        Matching sequences for the period considered.
    """

    tp_vol = 0.0
    vol_sum = 0.0
    for h, low, c, v in zip(highs, lows, closes, volumes):
        tp = (h + low + c) / 3.0
        tp_vol += tp * v
        vol_sum += v
    return tp_vol / vol_sum if vol_sum else 0.0

def obv(closes: Sequence[float], volumes: Sequence[float]) -> List[float]:
    """Return the On Balance Volume (OBV) series."""

    if not closes:
        return []
    out: List[float] = [0.0]
    for i in range(1, len(closes)):
        if closes[i] > closes[i - 1]:
            out.append(out[-1] + volumes[i])
        elif closes[i] < closes[i - 1]:
            out.append(out[-1] - volumes[i])
        else:
            out.append(out[-1])
    return out


def cross(last_fast: float, last_slow: float, prev_fast: float, prev_slow: float) -> int:
    """Detect a crossing between two series.

    Returns ``1`` for a bullish crossover, ``-1`` for a bearish crossover and
    ``0`` otherwise.
    """

    if prev_fast <= prev_slow and last_fast > last_slow:
        return 1
    if prev_fast >= prev_slow and last_fast < last_slow:
        return -1
    return 0


def [REDACTED](bid_vol: float, ask_vol: float) -> float:
    """Compute order book imbalance.

    The value is normalised between ``-1`` and ``1`` where positive numbers
    indicate bid dominance.  ``0`` is returned when both volumes are zero.
    """

    total = bid_vol + ask_vol
    return (bid_vol - ask_vol) / total if total else 0.0


def swing_levels(
    highs: Sequence[float], lows: Sequence[float], lookback: int
) -> Tuple[float, float]:
    """Return the most recent swing high and swing low.

    ``lookback`` defines how many completed candles are inspected.  The current
    candle is excluded to avoid look‑ahead bias.
    """

    if len(highs) < lookback + 1 or len(lows) < lookback + 1:
        return highs[-1], lows[-1]
    high = max(highs[-lookback - 1 : -1])
    low = min(lows[-lookback - 1 : -1])
    return high, low

# ---------------------------------------------------------------------------
# Pair selection
# ---------------------------------------------------------------------------

# The first and second level pair selection helpers now live in
# :mod:`engine.selection`.  They are re-exported here for backward compatibility
# and to keep the public API unchanged.
from .selection.scanner import scan_pairs  # noqa: E402
from .selection.momentum import select_active_pairs  # noqa: E402

# ---------------------------------------------------------------------------
# Signal generation
# ---------------------------------------------------------------------------

@dataclass
class Signal:
    """Trading signal with risk parameters."""

    symbol: str
    side: int  # 1 for long, -1 for short
    entry: float
    sl: float
    tp1: float
    tp2: float
    qty: float = 0.0
    score: Optional[float] = None
    quality: Optional[float] = None
    reasons: Optional[List[str]] = None

    def __post_init__(self) -> None:  # pragma: no cover - simple coercion
        if isinstance(self.side, str):
            self.side = 1 if self.side.lower() in {"long", "buy", "1", "true"} else -1

    @property
    def price(self) -> float:
        return self.entry


def _generate_signal(
    symbol: str,
    ohlcv: Dict[str, Sequence[float]],
    *,
    equity: float,
    risk_pct: float,
    ohlcv_15m: Optional[Dict[str, Sequence[float]]] = None,
    ohlcv_1h: Optional[Dict[str, Sequence[float]]] = None,
    order_book: Optional[Dict[str, float]] = None,
    tick_ratio_buy: Optional[float] = None,
    atr_disable_pct: float = 0.2,
    atr_reduce_pct: float = 2.0,
    swing_lookback: int = 5,
    macd_fast: int = 12,
    macd_slow: int = 26,
    macd_signal: int = 9,
    trend_ema_period: int = 200,
) -> Optional[Signal]:
    """Return a trading :class:`Signal` if conditions are met.

    ``ohlcv`` must contain ``open``, ``high``, ``low``, ``close`` and ``volume``
    sequences ordered from oldest to newest.  The function checks the following
    rules:

    * price positioned relative to VWAP and EMA20/EMA50 trend
    * RSI(14) crossing key levels (40/60)
    * OBV rising or high short‑term volume
    * Multi time frame confirmation (H1 EMA50 slope, RSI15 >/< 50)
    * Micro‑structure breakout of last swing high/low
    * MACD trend filter
    * Long‑term trend via configurable EMA filter
    * Order book imbalance and tape filters
    * Dynamic ATR‑based stop‑loss and take‑profit
    * Position sizing via ``calc_position_size``
    """

    closes = [float(x) for x in ohlcv.get("close", [])]
    highs = [float(x) for x in ohlcv.get("high", [])]
    lows = [float(x) for x in ohlcv.get("low", [])]
    vols = [float(x) for x in ohlcv.get("volume", [])]
    if len(closes) < 60 or len(highs) != len(lows) or len(closes) != len(highs):
        return None

    price = closes[-1]
    ema20 = ema(closes, 20)
    ema50 = ema(closes, 50)
    ema_trend = ema(closes, trend_ema_period)
    v = vwap(highs, lows, closes, vols)
    obv_series = obv(closes, vols)
    obv_rising = obv_series[-1] > obv_series[-2]
    vol_last3 = sum(vols[-3:])
    vol_ma20 = sum(vols[-20:]) / 20.0
    vol_rising = vol_last3 > vol_ma20

    macd_val, macd_sig, _ = calc_macd(
        closes, fast=macd_fast, slow=macd_slow, signal=macd_signal
    )

    # Multi timeframe filters -------------------------------------------------
    trend_dir = 0  # 1 = long only, -1 = short only, 0 = neutral
    if ohlcv_1h:
        h_closes = [float(x) for x in ohlcv_1h.get("close", [])]
        if len(h_closes) >= 52:
            h_ema50 = ema(h_closes, 50)
            if len(h_ema50) >= 2:
                slope = h_ema50[-1] - h_ema50[-2]
                if slope > 0:
                    trend_dir = 1
                elif slope < 0:
                    trend_dir = -1

    rsi_15 = None
    if ohlcv_15m:
        m_closes = [float(x) for x in ohlcv_15m.get("close", [])]
        if len(m_closes) >= 15:
            rsi_15 = calc_rsi(m_closes, 14)

    # RSI crossing logic (5m)
    rsi_curr = calc_rsi(closes[-15:], 14)
    rsi_prev = calc_rsi(closes[-16:-1], 14)

    atr = calc_atr(highs, lows, closes, 14)
    atr_pct = atr / price * 100.0 if price else 0.0
    if atr_pct < atr_disable_pct:
        return None
    size_mult = 0.5 if atr_pct > atr_reduce_pct else 1.0

    sl_dist = 0.5 * atr
    tp1_dist = 1.0 * atr
    tp2_dist = 1.5 * atr

    swing_high, swing_low = swing_levels(highs, lows, swing_lookback)

    obi_ok_long = obi_ok_short = True
    if order_book is not None:
        bid = float(order_book.get("bid_vol_aggreg", 0))
        ask = float(order_book.get("ask_vol_aggreg", 0))
        obi = [REDACTED](bid, ask)
        obi_ok_long = obi > 0.1
        obi_ok_short = obi < -0.1

    tick_ok_long = tick_ratio_buy is None or tick_ratio_buy > 0.55
    tick_ok_short = tick_ratio_buy is None or tick_ratio_buy < 0.45

    def _size(dist: float) -> float:
        return calc_position_size(equity, risk_pct, dist) * size_mult
    weights = {
        "ema": 15.0,
        "macd": 15.0,
        "vwap": 15.0,
        "rsi": 15.0,
        "obv": 10.0,
        "swing": 10.0,
        "atr": 20.0,
    }

    atr_score = min(atr_pct / atr_reduce_pct, 1.0) * weights["atr"]

    long_score = atr_score
    long_reasons: List[str] = []
    if price > v:
        long_score += weights["vwap"]
        long_reasons.append("vwap")
    if ema20[-1] > ema50[-1]:
        long_score += weights["ema"]
        long_reasons.append("ema")
    if rsi_prev <= 40 < rsi_curr:
        long_score += weights["rsi"]
        long_reasons.append("rsi")
    if macd_val > macd_sig:
        long_score += weights["macd"]
        long_reasons.append("macd")
    if obv_rising or vol_rising:
        long_score += weights["obv"]
        long_reasons.append("obv")
    if price > swing_high:
        long_score += weights["swing"]
        long_reasons.append("swing")

    short_score = atr_score
    short_reasons: List[str] = []
    if price < v:
        short_score += weights["vwap"]
        short_reasons.append("vwap")
    if ema20[-1] < ema50[-1]:
        short_score += weights["ema"]
        short_reasons.append("ema")
    if rsi_prev >= 60 > rsi_curr:
        short_score += weights["rsi"]
        short_reasons.append("rsi")
    if macd_val < macd_sig:
        short_score += weights["macd"]
        short_reasons.append("macd")
    if obv_series[-1] < obv_series[-2] or vol_rising:
        short_score += weights["obv"]
        short_reasons.append("obv")
    if price < swing_low:
        short_score += weights["swing"]
        short_reasons.append("swing")

    side: Optional[str] = None
    score: float = 0.0
    reasons: List[str] = []
    if (
        long_score >= short_score
        and long_score > 0
        and macd_val > macd_sig
        and obi_ok_long
        and tick_ok_long
        and trend_dir >= 0
        and price > ema_trend[-1]
    ):
        side = "long"
        score = long_score
        reasons = long_reasons
        sl = price - sl_dist
        tp1 = price + tp1_dist
        tp2 = price + tp2_dist
    elif (
        short_score > long_score
        and short_score > 0
        and macd_val < macd_sig
        and obi_ok_short
        and tick_ok_short
        and trend_dir <= 0
        and price < ema_trend[-1]
    ):
        side = "short"
        score = short_score
        reasons = short_reasons
        sl = price + sl_dist
        tp1 = price - tp1_dist
        tp2 = price - tp2_dist
    else:
        return None

    qty = _size(sl_dist)
    return Signal(symbol, side, price, sl, tp1, tp2, qty, score, score, reasons)


def generate_signal(*args, **kwargs) -> Optional[Signal]:
    if "config" in kwargs:
        config = kwargs.pop("config")
        symbol = kwargs.pop("symbol", None)
        ohlcv = kwargs.pop("ohlcv", None)
        if ohlcv is None:
            raise TypeError("ohlcv argument required")
        return _generate_signal(
            symbol or ohlcv.get("symbol", ""),
            ohlcv,
            equity=kwargs.pop("equity", 0.0),
            risk_pct=getattr(config, "RISK_PCT", 0.0),
            **kwargs,
        )
    return _generate_signal(*args, **kwargs)

# ---------------------------------------------------------------------------
# Backtesting utilities
# ---------------------------------------------------------------------------

def max_drawdown(equity_curve: Sequence[float]) -> float:
    peak = equity_curve[0]
    mdd = 0.0
    for x in equity_curve:
        if x > peak:
            peak = x
        dd = (peak - x) / peak * 100.0
        if dd > mdd:
            mdd = dd
    return mdd

def backtest(
    trades: Sequence[Dict[str, Any]],
    *,
    equity_start: float = 1_000.0,
    fee_rate: float = 0.0,
) -> Dict[str, float]:
    """Evaluate a list of trade dictionaries.

    Each trade must provide ``symbol``, ``entry``, ``exit``, ``side`` and may
    optionally include ``duration`` in minutes.  Results are aggregated into
    common performance metrics to quickly evaluate the strategy.
    """

    equity = equity_start
    equity_curve = [equity]
--- [115/191] ./engine/hooks/prewarm_cache.py ---
# -*- coding: utf-8 -*-
"""
Pré-chauffe léger du cache OHLCV.

Objectif: ne PAS bloquer le lancement. On log juste un statut "warmup OK"
pour chaque symbole, et on s'assure que le dossier data existe.
Si tu veux rebrancher un vrai downloader plus tard, expose simplement une
fonction `prewarm_cache(cfg, symbols, timeframe, out_dir)` avec la même
signature.
"""
from __future__ import annotations
from pathlib import Path
from typing import Iterable


def prewarm_cache(cfg: dict, symbols: Iterable[str], timeframe: str, out_dir: str | Path) -> None:
    out = Path(out_dir)
    out.mkdir(parents=True, exist_ok=True)
    for sym in symbols:
        # Marqueur vide; permet à d’autres services de voir que le symbole est "préparé"
        (out / f"{sym}-{timeframe}.csv").touch(exist_ok=True)
        print(f"[cache] warmup OK for {sym}")--- [116/191] ./engine/core/indicators.py ---
from __future__ import annotations
import pandas as pd

def ema(s: pd.Series, n: int) -> pd.Series:
    return s.ewm(span=max(1,int(n)), adjust=False).mean()

def macd(close: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9):
    macd_line = ema(close, fast) - ema(close, slow)
    signal_line = ema(macd_line, signal)
    hist = macd_line - signal_line
    return macd_line, signal_line, hist

def atr(df: pd.DataFrame, n: int = 14) -> pd.Series:
    high, low, close = df["high"], df["low"], df["close"]
    prev_close = close.shift(1)
    tr = (high - low).abs().combine((high - prev_close).abs(), max).combine((low - prev_close).abs(), max)
    return tr.rolling(max(1,int(n))).mean()--- [117/191] ./engine/core/signals.py ---
# engine/core/signals.py
from __future__ import annotations
import pandas as pd

def _ema(series: pd.Series, period: int) -> pd.Series:
    return series.ewm(span=period, adjust=False).mean()

def _atr(df: pd.DataFrame, period: int = 14) -> pd.Series:
    high, low, close = df["high"], df["low"], df["close"]
    prev_close = close.shift(1)
    tr = pd.concat([
        high - low,
        (high - prev_close).abs(),
        (low - prev_close).abs()
    ], axis=1).max(axis=1)
    return tr.rolling(period).mean()

def compute_signals(df: pd.DataFrame, params: dict[str, float]) -> pd.DataFrame:
    """
    Applique EMA crossover + ATR.
    Retourne DataFrame avec colonnes: ts, open, high, low, close, volume, ema_fast, ema_slow, atr, signal
    signal = +1 long / -1 short / 0 neutre
    """
    fast = int(params.get("ema_fast", 20))
    slow = int(params.get("ema_slow", 50))
    atr_period = int(params.get("atr_period", 14))

    out = df.copy()
    out["ema_fast"] = _ema(out["close"], fast)
    out["ema_slow"] = _ema(out["close"], slow)
    out["atr"] = _atr(out, atr_period)

    sig = 0
    signals = []
    for f, s in zip(out["ema_fast"], out["ema_slow"]):
        if pd.isna(f) or pd.isna(s):
            signals.append(0)
        elif f > s and sig <= 0:
            sig = 1
            signals.append(1)
        elif f < s and sig >= 0:
            sig = -1
            signals.append(-1)
        else:
            signals.append(sig)
    out["signal"] = signals
    return out--- [118/191] ./engine/core/signal.py ---
from __future__ import annotations
import pandas as pd
from .indicators import ema, atr

def compute_signals(df: pd.DataFrame, params: dict) -> pd.DataFrame:
    fast = int(params.get("ema_fast", 20))
    slow = int(params.get("ema_slow", 50))
    atr_n = int(params.get("atr_period", 14))
    df = df.copy()
    df["ema_fast"] = ema(df["close"], fast)
    df["ema_slow"] = ema(df["close"], slow)
    df["atr"] = atr(df, atr_n)
    # signal = +1 si croisement haussier, -1 si baissier, 0 sinon
    cond_up = (df["ema_fast"] > df["ema_slow"]) & (df["ema_fast"].shift(1) <= df["ema_slow"].shift(1))
    cond_dn = (df["ema_fast"] < df["ema_slow"]) & (df["ema_fast"].shift(1) >= df["ema_slow"].shift(1))
    df["signal"] = 0
    df.loc[cond_up, "signal"] = 1
    df.loc[cond_dn, "signal"] = -1
    return df--- [119/191] ./engine/strategies/twolayer_scalp.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TwoLayer_Scalp — couche régime multi‑TF + couche entrées paramétrables.
Supporte:
  - JSON unique (héritage) OU 2 JSON scindés (recommandé).

Expose:
  - [REDACTED](schema_json, [REDACTED], schema_entries_json)
  - [REDACTED](df_by_tf, schema_backtest)
  - build_entries_frame(df_entry_tf, schema_entries)
  - run_backtest_exec(df_exec, p_buy, schema_backtest, schema_entries)
"""

from __future__ import annotations
from dataclasses import dataclass
from typing import Dict, Optional, Tuple
import math, json
import numpy as np
import pandas as pd

# -------- Indicateurs compacts (pas de dépendances exotiques) ----------
def ema(s: pd.Series, period: int) -> pd.Series:
    return s.ewm(span=period, adjust=False).mean()

def macd(s: pd.Series, fast=12, slow=26, signal=9):
    ema_f = ema(s, fast); ema_s = ema(s, slow)
    macd_line = ema_f - ema_s
    signal_line = ema(macd_line, signal)
    hist = macd_line - signal_line
    return macd_line, signal_line, hist

def rsi(s: pd.Series, length=14):
    d = s.diff()
    up = d.clip(lower=0.0); dn = -d.clip(upper=0.0)
    roll_up = up.ewm(alpha=1/length, adjust=False).mean()
    roll_dn = dn.ewm(alpha=1/length, adjust=False).mean()
    rs = roll_up / (roll_dn + 1e-12)
    return 100.0 - 100.0 / (1.0 + rs)

def _true_range(df: pd.DataFrame):
    pc = df["close"].shift(1)
    return pd.concat([
        df["high"] - df["low"],
        (df["high"] - pc).abs(),
        (df["low"] - pc).abs()
    ], axis=1).max(axis=1)

def atr(df: pd.DataFrame, length=14):
    return _true_range(df).rolling(length, min_periods=length).mean()

def bbands(s: pd.Series, length=20, std=2.0):
    m = s.rolling(length).mean()
    sd = s.rolling(length).std(ddof=0)
    u = m + std * sd
    l = m - std * sd
    return m, u, l

def vwap(df: pd.DataFrame):
    pv = (df["close"] * df["volume"]).cumsum()
    vv = df["volume"].cumsum().replace(0, np.nan)
    return pv / vv

def keltner(df: pd.DataFrame, length=20, atr_mult=1.5):
    mid = ema(df["close"], length)
    rng = atr(df, length)
    return mid, mid + atr_mult*rng, mid - atr_mult*rng

def obv(df: pd.DataFrame):
    direction = np.sign(df["close"].diff().fillna(0.0))
    return (direction * df["volume"]).fillna(0.0).cumsum()

def adx(df: pd.DataFrame, length=14):
    up = df["high"].diff()
    dn = -df["low"].diff()
    plus_dm = np.where((up > dn) & (up > 0), up, 0.0)
    minus_dm = np.where((dn > up) & (dn > 0), dn, 0.0)
    tr = _true_range(df)
    atr_n = tr.rolling(length).mean()
    plus_di = 100 * (pd.Series(plus_dm, index=df.index).rolling(length).sum() / (atr_n*length + 1e-12))
    minus_di = 100 * (pd.Series(minus_dm, index=df.index).rolling(length).sum() / (atr_n*length + 1e-12))
    dx = (abs(plus_di - minus_di) / (plus_di + minus_di + 1e-12)) * 100
    return dx.rolling(length).mean().rename("adx")

# -------- Helpers score régime ----------------------------------------
def _tanh_norm(x: pd.Series, win: int = 100) -> pd.Series:
    mu = x.rolling(win).mean()
    sd = x.rolling(win).std(ddof=0)
    z = (x - mu) / (sd + 1e-12)
    return np.tanh(z)

@dataclass
class Costs:
    maker: float
    taker: float
    slip_bps: float

def [REDACTED](df_by_tf: Dict[str, pd.DataFrame], schema_backtest: Dict) -> pd.Series:
    reg = schema_backtest["regime_layer"]
    inds = reg["indicators"]
    atr_gate = reg.get("[REDACTED]", {})
    norm_kind = reg.get("score_normalization", "tanh")

    # poids par TF
    keys = list(df_by_tf.keys())
    w = np.array([reg["tf_weights"].get(tf, 0.0) for tf in keys], dtype=float)
    w = w / w.sum() if w.sum() > 0 else np.ones_like(w)/max(1, len(w))

    tf_scores = []
    for tf, df in df_by_tf.items():
        ef = ema(df["close"], inds["ema"]["fast"].get(tf, 12))
        es = ema(df["close"], inds["ema"]["slow"].get(tf, 26))
        el = ema(df["close"], inds["ema"]["long"].get(tf, 200))
        _, _, hist = macd(df["close"],
                          inds["macd"]["fast"].get(tf,12),
                          inds["macd"]["slow"].get(tf,26),
                          inds["macd"]["signal"].get(tf,9))
        rsi_v = rsi(df["close"], inds["rsi"]["length"].get(tf,14))
        adx_v = adx(df, reg["indicators"]["adx"]["length"])
        obv_v = obv(df)
        obv_slope = obv_v.diff(reg["indicators"]["obv"]["slope_lookback"]).fillna(0.0)

        f_ema = (ef - es) / (el + 1e-12)
        if norm_kind == "tanh":
            f_ema = _tanh_norm(f_ema, inds["macd"].get("norm_window", 100))
            f_macd = _tanh_norm(hist, inds["macd"].get("norm_window", 100))
            f_rsi  = ((rsi_v - 50.0)/50.0).clip(-1,1)
            f_adx  = (adx_v.fillna(0.0)/100.0).clip(0,1)
            f_obv  = _tanh_norm(obv_slope, reg["indicators"]["obv"]["[REDACTED]"]).clip(-1,1)
        else:
            f_macd, f_rsi, f_adx, f_obv = hist, (rsi_v-50)/50, adx_v/100.0, obv_slope

        w_ema = inds["ema"]["weight"]; w_macd = inds["macd"]["weight"]
        w_rsi = inds["rsi"]["weight"]; w_adx  = inds["adx"]["weight"]
        w_obv = inds["obv"]["weight"]
        score = w_ema*f_ema + w_macd*f_macd + w_rsi*f_rsi + w_adx*f_adx + w_obv*f_obv

        a = atr(df).fillna(method="ffill")
        gate = (a / (df["close"] + 1e-12) >= atr_gate.get(tf, 0.0)).astype(float)
        tf_scores.append(score * gate)

    M = pd.concat(tf_scores, axis=1).fillna(method="ffill")
    M.columns = keys
    raw = (M * w).sum(axis=1)
    p_buy = (raw.clip(-1,1) + 1.0) / 2.0
    return p_buy.clip(0,1)

def hysteresis_state(p_buy: pd.Series, hyst: Dict[str, float]) -> pd.Series:
    buy_on = hyst["buy_on_above"]; buy_exit = hyst["buy_exit_below"]
    sell_on = hyst.get("sell_on_above", buy_on); sell_exit = hyst.get("sell_exit_below", buy_exit)
    st = 0
    out = []
    for v in p_buy.fillna(0.0).values:
        if st >= 0:
            if v >= buy_on: st = 1
            elif v <= buy_exit: st = 0
        if st <= 0:
            if v <= (1 - sell_on): st = -1
            elif v >= (1 - sell_exit): st = 0
        out.append(st)
    return pd.Series(out, index=p_buy.index)

# -------- Entrées sets --------------------------------------------------
def [REDACTED](df: pd.DataFrame, common: Dict, spec: Dict) -> pd.DataFrame:
    out = pd.DataFrame(index=df.index)
    out["ema20"] = ema(df["close"], 20)
    vwp = vwap(df)
    bbm, bbu, bbl = bbands(df["close"], common["bbands"]["length"], common["bbands"]["stddev"])
    rsi_len = spec["signals"]["rsi_cross"]["length"]; lvl = spec["signals"]["rsi_cross"]["level"]
    rsi_v = rsi(df["close"], rsi_len); rsi_prev = rsi_v.shift(1)
    touch = (df["low"] <= out["ema20"]) | (df["low"] <= vwp) | (df["low"] <= bbl)
    rsi_cross_up = (rsi_prev < lvl) & (rsi_v >= lvl)
    body = (df["close"] - df["open"]).abs()
    body_ok = body >= body.rolling(50).median()
    if not spec["signals"].get("[REDACTED]", True):
        body_ok = pd.Series(True, index=df.index)
    cond = touch & rsi_cross_up & body_ok
    out["entry_long"] = cond.astype(int)
    a = atr(df, 14)
    out["sl"] = df["close"] - spec["risk"]["sl_atr_mult"] * a
    out["tp"] = df["close"] + spec["risk"]["tp_atr_mult"] * a
    out["entry_set"] = "pullback_trend"
    return out[["entry_long","sl","tp","entry_set"]]

def _entry_breakout(df: pd.DataFrame, common: Dict, spec: Dict) -> pd.DataFrame:
    n = spec["signals"]["close_above_high_n"]
    hh = df["high"].rolling(n).max().shift(1)
    _, _, hist = macd(df["close"])
    macd_up = hist > hist.shift(1)
    bbm, bbu, bbl = bbands(df["close"], common["bbands"]["length"], common["bbands"]["stddev"])
    km, ku, kl = keltner(df, common["keltner"]["length"], common["keltner"]["atr_mult"])
    inside = (bbu < ku) & (bbl > kl); expand = (bbu > ku) & (bbl < kl)
    squeeze_ok = inside.shift(1).fillna(False) & expand.fillna(False)
    cond = (df["close"] > hh) & macd_up & squeeze_ok
    out = pd.DataFrame(index=df.index)
    out["entry_long"] = cond.astype(int)
    a = atr(df, 14)
    rl = df["low"].rolling(n).min().shift(1)
    if spec["risk"].get("sl_method","") == "below_range_n_low":
        out["sl"] = rl
    else:
        out["sl"] = df["close"] - spec["risk"].get("sl_atr_mult", 1.0) * a
    if spec["risk"].get("tp_method","") in ("range_height_or_atr",):
        rh = (df["high"].rolling(n).max().shift(1) - df["low"].rolling(n).min().shift(1)).abs()
        out["tp"] = df["close"] + np.maximum(rh, spec["risk"].get("tp_atr_mult",1.5)*a)
    else:
        out["tp"] = df["close"] + spec["risk"].get("tp_atr_mult",1.5)*a
    out["entry_set"] = "breakout"
    return out[["entry_long","sl","tp","entry_set"]]

_ENTRY_DISPATCH = {
    "pullback_trend": [REDACTED],
    "breakout": _entry_breakout,
}

def build_entries_frame(df_entry_tf: pd.DataFrame, schema_entries: Dict) -> pd.DataFrame:
    sets = schema_entries["entry_layer"]["sets"]; common = schema_entries["entry_layer"]["common"]
    parts = []
    for name in sets.keys():
        if name in _ENTRY_DISPATCH:
            parts.append(_ENTRY_DISPATCH[name](df_entry_tf, common, sets[name]))
    if not parts:
        return pd.DataFrame(index=df_entry_tf.index, data={"entry_long":0,"sl":np.nan,"tp":np.nan,"entry_set":""})
    out = pd.DataFrame(index=df_entry_tf.index)
    out["entry_long"] = pd.concat([p["entry_long"] for p in parts], axis=1).max(axis=1).astype(int)
    out["sl"] = parts[0]["sl"]; out["tp"] = parts[0]["tp"]
    first = []
    for i in range(len(df_entry_tf)):
        tag = ""
        for p in parts:
            if int(p["entry_long"].iloc[i]) == 1:
                tag = str(p["entry_set"].iloc[0]) if "entry_set" in p.columns else ""
                break
        first.append(tag)
    out["entry_set"] = first
    return out

# -------- Backtest exécution -------------------------------------------
def [REDACTED](p_buy: pd.Series, schema_backtest: Dict) -> pd.Series:
    return hysteresis_state(p_buy, schema_backtest["regime_layer"]["hysteresis"])

def run_backtest_exec(df_exec: pd.DataFrame, p_buy: pd.Series, schema_backtest: Dict, schema_entries: Dict) -> Dict:
    state = [REDACTED](p_buy, schema_backtest)
    timeout_bars = schema_entries["entry_layer"]["common"]["timeout_bars"]
    maker = schema_backtest["costs"].get("maker_fee_rate", 0.0002)
    taker = schema_backtest["costs"].get("taker_fee_rate", 0.0008)
    slip = schema_backtest["costs"].get("slippage_max_bps", 5.0) / 10000.0
    prefer_maker = schema_entries.get("execution", {}).get("prefer_maker", True)
    [REDACTED] = schema_entries.get("execution", {}).get("[REDACTED]", True)
    risk_pct = schema_backtest["risk_management"]["position_sizing"]["risk_pct_per_trade"]

    in_pos = False; entry_px = 0.0; sl = np.nan; tp = np.nan
    eq = 1.0; curve = [eq]; rets = []; bars = 0

    for i in range(len(df_exec)):
        c = df_exec["close"].iloc[i]; hi = df_exec["high"].iloc[i]; lo = df_exec["low"].iloc[i]
        sig = int(df_exec["entry_long"].iloc[i]) if "entry_long" in df_exec else 0
        st = int(state.iloc[i])

        if in_pos:
            bars += 1
            hit_tp = hi >= tp; hit_sl = lo <= sl; timeout = bars >= timeout_bars
            if hit_tp or hit_sl or timeout or (st <= 0):
                fee = taker
                exit_px = tp if hit_tp else (sl if hit_sl else c)
                exit_px = exit_px * (1 - fee) * (1 - slip)
                r = (exit_px - entry_px) / entry_px
                rets.append(r); eq *= (1 + r * risk_pct); curve.append(eq)
                in_pos = False; bars = 0
                continue

        if (not in_pos) and (st > 0) and (sig == 1):
            fee = maker if prefer_maker else taker
            if [REDACTED] and df_exec.get("entry_set", pd.Series(index=df_exec.index, data="")).iloc[i] == "breakout":
                fee = taker
            entry_px = c * (1 + fee) * (1 + slip)
            sl = df_exec["sl"].iloc[i]; tp = df_exec["tp"].iloc[i]
            in_pos = True; bars = 0

        curve.append(eq)

    trades = len(rets); wins = sum(1 for r in rets if r > 0)
    pf = (sum(r for r in rets if r > 0) / (sum(-r for r in rets if r <= 0) + 1e-12)) if trades else 0.0
    wr = wins / trades if trades else 0.0
    arr = np.array(curve, dtype=float); roll = np.maximum.accumulate(arr)
    dd = (arr - roll) / (roll + 1e-12)
    mdd = -dd.min() if len(dd) else 0.0
    if trades > 1:
        mu = float(np.mean(rets)); sd = float(np.std(rets, ddof=1) + 1e-12)
        sharpe = (mu / sd) * math.sqrt(252)
    else:
        sharpe = 0.0
    return {"pf": float(pf), "mdd": float(mdd), "trades": int(trades), "wr": float(wr), "sharpe": float(sharpe), "equity": float(arr[-1])}

# -------- Chargement schémas (avec fallback schemas/) ------------------
def [REDACTED](schema_json: Optional[str],
                           [REDACTED]: Optional[str],
                           schema_entries_json: Optional[str]) -> Tuple[Dict, Dict]:
    """
    Renvoie (schema_backtest, schema_entries)
    Priorité :
      1) schema_json (JSON unique)
      2) [REDACTED] + schema_entries_json
      3) defaults: <repo_root>/schemas/schema_backtest.json + schema_entries.json
    """
    import os, json as _json
    if schema_json:
        with open(schema_json, "r", encoding="utf-8") as f:
            full = _json.load(f)
        back = {
            "schema_version": full.get("schema_version", ""),
            "strategy_name": full.get("strategy_name", "TwoLayer_Scalp"),
            "assets": full.get("assets", []),
            "timeframes": full.get("timeframes", {}),
            "regime_layer": full.get("regime_layer", {}),
            "risk_management": full.get("risk_management", {}),
            "costs": full.get("costs", {}),
            "backtest": full.get("backtest", {}),
            "optimization": full.get("optimization", {}),
            "outputs": full.get("outputs", {})
        }
        entries = {"entry_layer": full.get("entry_layer", {}), "execution": full.get("execution", {})}
        return back, entries

    if [REDACTED] and schema_entries_json:
        with open([REDACTED], "r", encoding="utf-8") as f:
            back = _json.load(f)
        with open(schema_entries_json, "r", encoding="utf-8") as f:
            entries = _json.load(f)
        return back, entries

    # Fallback défaut: schemas/ à la racine du repo
    here = os.path.abspath(os.path.dirname(__file__))           # .../engine/strategies
    repo_root = os.path.abspath(os.path.join(here, "..", "..")) # racine repo
    sb_def = os.path.join(repo_root, "schemas", "schema_backtest.json")
    se_def = os.path.join(repo_root, "schemas", "schema_entries.json")
    if os.path.isfile(sb_def) and os.path.isfile(se_def):
        with open(sb_def, "r", encoding="utf-8") as f:
            back = _json.load(f)
        with open(se_def, "r", encoding="utf-8") as f:
            entries = _json.load(f)
        return back, entries

    raise FileNotFoundError(
        "Aucun schéma fourni et fichiers par défaut introuvables. "
        "Place `schemas/schema_backtest.json` et `schemas/schema_entries.json` à la racine, "
        "ou passe --schema-json / --schema-backtest + --schema-entries."
    )--- [120/191] ./engine/strategies/registry.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Strategy registry: map strategy name -> class
"""

from __future__ import annotations
from typing import Dict, Type
from .base import StrategyBase
from .ema_atr_v1 import EmaAtrV1  # <- casse corrigée

# Registry of available strategies
_REGISTRY: Dict[str, Type[StrategyBase]] = {
    "ema_atr_v1": EmaAtrV1,
    # "two_layer_lite": TwoLayerLite,  # prochainement
}

def create(name: str, params: dict) -> StrategyBase:
    if name not in _REGISTRY:
        raise ValueError(f"Unknown strategy: {name} · disponibles: {list(_REGISTRY)}")
    return _REGISTRY[name](params)

def list_strategies() -> list[str]:
    return list(_REGISTRY.keys())--- [121/191] ./engine/strategies/base.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Base classes for strategies
"""

from __future__ import annotations
from typing import Dict, Any
import numpy as np
import pandas as pd

class Metrics:
    """Container for backtest metrics."""
    def __init__(self, pf: float, mdd: float, trades: int,
                 wr: float, sharpe: float):
        self.pf = pf
        self.mdd = mdd
        self.trades = trades
        self.wr = wr
        self.sharpe = sharpe

    @classmethod
    def from_trades(cls, pnl: np.ndarray) -> "Metrics":
        """Compute metrics from an array of trade PnLs."""
        if pnl is None or len(pnl) == 0:
            return cls(0.0, 1.0, 0, 0.0, 0.0)

        trades = len(pnl)
        wins = (pnl > 0).sum()
        losses = (pnl < 0).sum()

        wr = wins / trades if trades > 0 else 0.0
        gross_profit = pnl[pnl > 0].sum()
        gross_loss = -pnl[pnl < 0].sum()
        pf = (gross_profit / gross_loss) if gross_loss > 0 else float("inf")

        eq = pnl.cumsum()
        peak = np.maximum.accumulate(eq)
        dd = (peak - eq).max() if len(eq) > 0 else 0.0
        mdd = dd / (peak.max() + 1e-9) if peak.max() > 0 else 1.0

        sharpe = pnl.mean() / (pnl.std() + 1e-9) * np.sqrt(252) if trades > 1 else 0.0

        return cls(pf, mdd, trades, wr, sharpe)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "pf": self.pf,
            "mdd": self.mdd,
            "trades": self.trades,
            "wr": self.wr,
            "sharpe": self.sharpe,
        }


class StrategyBase:
    """Base class for all strategies."""

    def __init__(self, params: Dict[str, Any]):
        self.params = params or {}

    def backtest(self, df: pd.DataFrame) -> Metrics:
        """Run backtest on OHLCV dataframe. Override in subclasses."""
        raise NotImplementedError

    def describe(self) -> Dict[str, Any]:
        return {"name": self.__class__.__name__, **self.params}--- [122/191] ./engine/strategies/__init__.py ---
--- [123/191] ./engine/strategies/ema_atr_v1.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from __future__ import annotations
import numpy as np
from typing import Dict, Any, Optional
from .base import StrategyBase, Metrics


class EmaAtrV1(StrategyBase):
    """
    EMA/ATR v1 (simple, costs-aware)
    - Signaux: croisement EMA(fast)/EMA(slow)
    - PnL: variation sur N barres (lookahead_bars)
    - Coûts par trade: 2*(fee) + 2*(slippage_bps/10_000)
    """

    name = "ema_atr_v1"

    def __init__(self, params: Optional[Dict[str, Any]] = None):
        super().__init__(
            params or {
                "ema_fast": 12,
                "ema_slow": 34,
                "lookahead_bars": 10,
                "taker_fee_rate": 0.0008,
                "maker_fee_rate": 0.0002,
                "prefer_maker": True,
                "slippage_bps": 5.0,
            }
        )

    @staticmethod
    def _ema(a: np.ndarray, n: int) -> np.ndarray:
        alpha = 2.0 / (n + 1.0)
        out = np.empty_like(a); out[:] = np.nan
        prev = a[0]
        for i, x in enumerate(a):
            prev = alpha * x + (1 - alpha) * prev
            out[i] = prev
        for i in range(1, n):
            out[i] = np.nan
        return out

    def _total_cost_rate(self) -> float:
        prefer_maker = bool(self.params.get("prefer_maker", True))
        maker_fee = float(self.params.get("maker_fee_rate", 0.0002))
        taker_fee = float(self.params.get("taker_fee_rate", 0.0008))
        fee = maker_fee if prefer_maker else taker_fee
        slip = float(self.params.get("slippage_bps", 5.0)) / 10000.0
        return 2.0 * fee + 2.0 * slip

    def backtest(self, df) -> Metrics:
        if df is None or len(df) < 200:
            return Metrics(1.0, 0.5, 0, 0.0, 0.0)

        price = df["close"].values.astype("float64")
        fast = int(self.params.get("ema_fast", 12))
        slow = int(self.params.get("ema_slow", 34))
        look = int(self.params.get("lookahead_bars", 10))

        ema_f = self._ema(price, fast)
        ema_s = self._ema(price, slow)

        longs  = (ema_f[1:] >= ema_s[1:]) & (ema_f[:-1] < ema_s[:-1])
        shorts = (ema_f[1:] <= ema_s[1:]) & (ema_f[:-1] > ema_s[:-1])

        pnl = []
        trades = 0
        cost = self._total_cost_rate()

        for i, sig in enumerate(longs, start=1):
            if sig and i + look < len(price):
                r = (price[i + look] - price[i]) / price[i]
                pnl.append(r - cost); trades += 1

        for i, sig in enumerate(shorts, start=1):
            if sig and i + look < len(price):
                r = (price[i] - price[i + look]) / price[i]
                pnl.append(r - cost); trades += 1

        if trades == 0:
            return Metrics(1.0, 0.5, 0, 0.0, 0.0)

        pnl = np.array(pnl, dtype="float64")
        wins = (pnl > 0).sum()
        wr = float(wins) / float(trades)
        gain = pnl[pnl > 0].sum()
        loss = -pnl[pnl < 0].sum()
        pf = (gain / loss) if loss > 1e-12 else 2.0

        eq = (1.0 + pnl).cumprod()
        peak = np.maximum.accumulate(eq)
        dd = 1.0 - eq / peak
        mdd = float(np.nanmax(dd)) if len(dd) else 0.0

        std = pnl.std()
        sharpe = float(pnl.mean() / std) if std > 1e-12 else 0.0

        return Metrics(float(pf), float(mdd), int(trades), float(wr), float(sharpe))--- [124/191] ./engine/client.py ---
import logging
from typing import Any, Dict, Optional

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry


class HTTPError(RuntimeError):
    """Raised when an HTTP request fails"""


class HttpClient:
    """Simple HTTP client with persistent session and retry logic.

    The client exposes a :py:meth:`close` method and implements the context
    manager protocol so it can be used with ``with`` statements to ensure
    that the underlying :class:`requests.Session` is properly closed.
    """

    def __init__(
        self,
        base_url: str,
        *,
        timeout: float = 10.0,
        max_retries: int = 3,
        backoff_factor: float = 0.3,
        status_forcelist: Optional[list[int]] = None,
    ) -> None:
        self.base_url = base_url.rstrip("/")
        self.timeout = timeout
        self.session = requests.Session()
        retry = Retry(
            total=max_retries,
            backoff_factor=backoff_factor,
            status_forcelist=status_forcelist or [429, 500, 502, 503, 504],
            allowed_methods=[
                "HEAD",
                "GET",
                "OPTIONS",
                "POST",
                "PUT",
                "DELETE",
                "PATCH",
            ],
        )
        adapter = HTTPAdapter(max_retries=retry)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

    def close(self) -> None:
        """Close the underlying :class:`requests.Session`."""
        self.session.close()

    # ------------------------------------------------------------------
    # Context manager support
    # ------------------------------------------------------------------
    def __enter__(self) -> "HttpClient":
        return self

    def __exit__(self, exc_type, exc, tb) -> None:  # type: ignore[override]
        self.close()

    def request(
        self,
        method: str,
        path: str,
        *,
        params: Optional[Dict[str, Any]] = None,
        json: Optional[Dict[str, Any]] = None,
        headers: Optional[Dict[str, str]] = None,
    ) -> Dict[str, Any]:
        """Perform an HTTP request and return JSON data.

        Errors during the request raise ``HTTPError``. If the response cannot
        be decoded as JSON, a dictionary describing the issue is returned.
        """
        url = f"{self.base_url}{path}"
        try:
            resp = self.session.request(
                method,
                url,
                params=params,
                json=json,
                headers=headers,
                timeout=self.timeout,
            )
            resp.raise_for_status()
        except requests.RequestException as exc:  # network or HTTP errors
            msg = f"HTTP error calling {url}: {exc}"
            logging.error(msg)
            raise HTTPError(msg) from exc

        try:
            return resp.json()
        except ValueError:  # invalid JSON
            msg = "Invalid JSON in response"
            logging.error("%s for %s: %s", msg, url, resp.text)
            return {"success": False, "error": msg, "text": resp.text}
--- [125/191] ./engine/strategy/factory.py ---
annulé--- [126/191] ./engine/risk/__init__.py ---
# scalp/risk/__init__.py
from .manager import (
    Caps,
    compute_size,
    calc_position_size,  # alias legacy
    RiskManager,         # shim legacy
)

__all__ = ["Caps", "compute_size", "calc_position_size", "RiskManager"]--- [127/191] ./engine/risk/manager.py ---
# scalp/risk/manager.py
from __future__ import annotations
from dataclasses import dataclass
from typing import Optional, Dict, Any

@dataclass
class Caps:
    min_qty: float = 0.0
    min_notional: float = 0.0
    max_leverage: float = 20.0

def _get_caps(caps_by_symbol: Optional[Dict[str, Any]], symbol: str) -> Caps:
    if not caps_by_symbol:
        return Caps()
    c = caps_by_symbol.get(symbol, {})
    return Caps(
        min_qty=float(c.get("min_qty", 0.0) or 0.0),
        min_notional=float(c.get("min_notional", 0.0) or 0.0),
        max_leverage=float(c.get("max_leverage", 20.0) or 20.0),
    )

def compute_size(
    *,
    symbol: str,
    price: float,
    balance_cash: float,
    risk_pct: float = 0.5,
    caps_by_symbol: Optional[Dict[str, Any]] = None,
) -> float:
    """Sizing robuste avec gardes min_notional / min_qty."""
    price = max(1e-9, float(price))
    balance_cash = max(0.0, float(balance_cash))
    risk_pct = max(0.0, float(risk_pct))

    notionnel = balance_cash * risk_pct
    qty = notionnel / price

    caps = _get_caps(caps_by_symbol, symbol)
    if caps.min_notional > 0 and (qty * price) < caps.min_notional:
        qty = caps.min_notional / price
    if caps.min_qty > 0 and qty < caps.min_qty:
        qty = caps.min_qty
    return max(0.0, qty)

# --- Shims pour compatibilité ancienne API -----------------------------------

def calc_position_size(symbol: str, price: float, balance_cash: float,
                       risk_pct: float = 0.5,
                       caps_by_symbol: Optional[Dict[str, Any]] = None) -> float:
    """Alias legacy → compute_size."""
    return compute_size(
        symbol=symbol, price=price, balance_cash=balance_cash,
        risk_pct=risk_pct, caps_by_symbol=caps_by_symbol
    )

class RiskManager:
    """
    Shim minimal compatible avec l'ancien code:
      rm = RiskManager(risk_pct=0.5, caps_by_symbol={...})
      qty = rm.size(symbol, price, balance_cash)
    """
    def __init__(self, risk_pct: float = 0.5, caps_by_symbol: Optional[Dict[str, Any]] = None):
        self.risk_pct = float(risk_pct)
        self.caps_by_symbol = caps_by_symbol or {}

    def size(self, symbol: str, price: float, balance_cash: float) -> float:
        return compute_size(
            symbol=symbol, price=price, balance_cash=balance_cash,
            risk_pct=self.risk_pct, caps_by_symbol=self.caps_by_symbol
        )--- [128/191] ./engine/adapters/bitget.py ---
# scalp/adapters/bitget.py
from __future__ import annotations
from typing import Any, Dict, List, Optional
import inspect, os
import requests

# Client bas-niveau fourni par le repo
from engine.bitget_client import BitgetFuturesClient as _Base


def _to_float(x, default: float = 0.0) -> float:
    try:
        return float(x)
    except Exception:
        return default


def _select_base_url() -> str:
    env = os.environ.get("BITGET_BASE_URL")
    if env:
        return env
    paper = os.environ.get("PAPER_TRADE", "true").lower() in ("1", "true", "yes", "on")
    return "https://api-testnet.bitget.com" if paper else "https://api.bitget.com"


class BitgetFuturesClient(_Base):
    """
    Adaptateur Bitget:
      - __init__ dynamique (passe seulement les kwargs que le client accepte)
      - Normalisations robustes: assets, ticker(s), positions, fills
    """

    # --------------------- INIT dynamique ---------------------
    def __init__(self, *args: Any, **kwargs: Any) -> None:
        """
        Accepte indifféremment:
          api_key/apiKey/access_key/accessKey/key
          api_secret/apiSecret/secret/secret_key/secretKey
          passphrase/password/api_passphrase/apiPassphrase
          base_url/baseUrl/host/endpoint (ou auto)
        On n'envoie au client de base que les noms présents dans sa signature.
        """
        user_kwargs = dict(kwargs)

        # Collecte des valeurs possibles (tous alias)
        incoming_key = [REDACTED]
            user_kwargs.pop("api_key", None)
            or user_kwargs.pop("apiKey", None)
            or user_kwargs.pop("access_key", None)
            or user_kwargs.pop("accessKey", None)
            or user_kwargs.pop("key", None)
            or user_kwargs.pop("API_KEY=[REDACTED] None)
        )
        incoming_secret = [REDACTED]
            user_kwargs.pop("api_secret", None)
            or user_kwargs.pop("apiSecret", None)
            or user_kwargs.pop("secret_key", None)
            or user_kwargs.pop("secretKey", None)
            or user_kwargs.pop("secret", None)
            or user_kwargs.pop("API_SECRET=[REDACTED] None)
        )
        incoming_pass = (
            user_kwargs.pop("passphrase", None)
            or user_kwargs.pop("password", None)
            or user_kwargs.pop("api_passphrase", None)
            or user_kwargs.pop("apiPassphrase", None)
        )
        incoming_base = (
            user_kwargs.pop("base_url", None)
            or user_kwargs.pop("baseUrl", None)
            or user_kwargs.pop("host", None)
            or user_kwargs.pop("endpoint", None)
            or _select_base_url()
        )

        # Signature réelle du client bas-niveau
        sig = inspect.signature(_Base.__init__)
        param_names = set(sig.parameters.keys())  # ex: {'self','access_key','secret_key','passphrase','base_url',...}

        def pick_name(cands: List[str]) -> Optional[str]:
            for c in cands:
                if c in param_names:
                    return c
            return None

        # Noms réellement supportés
        key_name = pick_name(["api_key", "apiKey", "access_key", "accessKey", "key"])
        sec_name = pick_name(["api_secret", "apiSecret", "secret_key", "secretKey", "secret"])
        pas_name = pick_name(["passphrase", "password", "api_passphrase", "apiPassphrase"])
        base_name = pick_name(["base_url", "baseUrl", "host", "endpoint"])
        req_mod_name = "requests_module" if "requests_module" in param_names else None

        # Construire kwargs à transmettre (une seule fois par nom)
        base_kwargs: Dict[str, Any] = {}
        if key_name and incoming_key is not None:
            base_kwargs[key_name] = incoming_key
        if sec_name and incoming_secret is not None:
            base_kwargs[sec_name] = incoming_secret
        if pas_name and incoming_pass is not None:
            base_kwargs[pas_name] = incoming_pass
        if base_name:
            base_kwargs[base_name] = incoming_base
        if req_mod_name:
            base_kwargs[req_mod_name] = requests

        # Ne transmettre aucun doublon : si user_kwargs contient un nom supporté
        # qui n'a pas été défini ci-dessus, on le relaie.
        for k, v in list(user_kwargs.items()):
            if k in param_names and k not in base_kwargs:
                base_kwargs[k] = v

        # Appel propre, 100% mots-clés (évite “missing positional arg” et “multiple values”)
        super().__init__(**base_kwargs)

    # --------------------- COMPTES / ASSETS ---------------------
    def get_assets(self) -> Dict[str, Any]:
        raw = super().get_assets()
        data = raw.get("data") or raw.get("result") or raw.get("assets") or []
        norm: List[Dict[str, Any]] = []
        for a in data:
            currency = a.get("currency") or a.get("marginCoin") or a.get("coin") or "USDT"
            equity = _to_float(a.get("equity", a.get("usdtEquity", a.get("totalEquity", 0))))
            available = _to_float(a.get("available", a.get("availableBalance", a.get("availableUSDT", 0))))
            norm.append({"currency": currency, "equity": equity, "available": available, **a})
        return {"success": True, "data": norm}

    # ------------------------ TICKER(S) -------------------------
    def get_ticker(self, symbol: Optional[str] = None) -> Dict[str, Any]:
        """
        Normalise vers liste d'objets: {symbol,lastPrice,bidPrice,askPrice,volume}
        Tolère top-level dict/list et items dict/list.
        """
        try:
            raw: Any = super().get_ticker(symbol) if symbol else super().get_tickers()
        except Exception as e:
            return {"success": False, "error": repr(e), "data": []}

        items: List[Any] = []
        if isinstance(raw, dict):
            d = raw.get("data")
            if symbol and isinstance(d, dict):
                items = [d]
            else:
                items = d or raw.get("result") or raw.get("tickers") or []
        elif isinstance(raw, (list, tuple)):
            items = list(raw)

        norm: List[Dict[str, Any]] = []
        for t in items:
            if isinstance(t, dict):
                s = (t.get("symbol") or t.get("instId") or t.get("instrumentId") or "").replace("_", "")
                last_ = t.get("lastPrice", t.get("last", t.get("close", t.get("markPrice", 0))))
                bid_ = t.get("bidPrice", t.get("bestBidPrice", t.get("bestBid", t.get("buyOne", last_))))
                ask_ = t.get("askPrice", t.get("bestAskPrice", t.get("bestAsk", t.get("sellOne", last_))))
                vol_usdt = t.get("usdtVolume", t.get("quoteVolume", t.get("turnover24h", None)))
                vol_base = t.get("baseVolume", t.get("volume", t.get("size24h", 0)))
                volume = _to_float(vol_usdt if vol_usdt is not None else vol_base)
                norm.append({
                    "symbol": s,
                    "lastPrice": _to_float(last_),
                    "bidPrice": _to_float(bid_),
                    "askPrice": _to_float(ask_),
                    "volume": volume
                })
            else:
                seq = list(t)
                if len(seq) >= 5:
                    first_ts = isinstance(seq[0], (int, float)) and seq[0] > 10**10
                    if first_ts:
                        close = _to_float(seq[4]); vol = _to_float(seq[5] if len(seq) > 5 else 0.0)
                    else:
                        close = _to_float(seq[3]); vol = _to_float(seq[4] if len(seq) > 4 else 0.0)
                else:
                    close = _to_float(seq[-1] if seq else 0.0); vol = 0.0
                s = (symbol or "").replace("_", "")
                norm.append({"symbol": s, "lastPrice": close, "bidPrice": close, "askPrice": close, "volume": vol})

        return {"success": True, "data": norm}

    # --------------- POSITIONS / ORDRES / FILLS -----------------
    def get_open_positions(self, symbol: Optional[str] = None) -> Dict[str, Any]:
        raw: Dict[str, Any] = super().get_positions() if hasattr(super(), "get_positions") else {}
        items = raw.get("data") or raw.get("result") or raw.get("positions") or []
        out: List[Dict[str, Any]] = []
        for p in items:
            s = (p.get("symbol") or p.get("instId") or "").replace("_", "")
            if symbol and s != symbol:
                continue
            side = (p.get("holdSide") or p.get("posSide") or p.get("side") or "").lower()
            qty = _to_float(p.get("size", p.get("holdAmount", p.get("total", 0))))
            avg = _to_float(p.get("avgOpenPrice", p.get("avgPrice", p.get("entryPrice", 0))))
            out.append({"symbol": s, "side": side, "qty": qty, "avgEntryPrice": avg})
        return {"success": True, "data": out}

    def get_fills(self, symbol: str, order_id: Optional[str] = None, limit: int = 100) -> Dict[str, Any]:
        raw: Dict[str, Any] = super().get_fills(symbol=symbol) if hasattr(super(), "get_fills") else {}
        items = raw.get("data") or raw.get("result") or []
        out: List[Dict[str, Any]] = []
        for f in items[:limit]:
            s = (f.get("symbol") or f.get("instId") or "").replace("_", "")
            if s != symbol:
                continue
            if order_id and str(f.get("orderId") or f.get("ordId") or "") != str(order_id):
                continue
            out.append({
                "orderId": str(f.get("orderId") or f.get("ordId") or ""),
                "tradeId": str(f.get("tradeId") or f.get("fillId") or f.get("execId") or ""),
                "price": _to_float(f.get("price", f.get("fillPx", 0))),
                "qty": _to_float(f.get("size", f.get("fillSz", 0))),
                "fee": _to_float(f.get("fee", f.get("fillFee", 0))),
                "ts": int(f.get("ts", f.get("time", 0))),
            })
        return {"success": True, "data": out}

    def cancel_order(self, symbol: str, order_id: str) -> Dict[str, Any]:
        raw = super().cancel_order(symbol=symbol, orderId=order_id) if hasattr(super(), "cancel_order") else {}
        ok = bool(raw.get("success", True)) if isinstance(raw, dict) else True
        return {"success": ok, "data": {"orderId": order_id}}--- [129/191] ./engine/adapters/market_data.py ---
# scalper/backtest/market_data.py
from __future__ import annotations

import os
from pathlib import Path
from typing import Any

import pandas as pd

BT_DEBUG = int(os.getenv("BT_DEBUG", "0") or "0")

def _log(msg: str) -> None:
    if BT_DEBUG:
        print(f"[bt.debug] {msg}", flush=True)

def _csv_path(data_dir: str | Path, symbol: str, timeframe: str) -> Path:
    root = Path(data_dir)
    root.mkdir(parents=True, exist_ok=True)
    tf = timeframe.replace(":", "")
    return root / f"{symbol}-{tf}.csv"

def _read_csv(path: Path) -> pd.DataFrame:
    _log(f"lecture CSV: {path}")
    df = pd.read_csv(path)
    ts_col = next((c for c in df.columns if c.lower() in ("ts", "timestamp", "time", "date")), None)
    if ts_col is None:
        raise ValueError("Colonne temps introuvable (timestamp/time/date)")
    df = df.rename(columns={ts_col: "timestamp"})
    df["timestamp"] = pd.to_datetime(df["timestamp"], utc=True, [REDACTED]=True)
    df = df.set_index("timestamp").sort_index()
    _log(f"→ CSV ok, n={len(df)}, t0={df.index.min()}, t1={df.index.max()}")
    return df

def _write_csv(path: Path, df: pd.DataFrame) -> None:
    tmp = df.reset_index().rename(columns={"index": "timestamp"})
    if "timestamp" not in tmp.columns:
        tmp = tmp.rename(columns={"index": "timestamp"})
    tmp.to_csv(path, index=False)
    _log(f"écrit CSV: {path} (n={len(df)})")

def [REDACTED](exchange: Any, symbol: str, timeframe: str, *, limit: int = 1000) -> pd.DataFrame:
    _log(f"fetch via exchange.fetch_ohlcv: symbol={symbol} tf={timeframe} limit={limit}")
    raw = exchange.fetch_ohlcv(symbol, timeframe=timeframe, limit=limit)  # peut être sync ou adapté
    # Normalisation minimaliste (liste de listes)
    rows = []
    for r in raw:
        ts = int(r[0])
        unit = "ms" if ts > 10_000_000_000 else "s"
        ts = pd.to_datetime(ts, unit=unit, utc=True)
        rows.append([ts, float(r[1]), float(r[2]), float(r[3]), float(r[4]), float(r[5])])
    df = pd.DataFrame(rows, columns=["timestamp", "open", "high", "low", "close", "volume"]).set_index("timestamp").sort_index()
    _log(f"→ exchange ok, n={len(df)}, t0={df.index.min()}, t1={df.index.max()}")
    return df

def [REDACTED](exchange: Any, data_dir: str = "data", *, api_limit: int = 1000):
    """
    Loader hybride:
      1) lit data/<SYMBOL>-<TF>.csv si présent,
      2) sinon fetch via exchange.fetch_ohlcv, puis écrit le CSV en cache.
    """
    def load(symbol: str, timeframe: str, start: str | None, end: str | None) -> pd.DataFrame:
        path = _csv_path(data_dir, symbol, timeframe)
        if path.exists():
            df = _read_csv(path)
            src = "csv"
        else:
            df = [REDACTED](exchange, symbol, timeframe, limit=api_limit)
            _write_csv(path, df)
            src = "exchange"
        if start:
            df = df.loc[pd.Timestamp(start, tz="UTC") :]
        if end:
            df = df.loc[: pd.Timestamp(end, tz="UTC")]
        _log(f"loader -> {symbol} {timeframe} (src={src}) n={len(df)} "
             f"range=[{df.index.min()} .. {df.index.max()}]")
        return df
    return load--- [130/191] ./engine/adapters/bitget_fetch.py ---
# scalper/adapters/bitget_fetch.py
from __future__ import annotations

import asyncio
import inspect
import os
from typing import Any, Optional

BT_DEBUG = int(os.getenv("BT_DEBUG", "0") or "0")

def _log(msg: str) -> None:
    if BT_DEBUG:
        print(f"[bt.debug] {msg}", flush=True)

_TF_TO_SECS = {
    "1m": 60, "3m": 180, "5m": 300, "15m": 900, "30m": 1800,
    "1h": 3600, "4h": 14400, "1d": 86400,
}
_TF_TO_MIX = {  # granularity pour mix (docs Bitget)
    "1m": "1min", "3m": "3min", "5m": "5min", "15m": "15min",
    "30m": "30min", "1h": "1h", "4h": "4h", "1d": "1day",
}
_TF_TO_SPOT = {  # period pour spot (docs Bitget)
    "1m": "1min", "3m": "3min", "5m": "5min", "15m": "15min",
    "30m": "30min", "1h": "1hour", "4h": "4hour", "1d": "1day",
}

def _await_if_needed(val: Any) -> Any:
    if inspect.isawaitable(val):
        try:
            asyncio.get_running_loop()
        except RuntimeError:
            return asyncio.run(val)
        else:
            fut = asyncio.[REDACTED](val, asyncio.get_running_loop())
            return fut.result()
    return val

class BitgetFetchAdapter:
    """
    Adaptateur qui fournit une méthode CCXT-like:
      fetch_ohlcv(symbol, timeframe='5m', since=None, limit=1000)
    au-dessus d'un client Bitget existant (sync ou async).
    """
    def __init__(self, client: Any, *, market_hint: str | None = None):
        self.client = client
        self.market_hint = (market_hint or "").lower() or None
        _log(f"BitgetFetchAdapter attaché sur {type(client).__name__} (market_hint={self.market_hint})")
        if hasattr(client, "fetch_ohlcv") and callable(getattr(client, "fetch_ohlcv")):
            _log("Client expose déjà fetch_ohlcv → adaptation inutile (utilisation directe).")

    @staticmethod
    def _possible_methods(client: Any) -> list[str]:
        names = dir(client)
        base = [
            "fetch_ohlcv",
            "get_candlesticks", "candlesticks", "get_candles", "candles",
            "klines", "get_klines", "kline",
            "mix_get_candles", "mix_candles",
            "spot_get_candles", "spot_candles",
            "market_candles", "public_candles",
        ]
        # + heuristique: tout ce qui contient candle/kline
        extra = [n for n in names if ("candle" in n.lower() or "kline" in n.lower()) and callable(getattr(client, n))]
        out = []
        for n in base + extra:
            if n in names and callable(getattr(client, n)) and n not in out:
                out.append(n)
        _log(f"Méthodes candidates détectées: {out or '(aucune)'}")
        return out

    @staticmethod
    def _sym_variants(sym: str) -> list[str]:
        s = sym.upper()
        out = [s]
        if not s.endswith("_UMCBL"):
            out.append(f"{s}_UMCBL")
        if not s.endswith("_SPBL"):
            out.append(f"{s}_SPBL")
        _log(f"Variantes symbole testées: {out}")
        return out

    @staticmethod
    def _param_variants(timeframe: str, market_hint: Optional[str]) -> list[dict]:
        secs = _TF_TO_SECS.get(timeframe, 300)
        mix = _TF_TO_MIX.get(timeframe, "5min")
        spot = _TF_TO_SPOT.get(timeframe, "5min")
        variants = []
        if market_hint == "mix":
            variants.append({"granularity": mix})
        if market_hint == "spot":
            variants.append({"period": spot})
        variants += [
            {"timeframe": timeframe},
            {"interval": timeframe},
            {"k": secs},
            {"granularity": mix},
            {"period": spot},
        ]
        _log(f"Variantes params testées pour tf={timeframe}: {variants}")
        return variants

    @staticmethod
    def _normalize_rows(raw: Any) -> list[list[float]]:
        import pandas as pd  # local import
        if raw is None:
            raise ValueError("OHLCV vide")
        if isinstance(raw, dict) and "data" in raw:
            raw = raw["data"]
        if isinstance(raw, (list, tuple)) and raw and isinstance(raw[0], (list, tuple)):
            out = []
            for r in raw:
                ts = int(str(r[0]))
                o, h, l, c, v = map(float, (r[1], r[2], r[3], r[4], r[5]))
                out.append([ts, o, h, l, c, v])
            return out
        if "pandas" in str(type(raw)):
            df = raw
            if "timestamp" in df.columns:
                df["timestamp"] = pd.to_datetime(df["timestamp"], utc=True, [REDACTED]=True)
                df = df.set_index("timestamp").sort_index()
            df = df[["open", "high", "low", "close", "volume"]]
            return [[int(ts.value // 10**6), *map(float, row)] for ts, row in df.itertuples()]
        raise ValueError(f"Format OHLCV inattendu: {type(raw)}")

    def fetch_ohlcv(self, symbol: str, timeframe: str = "5m", since: Any | None = None, limit: int = 1000):
        methods = self._possible_methods(self.client)
        if not methods:
            raise AttributeError("Aucune méthode OHLCV trouvée sur le client Bitget")

        last_err: Exception | None = None
        for mname in methods:
            fn = getattr(self.client, mname)
            for sym in self._sym_variants(symbol):
                for par in self._param_variants(timeframe, self.market_hint):
                    kwargs = dict(par)
                    kwargs.setdefault("symbol", sym)
                    kwargs.setdefault("limit", limit)
                    if since is not None:
                        kwargs.setdefault("since", since)
                    try:
                        _log(f"→ Essai {mname}(kwargs={kwargs})")
                        res = _await_if_needed(fn(**kwargs))
                        rows = self._normalize_rows(res)
                        if rows:
                            unit = "ms" if rows and rows[0][0] > 10_000_000_000 else "s"
                            first = rows[0][0]; last = rows[-1][0]
                            _log(f"✓ OK via {mname} {sym} {par} | n={len(rows)} | "
                                 f"t0={first} {unit}, t1={last} {unit}")
                            return rows
                    except TypeError as e:
                        _log(f"TypeError {mname} {sym} {par}: {e}")
                        last_err = e
                    except Exception as e:
                        _log(f"Erreur {mname} {sym} {par}: {e}")
                        last_err = e
        raise last_err or RuntimeError("Impossible d'obtenir l'OHLCV via le client Bitget")

def ensure_bitget_fetch(exchange: Any, *, market_hint: str | None = None) -> Any:
    """Renvoie l'exchange si fetch_ohlcv existe, sinon un wrapper qui l’implémente. Log debug si BT_DEBUG=1."""
    if hasattr(exchange, "fetch_ohlcv") and callable(getattr(exchange, "fetch_ohlcv")):
        _log("exchange.fetch_ohlcv() déjà présent.")
        return exchange
    _log("exchange.fetch_ohlcv() absent → usage BitgetFetchAdapter.")
    return BitgetFetchAdapter(exchange, market_hint=market_hint)--- [131/191] ./engine/adapters/__init__.py ---
--- [132/191] ./engine/logging_utils.py ---
"""Logging helpers for the Scalp bot."""

from __future__ import annotations

import atexit
import csv
import json
import os
import sqlite3
import time
from pathlib import Path
from typing import Any, Dict, List


def get_jsonl_logger(path: str, max_bytes: int = 0, backup_count: int = 0):
    """Return a callable that logs events as JSON lines.

    Parameters
    ----------
    path: str
        Target file path for JSON lines.
    max_bytes: int, optional
        If >0, rotate the file when its size exceeds this value.
    backup_count: int, optional
        Number of rotated files to keep when ``max_bytes`` is set.
    """
    os.makedirs(os.path.dirname(path), exist_ok=True)
    log_file = open(path, "a", encoding="utf-8")

    def _close_file() -> None:
        try:
            log_file.close()
        except Exception:
            pass

    atexit.register(_close_file)

    def _rotate() -> None:
        nonlocal log_file
        log_file.close()
        for i in range(backup_count - 1, 0, -1):
            src = f"{path}.{i}"
            dst = f"{path}.{i + 1}"
            if os.path.exists(src):
                os.replace(src, dst)
        os.replace(path, f"{path}.1")
        log_file = open(path, "a", encoding="utf-8")

    def _log(event: str, payload: Dict[str, Any]) -> None:
        nonlocal log_file
        payload = dict(payload or {})
        payload["event"] = event
        payload["ts"] = int(time.time() * 1000)
        line = json.dumps(payload, ensure_ascii=False)
        if max_bytes and backup_count > 0:
            if log_file.tell() + len(line) + 1 > max_bytes:
                _rotate()
        log_file.write(line + "\n")
        log_file.flush()

    return _log


class TradeLogger:
    """Helper writing trade information to CSV and SQLite files."""

    fields = [
        "pair",
        "tf",
        "dir",
        "entry",
        "sl",
        "tp",
        "score",
        "reasons",
        "pnl",
    ]

    def __init__(self, csv_path: str, sqlite_path: str) -> None:
        os.makedirs(os.path.dirname(csv_path), exist_ok=True)
        self.csv_path = csv_path
        self.sqlite_path = sqlite_path

        # Ensure CSV has header
        if not os.path.exists(csv_path):
            with open(csv_path, "w", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=self.fields)
                writer.writeheader()

        # Setup SQLite store
        self.conn = sqlite3.connect(sqlite_path)
        cur = self.conn.cursor()
        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS trades (
                pair TEXT,
                tf TEXT,
                dir TEXT,
                entry REAL,
                sl REAL,
                tp REAL,
                score REAL,
                reasons TEXT,
                pnl REAL
            )
            """
        )
        self.conn.commit()
        atexit.register(self.conn.close)

    def log(self, data: Dict[str, Any]) -> None:
        row = {k: data.get(k) for k in self.fields}
        with open(self.csv_path, "a", newline="", encoding="utf-8") as f:
            writer = csv.DictWriter(f, fieldnames=self.fields)
            writer.writerow(row)
        cur = self.conn.cursor()
        cur.execute(
            "INSERT INTO trades (pair, tf, dir, entry, sl, tp, score, reasons, pnl) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
            (
                row["pair"],
                row["tf"],
                row["dir"],
                row["entry"],
                row["sl"],
                row["tp"],
                row["score"],
                row["reasons"],
                row["pnl"],
            ),
        )
        self.conn.commit()


BASE_DIR = Path(__file__).resolve().parents[2]


def _append_csv(path: Path, fields: List[str], row: Dict[str, Any]) -> None:
    """Append a row to ``path`` creating the file with ``fields`` if needed."""
    path.parent.mkdir(parents=True, exist_ok=True)
    file_exists = path.exists()
    with path.open("a", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fields)
        if not file_exists:
            writer.writeheader()
        writer.writerow({k: row.get(k) for k in fields})


def log_position(data: Dict[str, Any]) -> None:
    """Log a closed position to ``../positions.csv``."""
    fields = [
        "timestamp",
        "pair",
        "direction",
        "entry",
        "exit",
        "pnl_pct",
        "fee_rate",
        "notes",
    ]
    _append_csv(BASE_DIR / "positions.csv", fields, data)


def log_operation_memo(data: Dict[str, Any]) -> None:
    """Log operation details to ``../operations_memo.csv``."""
    fields = ["timestamp", "pair", "details"]
    _append_csv(BASE_DIR / "operations_memo.csv", fields, data)
--- [133/191] ./engine/live/scheduler.py ---
from __future__ import annotations
import asyncio
from typing import AsyncIterator

class Scheduler:
    def __init__(self, interval_sec: int = 2):
        self.interval = max(1, int(interval_sec))

    async def ticks(self) -> AsyncIterator[int]:
        i = 0
        while True:
            yield i
            i += 1
            await asyncio.sleep(self.interval)
            --- [134/191] ./engine/live/commands.py ---
# engine/live/commands.py
from __future__ import annotations
import asyncio
import logging
import os
from typing import Any, AsyncIterator, Dict, Optional

import requests

log = logging.getLogger("engine.live.commands")


async def [REDACTED](
    token: [REDACTED]
    chat_id: str,
    *,
    poll_secs: int = 5,
    allowed_cmds: Optional[set[str]] = None,
) -> AsyncIterator[Dict[str, Any]]:
    """
    Flux de commandes Telegram via long‑polling.
    Émet des dicts {type: "tg", cmd: "status" | "reload" | "stop" | "help", raw: <update>}
    """
    allowed_cmds = allowed_cmds or {"status", "reload", "stop", "help", "watchlist"}
    url = f"https://api.telegram.org/bot{token}/getUpdates"
    offset = 0

    while True:
        try:
            r = requests.get(url, params={"timeout": poll_secs, "offset": offset + 1}, timeout=poll_secs + 3)
            data = r.json()
            for upd in data.get("result", []):
                offset = upd.get("update_id", offset)
                msg = upd.get("message") or upd.get("edited_message") or {}
                if str(msg.get("chat", {}).get("id")) != str(chat_id):
                    continue
                text = (msg.get("text") or "").strip()
                if not text.startswith("/"):
                    continue
                cmd = text.lstrip("/").split()[0].lower()
                if cmd in allowed_cmds:
                    yield {"type": "tg", "cmd": cmd, "raw": upd}
        except Exception:
            log.debug("poll telegram failed", exc_info=True)
        await asyncio.sleep(poll_secs)


def [REDACTED](cfg: Dict[str, Any] | None = None) -> Optional[AsyncIterator[Dict[str, Any]]]:
    """
    Construit un flux de commandes si TELEGRAM_* sont présents.
    Définir DISABLE_TG_COMMANDS=1 pour désactiver.
    """
    if os.getenv("DISABLE_TG_COMMANDS", "").lower() in {"1", "true", "yes"}:
        return None

    token = [REDACTED]"TELEGRAM_BOT_TOKEN=[REDACTED] or (cfg or {}).get("secrets", {}).get("telegram", {}).get("token")
    chat_id = os.getenv("TELEGRAM_CHAT_ID") or (cfg or {}).get("secrets", {}).get("telegram", {}).get("chat_id")
    if token and chat_id:
        return [REDACTED](token, chat_id)
    return None--- [135/191] ./engine/live/trader.py ---
# engine/live/trader.py
from __future__ import annotations
import math
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional, Sequence

@dataclass
class Position:
    symbol: str
    tf: str
    side: str = "flat"   # "long" | "flat"
    entry: float = 0.0
    size: float = 0.0
    trail: Optional[float] = None

class OrderLogger:
    def __init__(self, path: Path) -> None:
        self.path = path
        self.path.parent.mkdir(parents=True, exist_ok=True)
        if not self.path.exists():
            self._write(["ts","symbol","tf","action","price","size","reason"])

    def _write(self, row: Sequence[str | float | int]) -> None:
        with self.path.open("a", encoding="utf-8") as f:
            f.write(",".join(str(x).replace(",", " ") for x in row) + "\n")

    def log(self, **kw) -> None:
        self._write([kw.get("ts", int(time.time()*1000)), kw["symbol"], kw["tf"],
                     kw["action"], kw.get("price", 0.0), kw.get("size", 0.0),
                     kw.get("reason","")])

class Trader:
    """
    Gère un état de position très simple (one-way, long only) + logs, et passe
    aux ordres réels si paper_trade=False.
    """
    def __init__(self, *, paper_trade: bool, client: Any | None, order_logger: OrderLogger) -> None:
        self.paper = paper_trade
        self.client = client
        self.log = order_logger
        self.state: Dict[tuple[str,str], Position] = {}

    def _pos(self, symbol: str, tf: str) -> Position:
        key = [REDACTED] tf)
        if key not in self.state:
            self.state[key] = Position(symbol=symbol, tf=tf)
        return self.state[key]

    def _order_real_market(self, symbol: str, side: str, size: float) -> None:
        if not self.client:
            return
        try:
            self.client.[REDACTED](symbol, side, size)
        except Exception:
            # on se contente de logguer; l'orchestrateur poursuit
            pass

    def compute_size(self, equity: float, price: float, atr: float, risk_pct: float) -> float:
        # risk par unité ~ ATR (garde-fou min)
        risk_per_unit = max(atr, price * 0.002)
        risk_cash = max(0.0, equity * risk_pct)
        units = risk_cash / risk_per_unit if risk_per_unit > 0 else 0.0
        return max(0.0, round(units, 6))

    def on_signal(self, *, symbol: str, tf: str, price: float, atr: float,
                  params: Dict[str, float], signal_now: int, signal_prev: int,
                  equity: float = 10_000.0, ts: int | None = None) -> None:
        ts = ts or int(time.time() * 1000)
        p = self._pos(symbol, tf)
        trail_mult = float(params.get("trail_atr_mult", 2.0))
        risk_pct = float(params.get("risk_pct_equity", 0.02))
        if p.side == "flat":
            if signal_prev <= 0 and signal_now > 0:
                size = self.compute_size(equity, price, atr, risk_pct)
                p.side = "long"; p.entry = price; p.size = size
                p.trail = price - trail_mult * atr if atr > 0 else None
                self.log.log(ts=ts, symbol=symbol, tf=tf, action="BUY", price=price, size=size, reason="ema_cross_up")
                if not self.paper and size > 0:
                    self._order_real_market(symbol, "buy", size)
        else:
            # update trailing stop
            new_trail = price - trail_mult * atr if atr > 0 else None
            if new_trail is not None and (p.trail is None or new_trail > p.trail):
                p.trail = new_trail
            # sortie par signal inverse ou par cassure du trail
            exit_by_signal = (signal_prev >= 0 and signal_now < 0)
            exit_by_trail = (p.trail is not None and price < p.trail)
            if exit_by_signal or exit_by_trail:
                self.log.log(ts=ts, symbol=symbol, tf=tf, action="SELL", price=price, size=p.size,
                             reason="ema_cross_down" if exit_by_signal else "trail_hit")
                if not self.paper and p.size > 0:
                    self._order_real_market(symbol, "sell", p.size)
                # flat
                self.state[(symbol, tf)] = Position(symbol=symbol, tf=tf)--- [136/191] ./engine/live/state.py ---
# engine/live/state.py
from __future__ import annotations
import time, subprocess
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional

from engine.backtest.loader_csv import load_csv_ohlcv
from engine.config.loader import load_config
from engine.config.strategies import load_strategies

_TF_MIN = {"1m":1,"5m":5,"15m":15,"1h":60,"4h":240,"1d":1440}

def _is_fresh(now_ms: int, last_ms: Optional[int], tf: str, mult: float) -> bool:
    if last_ms is None: return False
    return (now_ms - last_ms) <= (mult * _TF_MIN.get(tf,1) * 60_000)

@dataclass
class Cell:
    lbl: str  # MIS/OLD/DAT/OK
    col: str  # k/r/o/g

class MarketState:
    """Gère l’état (fraîcheur des CSV + présence/validité des stratégies) et actions AUTO."""
    def __init__(self, symbols: List[str], tfs: List[str], data_dir: str, fresh_mult: float = 1.0):
        self.symbols = symbols
        self.tfs = tfs
        self.data_dir = data_dir
        self.fresh_mult = fresh_mult
        self.grid: Dict[Tuple[str,str], Tuple[str,str]] = {}  # (lbl, col)
        self._cooldowns: Dict[str, float] = {}  # clé → epoch

    def _last_ts_ms(self, symbol: str, tf: str) -> Optional[int]:
        try:
            rows = load_csv_ohlcv(self.data_dir, symbol, tf, max_rows=1)
            return int(rows[-1][0]) if rows else None
        except Exception:
            return None

    def _status_cell(self, strategies: Dict[str, Dict], symbol: str, tf: str) -> Tuple[str,str]:
        now = int(time.time()*1000)
        last = self._last_ts_ms(symbol, tf)
        if last is None:
            return ("MIS","k")
        if not _is_fresh(now, last, tf, self.fresh_mult):
            return ("OLD","r")
        s = strategies.get(f"{symbol}:{tf}")
        if s and not s.get("expired"):
            return ("OK ","g")
        return ("DAT","o")

    def refresh(self, strategies: Optional[Dict[str, Dict]] = None) -> None:
        if strategies is None:
            strategies = load_strategies()
        new_grid: Dict[Tuple[str,str], Tuple[str,str]] = {}
        for s in self.symbols:
            for tf in self.tfs:
                new_grid[(s, tf)] = self._status_cell(strategies, s, tf)
        self.grid = new_grid

    def ready_pairs(self) -> List[Tuple[str,str]]:
        """Retourne (symbol, tf) dont la cellule est OK (verte)."""
        return [(s,tf) for (s,tf),(lbl,col) in self.grid.items() if col == "g"]

    # ---------- AUTO ----------
    def _cool(self, key: [REDACTED] cooldown_secs: int) -> bool:
        now = time.time()
        t0 = self._cooldowns.get(key, 0.0)
        if (now - t0) >= cooldown_secs:
            self._cooldowns[key] = now
            return True
        return False

    def _run(self, args: List[str]) -> int:
        p = subprocess.run(args, capture_output=True, text=True)
        if p.stdout: print(p.stdout.strip())
        if p.stderr: print(p.stderr.strip())
        return p.returncode

    def auto_actions(self, limit: int, cooldown: int) -> None:
        """Déclenche automatiquement refresh/backtest/promote selon l’état."""
        # 1) refresh par TF si on voit MIS/OLD
        for tf in self.tfs:
            if any(self.grid.get((s,tf),("",""))[0] in ("MIS","OLD") for s in self.symbols):
                key = [REDACTED]"refresh:{tf}"
                if self._cool(key, cooldown):
                    print(f"[auto] refresh tf={tf}")
                    self._run(["python","-m","jobs.refresh_pairs","--timeframe",tf,"--top","0","--backfill-tfs",tf,"--limit",str(limit)])

        # 2) backtest + promote si on observe au moins un DAT
        if any(lbl == "DAT" for (lbl, _c) in self.grid.values()):
            if self._cool("backtest", cooldown):
                print("[auto] backtest + promote")
                self._run(["python","-m","jobs.backtest","--from-watchlist","--tfs",",".join(self.tfs)])
                self._run(["python","-m","jobs.promote","--backup"])--- [137/191] ./engine/live/orchestrator.py ---
from __future__ import annotations
from dataclasses import dataclass
from typing import List

from engine.config.loader import load_config
from engine.config.watchlist import load_watchlist
from engine.live.scheduler import Scheduler
from engine.live.state import MarketState
from engine.live.health import HealthBoard

@dataclass
class RunConfig:
    symbols: List[str]
    timeframes: List[str]
    refresh_secs: int
    data_dir: str
    limit: int
    exec_enabled: bool = False
    auto: bool = True
    fresh_mult: float = 1.0
    cooldown_secs: int = 60

def [REDACTED]() -> RunConfig:
    cfg = load_config()
    rt = cfg.get("runtime", {}) or {}
    wl = cfg.get("watchlist", {}) or {}
    mt = cfg.get("maintainer", {}) or {}
    auto_cfg = cfg.get("auto") or {}

    wl_doc = load_watchlist()
    syms = [(d.get("symbol") or "").replace("_", "").upper()
            for d in (wl_doc.get("top") or []) if d.get("symbol")] or ["BTCUSDT","ETHUSDT","SOLUSDT"]
    tfs = [str(x) for x in (wl.get("backfill_tfs") or ["1m","5m","15m"])]

    return RunConfig(
        symbols=syms,
        timeframes=tfs,
        refresh_secs=int(mt.get("live_interval_secs", 5)),
        data_dir=str(rt.get("data_dir") or "/notebooks/scalp_data/data"),
        limit=int(wl.get("backfill_limit", 1500)),
        exec_enabled=bool((cfg.get("trading") or {}).get("exec_enabled", False)),
        auto=bool(auto_cfg.get("enabled", True)),
        fresh_mult=float(auto_cfg.get("fresh_mult", mt.get("fresh_mult", 1.0))),
        cooldown_secs=int(auto_cfg.get("cooldown_secs", 60)),
    )

class Orchestrator:
    def __init__(self, cfg: RunConfig, exchange):
        self.cfg = cfg
        self.exchange = exchange
        self.state = MarketState(cfg.symbols, cfg.timeframes, cfg.data_dir, cfg.fresh_mult)
        self.health = HealthBoard(self.state)
        self.sched = Scheduler(interval_sec=max(1, int(cfg.refresh_secs)))

    async def start(self) -> None:
        self.health.banner()
        async for _ in self.sched.ticks():       # boucle infinie
            await self._step()

    async def step_once(self) -> None:
        await self._step()

    async def _step(self) -> None:
        self.state.refresh()
        self.health.render()
        if self.cfg.auto:
            self.state.auto_actions(limit=self.cfg.limit, cooldown=self.cfg.cooldown_secs)
        for (s, tf) in self.state.ready_pairs():
            pass  # placeholder signaux--- [138/191] ./engine/live/health.py ---
# engine/live/health.py
from __future__ import annotations
from typing import Dict, List, Tuple
import re

# codes ANSI simples
_COL = {"k":"\x1b[30m","r":"\x1b[31m","g":"\x1b[32m","o":"\x1b[33m","x":"\x1b[0m"}

def _short(sym: str) -> str:
    s = sym.upper().replace("_","")
    return s[:-4] if s.endswith("USDT") else s

def _center(t: str, w: int) -> str:
    pad = max(0, w-len(t)); return " "*(pad//2) + t + " "*(pad-pad//2)

class HealthBoard:
    """Affichage console: tableau PAIR × TF avec MIS/OLD/DAT/OK."""
    def __init__(self, state):
        self.state = state
        self._legend_once = False

    def banner(self) -> None:
        print("\x1b[2J\x1b[H", end="")
        print("=== ORCHESTRATOR ===")

    def _render_table(self) -> str:
        syms = self.state.symbols
        tfs  = self.state.tfs
        header = ["PAIR", *tfs]
        rows: List[List[str]] = []
        counts = {"k":0,"r":0,"o":0,"g":0}

        for s in syms:
            line = [_short(s)]
            for tf in tfs:
                lbl, col = self.state.grid.get((s, tf), ("MIS","k"))
                counts[col] = counts.get(col,0) + 1
                cell = f"{_COL[col]}{lbl}{_COL['x']}"
                line.append(cell)
            rows.append(line)

        # largeurs visibles (sans ANSI)
        def vislen(x: str) -> int:
            return len(re.sub(r"\x1b\[[0-9;]*m","",x))

        widths = [max(vislen(r[i]) for r in [header]+rows) for i in range(len(header))]

        def fmt(row: List[str]) -> str:
            out=[]
            for i,v in enumerate(row):
                w=widths[i]
                if "\x1b[" in v:
                    plain = re.sub(r"\x1b\[[0-9;]*m","",v)
                    v = v.replace(plain, _center(plain, w))
                else:
                    v = _center(v, w)
                out.append(v)
            return " | ".join(out)

        sep = ["-"*w for w in widths]
        table = "\n".join([fmt(header), fmt(sep), *[fmt(r) for r in rows]])
        legend = (f"{_COL['k']}MIS{_COL['x']}=no data • "
                  f"{_COL['r']}OLD{_COL['x']}=stale • "
                  f"{_COL['o']}DAT{_COL['x']}=data no strat • "
                  f"{_COL['g']}OK{_COL['x']}=ready")
        stats = f"stats • MIS={counts['k']} OLD={counts['r']} DAT={counts['o']} OK={counts['g']}"
        return f"{table}\n{legend}\n{stats}"

    def render(self) -> None:
        print("\x1b[2J\x1b[H", end="")
        print(self._render_table())--- [139/191] ./engine/live/.ipynb_checkpoints/notify-checkpoint.py ---
# engine/live/notify.py
from __future__ import annotations
import asyncio
import logging
import os
from typing import Any

import requests

log = logging.getLogger("engine.live.notify")


class _NullNotifier:
    async def send(self, text: str) -> None:
        log.info("[NOTIFY] %s", text)


class TelegramNotifier:
    def __init__(self, token: [REDACTED] chat_id: str, timeout: float = 5.0) -> None:
        self.token, self.chat_id, self.timeout = token, chat_id, timeout

    async def send(self, text: str) -> None:
        url = f"https://api.telegram.org/bot{self.token}/sendMessage"
        payload = {"chat_id": self.chat_id, "text": text}
        def _post() -> None:
            requests.post(url, json=payload, timeout=self.timeout)
        await asyncio.get_running_loop().run_in_executor(None, _post)


def build_notifier(cfg: dict | None = None) -> Any:
    token = [REDACTED]"TELEGRAM_BOT_TOKEN=[REDACTED] or (cfg or {}).get("secrets", {}).get("telegram", {}).get("token")
    chat_id = os.getenv("TELEGRAM_CHAT_ID") or (cfg or {}).get("secrets", {}).get("telegram", {}).get("chat_id")
    if token and chat_id:
        return TelegramNotifier(token, chat_id)
    return _NullNotifier()--- [140/191] ./engine/live/__init__.py ---
 --- [141/191] ./engine/live/notify.py ---
# engine/live/notify.py
from __future__ import annotations
import asyncio
import logging
import os
from typing import Any

import requests

log = logging.getLogger("engine.live.notify")


class _NullNotifier:
    async def send(self, text: str) -> None:
        log.info("[NOTIFY] %s", text)


class TelegramNotifier:
    def __init__(self, token: [REDACTED] chat_id: str, timeout: float = 5.0) -> None:
        self.token, self.chat_id, self.timeout = token, chat_id, timeout

    async def send(self, text: str) -> None:
        url = f"https://api.telegram.org/bot{self.token}/sendMessage"
        payload = {"chat_id": self.chat_id, "text": text}
        def _post() -> None:
            requests.post(url, json=payload, timeout=self.timeout)
        await asyncio.get_running_loop().run_in_executor(None, _post)


def build_notifier(cfg: dict | None = None) -> Any:
    token = [REDACTED]"TELEGRAM_BOT_TOKEN=[REDACTED] or (cfg or {}).get("secrets", {}).get("telegram", {}).get("token")
    chat_id = os.getenv("TELEGRAM_CHAT_ID") or (cfg or {}).get("secrets", {}).get("telegram", {}).get("chat_id")
    if token and chat_id:
        return TelegramNotifier(token, chat_id)
    return _NullNotifier()--- [142/191] ./engine/trade_utils.py ---
# scalper/trade_utils.py
from __future__ import annotations

from typing import Optional


def [REDACTED](
    equity: float,
    price: float,
    risk_pct: float,
    *,
    symbol: Optional[str] = None,
    min_qty: float = 0.0,
    max_leverage: float = 1.0,
) -> float:
    """
    Sizing simple: position notionnelle = equity * risk_pct * max_leverage
    qty = notionnel / price
    - min_qty : borne basse éventuelle (0 pour ignorer)
    - max_leverage : si tu veux simuler un levier (1 par défaut)
    """
    equity = float(max(0.0, equity))
    price = float(max(1e-12, price))
    risk_pct = float(max(0.0, risk_pct))
    notionnel = equity * risk_pct * max_leverage
    qty = notionnel / price
    if min_qty > 0 and qty < min_qty:
        return 0.0
    return float(qty)--- [143/191] ./engine/exchange/bitget_rest.py ---
# engine/exchange/bitget_rest.py
from __future__ import annotations

import base64
import hashlib
import hmac
import json
import logging
import time
from dataclasses import dataclass
from typing import Any, Dict, Mapping, Optional

import requests

__all__ = ["BitgetFuturesClient", "ApiError"]

log = logging.getLogger("engine.exchange.bitget_rest")


# =============================================================================
# Exceptions
# =============================================================================
class ApiError(RuntimeError):
    """Erreur API Bitget (HTTP != 200, code != '00000', ou payload invalide)."""

    def __init__(self, message: str, *, http_status: int | None = None, body: Any | None = None):
        super().__init__(message)
        self.http_status = http_status
        self.body = body


# =============================================================================
# Helpers
# =============================================================================
def _now_ms() -> int:
    return int(time.time() * 1000)


def _canonical_json(obj: Mapping[str, Any] | None) -> str:
    if not obj:
        return ""
    # JSON compact, trié, ascii pour signature stable
    return json.dumps(obj, separators=(",", ":"), sort_keys=True, ensure_ascii=True)


# =============================================================================
# Client REST Futures (USDT-M)
# =============================================================================
@dataclass
class _Auth:
    access_key: [REDACTED]
    secret_key: [REDACTED]
    passphrase: [REDACTED]
    recv_window: int = 30_000  # ms


class BitgetFuturesClient:
    """
    Client REST léger pour les Futures USDT-M de Bitget.

    Points clés:
    - Public: get_ticker(symbol?), get_klines(symbol, interval, limit, start/end?)
    - Privé : get_account(), get_open_orders(symbol?), cancel_order(), cancel_all()
              [REDACTED](), [REDACTED]()
              [REDACTED](), set_leverage()
    - Safe: gestion d’erreurs centralisée, timeouts, session réutilisable.
    """

    def __init__(
        self,
        *,
        access_key: [REDACTED] = "",
        secret_key: [REDACTED] = "",
        passphrase: [REDACTED] = "",
        base_url: str = "https://api.bitget.com",
        paper_trade: bool = True,
        timeout: float = 10.0,
        session: Optional[requests.Session] = None,
    ) -> None:
        self.base_url = base_url.rstrip("/")
        self.auth = _Auth(access_key, secret_key, passphrase)
        self.paper = paper_trade
        self.timeout = float(timeout)
        self.sess = session or requests.Session()
        self.sess.headers.update({"Accept": "application/json"})
        log.info("BitgetFuturesClient ready (paper=%s base=%s)", self.paper, self.base_url)

    # -------------------------------------------------------------------------
    # Core HTTP
    # -------------------------------------------------------------------------
    def _sign(self, ts_ms: int, method: str, path: str, query: str, body: str) -> str:
        """
        Signature :
          sign = base64( HMAC_SHA256(secret, f"{ts}{method}{path}{query}{body}") )
        - method en MAJUSCULES
        - query inclut '?' si présent, sinon ""
        - body = chaîne JSON canonique (ou vide)
        """
        msg = f"{ts_ms}{method.upper()}{path}{query}{body}"
        return base64.b64encode(hmac.new(self.auth.secret_key.encode(), msg.encode(), hashlib.sha256).digest()).decode()

    def _request(
        self,
        method: str,
        path: str,
        *,
        params: Optional[Mapping[str, Any]] = None,
        body: Optional[Mapping[str, Any]] = None,
        signed: bool = False,
    ) -> Dict[str, Any]:
        url = f"{self.base_url}{path}"
        params = dict(params or {})
        body_json = _canonical_json(body)
        query = ""

        headers = {"Content-Type": "application/json"}
        if not signed:
            # Public
            resp = self.sess.request(
                method=method.upper(),
                url=url,
                params=params or None,
                timeout=self.timeout,
                headers=headers,
            )
        else:
            # Privé: timestamp + recvWindow
            ts = _now_ms()
            if "recvWindow" not in params:
                params["recvWindow"] = self.auth.recv_window
            # Construire la query string stable (requests la reformate si on passe 'params')
            if params:
                q_items = "&".join(f"{k}={params[k]}" for k in sorted(params))
                query = f"?{q_items}"
            signature = self._sign(ts, method, path, query, body_json)

            headers.update(
                {
                    "ACCESS-KEY=[REDACTED] self.auth.access_key,
                    "ACCESS-SIGN": signature,
                    "ACCESS-TIMESTAMP": str(ts),
                    "ACCESS-PASS=[REDACTED] self.auth.passphrase,
                }
            )

            resp = self.sess.request(
                method=method.upper(),
                url=url,
                params=params or None,
                data=body_json if body_json else None,
                timeout=self.timeout,
                headers=headers,
            )

        # Gestion d’erreurs HTTP
        if resp.status_code != 200:
            raise ApiError(f"HTTP {resp.status_code} for {path}", http_status=resp.status_code, body=resp.text)

        # Décodage JSON
        try:
            data = resp.json()
        except Exception as exc:
            raise ApiError(f"Non-JSON response for {path}: {resp.text[:200]}") from exc

        # Protocole Bitget: code == '00000' attendu
        code = str(data.get("code", ""))
        if code and code != "00000":
            raise ApiError(f"Bitget API error code={code} for {path}", body=data)

        return data

    # -------------------------------------------------------------------------
    # PUBLIC
    # -------------------------------------------------------------------------
    def get_ticker(self, symbol: Optional[str] = None) -> Dict[str, Any]:
        """
        Ticker futures. Si `symbol` est fourni, retourne uniquement cette entrée.
        """
        params = {"productType": "USDT-FUTURES"}
        data = self._request("GET", "/api/v2/mix/market/tickers", params=params, signed=False)
        if not symbol:
            return data

        # Harmoniser: extraire l’entrée du symbole demandé
        items = data.get("data") or []
        sym = symbol.replace("_", "").upper()
        hit: Dict[str, Any] | None = None
        for it in items:
            if (it.get("symbol") or "").replace("_", "").upper() == sym:
                hit = it
                break
        return {"data": hit or {}}

    def get_klines(
        self,
        symbol: str,
        interval: str = "1m",
        limit: int = 100,
        start: Optional[int] = None,
        end: Optional[int] = None,
    ) -> Dict[str, Any]:
        """
        OHLCV Futures.
        interval: '1m', '5m', '15m', '1h', ...
        start/end: timestamps ms optionnels.
        """
        params: Dict[str, Any] = {
            "symbol": symbol.replace("_", "").upper(),
            "granularity": interval,
            "productType": "USDT-FUTURES",
            "limit": max(1, min(int(limit), 1000)),
        }
        if start is not None:
            params["startTime"] = int(start)
        if end is not None:
            params["endTime"] = int(end)

        return self._request("GET", "/api/v2/mix/market/candles", params=params, signed=False)

    # -------------------------------------------------------------------------
    # PRIVÉ (Futures One-Way par défaut)
    # -------------------------------------------------------------------------
    def get_account(self) -> Dict[str, Any]:
        """Infos compte futures (marges, balances)."""
        return self._request("GET", "/api/v2/mix/account/accounts", params={"productType": "USDT-FUTURES"}, signed=True)

    def get_open_orders(self, symbol: Optional[str] = None) -> Dict[str, Any]:
        params: Dict[str, Any] = {"productType": "USDT-FUTURES"}
        if symbol:
            params["symbol"] = symbol.replace("_", "").upper()
        return self._request("GET", "/api/v2/mix/order/open-orders", params=params, signed=True)

    def cancel_order(self, symbol: str, order_id: str) -> Dict[str, Any]:
        body = {
            "symbol": symbol.replace("_", "").upper(),
            "productType": "USDT-FUTURES",
            "orderId": order_id,
        }
        return self._request("POST", "/api/v2/mix/order/cancel-order", body=body, signed=True)

    def cancel_all(self, symbol: Optional[str] = None) -> Dict[str, Any]:
        body: Dict[str, Any] = {"productType": "USDT-FUTURES"}
        if symbol:
            body["symbol"] = symbol.replace("_", "").upper()
        return self._request("POST", "/api/v2/mix/order/cancel-batch-orders", body=body, signed=True)

    # -- Mode de position & levier ---------------------------------------------
    def [REDACTED](self, symbol: str, product_type: str = "USDT-FUTURES") -> Dict[str, Any]:
        """Passe le mode de position en One-Way (si nécessaire)."""
        body = {
            "productType": product_type,
            "symbol": symbol.replace("_", "").upper(),
            "holdMode": "one_way",
        }
        return self._request("POST", "/api/v2/mix/account/set-position-mode", body=body, signed=True)

    def set_leverage(
        self,
        symbol: str,
        product_type: str = "USDT-FUTURES",
        margin_coin: str = "USDT",
        leverage: int = 2,
        side: str = "long",
    ) -> Dict[str, Any]:
        """Règle l’effet de levier (par side 'long'/'short' ou global selon l’API)."""
        lev = max(1, int(leverage))
        body = {
            "symbol": symbol.replace("_", "").upper(),
            "productType": product_type,
            "marginCoin": margin_coin,
            "leverage": str(lev),
            "holdSide": side.lower(),  # 'long' ou 'short'
        }
        return self._request("POST", "/api/v2/mix/account/set-leverage", body=body, signed=True)

    # -- Placement d’ordres (One-Way) ------------------------------------------
    def [REDACTED](
        self,
        symbol: str,
        side: str,
        size: float,
        product_type: str = "USDT-FUTURES",
        margin_coin: str = "USDT",
        client_oid: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Place un ordre MARKET en mode one-way.
        side: 'buy' ou 'sell'
        size: quantité de contrat (format arrondi côté serveur)
        """
        body: Dict[str, Any] = {
            "symbol": symbol.replace("_", "").upper(),
            "productType": product_type,
            "marginCoin": margin_coin,
            "size": f"{float(size):.6f}",
            "side": side.lower(),
            "orderType": "market",
        }
        if client_oid:
            body["clientOid"] = client_oid
        return self._request("POST", "/api/v2/mix/order/place-order", body=body, signed=True)

    def [REDACTED](
        self,
        symbol: str,
        side: str,
        size: float,
        price: float,
        product_type: str = "USDT-FUTURES",
        margin_coin: str = "USDT",
        tif: str = "GTC",
        client_oid: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Place un ordre LIMIT en mode one-way.
        tif: GTC/IOC/FOK suivant l’API.
        """
        body: Dict[str, Any] = {
            "symbol": symbol.replace("_", "").upper(),
            "productType": product_type,
            "marginCoin": margin_coin,
            "size": f"{float(size):.6f}",
            "price": f"{float(price):.8f}",
            "side": side.lower(),
            "orderType": "limit",
            "timeInForceValue": tif.upper(),
        }
        if client_oid:
            body["clientOid"] = client_oid
        return self._request("POST", "/api/v2/mix/order/place-order", body=body, signed=True)

    # -------------------------------------------------------------------------
    # Helpers d’accès
    # -------------------------------------------------------------------------
    def last_price(self, symbol: str) -> float:
        """Renvoie un prix last connu en tolérant plusieurs structures/clefs."""
        tick = self.get_ticker(symbol)
        data = tick.get("data")
        if isinstance(data, list) and data:
            data = data[0]
        if not isinstance(data, dict):
            return 0.0
        price_str = (
            data.get("lastPr")
            or data.get("lastPrice")
            or data.get("close")
            or data.get("price")
            or data.get("l")
        )
        try:
            return float(price_str)
        except Exception:
            return 0.0--- [144/191] ./engine/exchange/bitget_ccxt.py ---
from __future__ import annotations
import asyncio
from typing import Any, List, Sequence

class BitgetExchange:
    """Wrapper CCXT async. Installe `ccxt` pour l'utiliser, sinon bot tombera sur REST."""
    def __init__(self, api_key: [REDACTED] secret: [REDACTED] password: str, data_dir: str):
        try:
            import ccxt.async_support as ccxt  # type: ignore
        except Exception as e:  # ccxt non installé
            raise RuntimeError(f"ccxt non disponible: {e}")
        self._ccxt = ccxt.bitget({"apiKey": api_key, "secret": secret, "password": password})
        self.data_dir = data_dir

    async def fetch_ohlcv(self, symbol: str, timeframe: str, limit: int = 150) -> List[List[float]]:
        return await self._ccxt.fetch_ohlcv(symbol, timeframe=timeframe, limit=limit)

    async def close(self) -> None:
        try:
            await self._ccxt.close()
        except Exception:
            pass--- [145/191] ./engine/version.py ---
"""Utilities for managing the Scalp bot version."""

from __future__ import annotations

from pathlib import Path
import re

import subprocess


# Path to the VERSION file within the package
_VERSION_FILE = Path(__file__).resolve().parent / "VERSION"
_VERSION_RE = re.compile(r"^(\d+)\.(\d+)\.(\d+)$")


def get_version() -> str:
    """Return the current version of the bot.

    If the VERSION file does not exist the default version ``0.0.0`` is
    returned.
    """
    if not _VERSION_FILE.exists():
        return "0.0.0"
    return _VERSION_FILE.read_text().strip()


def _parse(version: str) -> tuple[int, int, int]:
    match = _VERSION_RE.match(version)
    if not match:
        raise ValueError(f"Invalid version: {version!r}")
    return tuple(int(x) for x in match.groups())


def bump_version(part: str = "patch") -> str:
    """Bump the version stored in the VERSION file.

    Parameters
    ----------
    part:
        Which component to increment. Accepted values are ``"major"``,
        ``"minor"`` and ``"patch"`` (default).
    """
    major, minor, patch = _parse(get_version())
    if part == "major":
        major += 1
        minor = 0
        patch = 0
    elif part == "minor":
        minor += 1
        patch = 0

    elif part == "patch":
        patch += 1
    else:
        raise ValueError(f"Unknown part: {part}")
    new_version = f"{major}.{minor}.{patch}"
    _VERSION_FILE.write_text(f"{new_version}\n")
    return new_version


def [REDACTED](message: str) -> str:
    """Bump the version according to a commit message.

    ``message`` is evaluated using a tiny subset of the Conventional
    Commits spec. Messages starting with ``feat`` bump the *minor*
    version, messages whose header ends with ``!`` or contain
    ``BREAKING CHANGE`` bump the *major* version. All other messages
    bump the *patch* component.
    """

    header = message.strip().splitlines()[0].lower()
    lower = message.lower()
    type_part = header.split(":")[0]
    if "!" in type_part or "breaking change" in lower:
        part = "major"
    elif type_part.startswith("feat"):
        part = "minor"
    else:
        part = "patch"
    return bump_version(part)


def [REDACTED]() -> str:
    """Read the latest git commit message and bump the version accordingly."""
    try:
        message = subprocess.check_output(
            ["git", "log", "-1", "--pretty=%B"], text=True
        ).strip()
    except Exception:
        message = ""
    return [REDACTED](message)


if __name__ == "__main__":
    print([REDACTED]())
--- [146/191] ./engine/pairs.py ---
"""Utilities to select trading pairs and detect signals."""
from __future__ import annotations
from typing import Any, Dict, List, Optional, Callable
from engine.strategy import Signal

from engine.bot_config import CONFIG
from engine.strategy import ema as default_ema, cross as default_cross
from engine.notifier import notify


def get_trade_pairs(client: Any) -> List[Dict[str, Any]]:
    """Return all trading pairs using the client's ``get_ticker`` method."""
    tick = client.get_ticker()
    data = tick.get("data") if isinstance(tick, dict) else []
    if not data:
        return []
    return data if isinstance(data, list) else [data]


def filter_trade_pairs(
    client: Any,
    *,
    volume_min: float = 5_000_000,
    max_spread_bps: float = 5.0,
    top_n: int = 40,
) -> List[Dict[str, Any]]:
    """Filter pairs by volume and spread."""
    pairs = get_trade_pairs(client)
    eligible: List[Dict[str, Any]] = []

    for info in pairs:
        sym = info.get("symbol")
        if not sym:
            continue
        try:
            vol = float(info.get("volume", 0))
        except (TypeError, ValueError):
            continue
        if vol < volume_min:
            continue
        try:
            bid = float(info.get("bidPrice", 0))
            ask = float(info.get("askPrice", 0))
        except (TypeError, ValueError):
            continue
        if bid <= 0 or ask <= 0:
            continue
        spread_bps = (ask - bid) / ((ask + bid) / 2) * 10_000
        if spread_bps >= max_spread_bps:
            continue
        eligible.append(info)

    eligible.sort(key=[REDACTED] row: float(row.get("volume", 0)), reverse=True)
    return eligible[:top_n]


def select_top_pairs(client: Any, top_n: int = 40, key: [REDACTED] = "volume") -> List[Dict[str, Any]]:
    """Return ``top_n`` pairs sorted by ``key``."""
    pairs = get_trade_pairs(client)

    def volume(row: Dict[str, Any]) -> float:
        try:
            return float(row.get(key, 0))
        except (TypeError, ValueError):
            return 0.0

    pairs.sort(key=[REDACTED] reverse=True)
    return pairs[:top_n]


def _ancienne_impl(
    client: Any,
    pairs: List[Dict[str, Any]],
    *,
    interval: str = "1m",
    ema_fast_n: Optional[int] = None,
    ema_slow_n: Optional[int] = None,
    ema_func=default_ema,
    cross_func=default_cross,
) -> List[Dict[str, Any]]:
    """Original implementation returning dicts."""
    ema_fast_n = ema_fast_n or CONFIG.get("EMA_FAST", 9)
    ema_slow_n = ema_slow_n or CONFIG.get("EMA_SLOW", 21)
    results: List[Dict[str, Any]] = []

    for info in pairs:
        symbol = info.get("symbol")
        if not symbol:
            continue
        k = client.get_kline(symbol, interval=interval)
        closes = k.get("data", {}).get("close", []) if isinstance(k, dict) else []
        if len(closes) < max(ema_fast_n, ema_slow_n) + 2:
            continue
        efull = ema_func(closes, ema_fast_n)
        eslow = ema_func(closes, ema_slow_n)
        signal = cross_func(efull[-1], eslow[-1], efull[-2], eslow[-2])
        if signal == 1:
            price_str = info.get("lastPr") or info.get("lastPrice") or 0.0
            results.append({"symbol": symbol, "signal": "long", "price": float(price_str)})
        elif signal == -1:
            price_str = info.get("lastPr") or info.get("lastPrice") or 0.0
            results.append({"symbol": symbol, "signal": "short", "price": float(price_str)})
    return results


def _to_signal(d: dict) -> Signal:
    side = 1 if d.get("signal") in ("long", "buy", 1, True) else -1
    return Signal(
        symbol=d.get("symbol"),
        side=side,
        entry=float(d.get("price", d.get("entry", 0))),
        sl=float(d.get("sl", 0)),
        tp1=float(d.get("tp1", 0)) or None,
        tp2=float(d.get("tp2", 0)) or None,
        score=d.get("score"),
        quality=d.get("quality"),
        reasons=d.get("reasons", []),
    )


def [REDACTED](
    client: Any,
    pairs: List[Dict[str, Any]],
    *,
    interval: str = "1m",
    ema_fast_n: Optional[int] = None,
    ema_slow_n: Optional[int] = None,
    ema_func=default_ema,
    cross_func=default_cross,
) -> List[Signal]:
    raw = _ancienne_impl(
        client,
        pairs,
        interval=interval,
        ema_fast_n=ema_fast_n,
        ema_slow_n=ema_slow_n,
        ema_func=ema_func,
        cross_func=cross_func,
    )
    return [_to_signal(x) for x in raw]


def send_selected_pairs(
    client: Any,
    top_n: int = 40,
    *,
    select_fn: Callable[[Any, int], List[Dict[str, Any]]] = select_top_pairs,
    notify_fn: Callable[[str, Optional[Dict[str, Any]]], None] = notify,
) -> Dict[str, str]:
    """Fetch top pairs, drop USD/USDT/USDC duplicates and notify their list.

    Returns the payload sent to ``notify_fn``. The mapping contains the
    comma-separated symbols for each color group (``green``, ``orange`` and
    ``red``) or an empty dictionary when no pairs are available.
    """

    def split_symbol(sym: str) -> tuple[str, str]:
        if "_" in sym:
            left, right = sym.split("_", 1)
            # Legacy style: BTC_USDT
            if len(right) <= 4:
                return left, right
            # Bitget futures style: BTCUSDT_UMCBL
            main = left
            if main.endswith("USDT"):
                return main[:-4], "USDT"
            if main.endswith("USDC"):
                return main[:-4], "USDC"
            if main.endswith("USD"):
                return main[:-3], "USD"
            return main, ""
        if sym.endswith("USDT"):
            return sym[:-4], "USDT"
        if sym.endswith("USDC"):
            return sym[:-4], "USDC"
        if sym.endswith("USD"):
            return sym[:-3], "USD"
        return sym, ""

    pairs = select_fn(client, top_n=top_n * 3)
    allowed = {s.split("_")[0].upper() for s in CONFIG.get("ALLOWED_SYMBOLS", [])}
    by_base: Dict[str, Dict[str, Any]] = {}
    for info in pairs:
        sym = info.get("symbol")
        if not sym:
            continue
        norm_sym = sym.split("_")[0].upper()
        if allowed and norm_sym not in allowed:
            continue
        base, quote = split_symbol(sym)
        existing = by_base.get(base)
        priority = {"USDT": 3, "USDC": 2, "USD": 1}
        if existing is None or priority.get(quote, 0) > priority.get(existing["quote"], 0):
            by_base[base] = {"data": info, "quote": quote}

    unique = sorted(
        (v["data"] for v in by_base.values()),
        key=[REDACTED] row: float(row.get("volume", 0)),
        reverse=True,
    )
    symbols: list[str] = []
    for row in unique[:top_n]:
        sym = row.get("symbol")
        if not sym:
            continue
        base, _ = split_symbol(sym)
        symbols.append(base)
    if symbols:
        n = len(symbols)
        third = max(n // 3, 1)
        green = symbols[:third]
        orange = symbols[third : 2 * third]
        red = symbols[2 * third :]
        payload: Dict[str, str] = {}
        if green:
            payload["green"] = ", ".join(green)
        if orange:
            payload["orange"] = ", ".join(orange)
        if red:
            payload["red"] = ", ".join(red)
        notify_fn("pair_list", payload)
        return payload
    return {}


def heat_score(volatility: float, volume: float, news: bool = False) -> float:
    """Return a heat score combining volatility, volume and a news flag."""
    mult = 2.0 if news else 1.0
    return volatility * volume * mult


def [REDACTED](
    pairs: List[Dict[str, Any]], *, top_n: int = 3
) -> List[Dict[str, Any]]:
    """Return ``top_n`` pairs ranked by ``heat_score``."""

    scored: List[Dict[str, Any]] = []
    for info in pairs:
        try:
            vol = float(info.get("volatility", 0))
            volume = float(info.get("volume", 0))
        except (TypeError, ValueError):
            continue
        score = heat_score(vol, volume, bool(info.get("news")))
        row = dict(info)
        row["heat_score"] = score
        scored.append(row)

    scored.sort(key=[REDACTED] r: r["heat_score"], reverse=True)
    return scored[:top_n]


def decorrelate_pairs(
    pairs: List[Dict[str, Any]],
    corr: Dict[str, Dict[str, float]],
    *,
    threshold: float = 0.8,
    top_n: int = 3,
) -> List[Dict[str, Any]]:
    """Return top pairs while avoiding highly correlated symbols.

    ``corr`` is a mapping of pair symbol to correlation with other symbols.  Two
    pairs are considered too correlated when the absolute value of the
    correlation exceeds ``threshold``.
    """

    selected: List[Dict[str, Any]] = []
    for info in [REDACTED](pairs, top_n=len(pairs)):
        sym = info.get("symbol")
        if not sym:
            continue
        if all(abs(corr.get(sym, {}).get(p["symbol"], 0.0)) < threshold for p in selected):
            selected.append(info)
        if len(selected) >= top_n:
            break
    return selected
--- [147/191] ./engine/ws.py ---
"""Minimal websocket manager with heartbeat and auto-resubscribe.

This module provides a light-weight framework to maintain a realtime
connection to an exchange.  The actual network layer is expected to be
supplied by the caller via ``connect`` and ``subscribe`` callbacks.  The
manager handles retrying failed connections and periodically invoking the
``subscribe`` callback as a heartbeat.  This keeps the code fully testable
without opening real network sockets.
"""
from __future__ import annotations

import asyncio
import logging
from typing import Awaitable, Callable, Optional


class WebsocketManager:
    """Maintain a websocket connection with heartbeat and retry."""

    def __init__(
        self,
        connect: Callable[[], Awaitable[None]],
        subscribe: Callable[[], Awaitable[None]],
        *,
        heartbeat_interval: float = 30.0,
        max_retries: int = 3,
    ) -> None:
        self._connect = connect
        self._subscribe = subscribe
        self.heartbeat_interval = heartbeat_interval
        self.max_retries = max_retries
        self._heartbeat_task: Optional[asyncio.Task] = None

    async def run(self) -> None:
        """Open the connection retrying on failure."""
        retries = 0
        while True:
            try:
                await self._connect()
                await self._subscribe()
                self._heartbeat_task = asyncio.create_task(self._heartbeat())
                return
            except Exception as exc:  # pragma: no cover - network errors
                logging.error("websocket connect failed: %s", exc)
                retries += 1
                if retries > self.max_retries:
                    raise
                await asyncio.sleep(1)

    async def _heartbeat(self) -> None:
        """Send periodic heartbeats and resubscribe on failure."""
        while True:
            await asyncio.sleep(self.heartbeat_interval)
            try:
                await self._subscribe()
            except Exception as exc:  # pragma: no cover - network errors
                logging.warning("websocket heartbeat failed: %s", exc)
                await self.run()
                break

    async def stop(self) -> None:
        """Cancel the heartbeat task if it is running."""
        task = self._heartbeat_task
        if task and not task.done():
            task.cancel()
            try:
                await task
            except BaseException:  # pragma: no cover - cancellation
                pass
        self._heartbeat_task = None
--- [148/191] ./engine/app.py ---
from __future__ import annotations
import asyncio
import logging

from engine.bootstrap import [REDACTED], build_exchange

log = logging.getLogger("app")

async def _main_async() -> None:
    # import paresseux pour éviter toute boucle d'import
    from engine.live.orchestrator import Orchestrator, [REDACTED]

    [REDACTED]()
    cfg = [REDACTED]()
    ex = build_exchange()

    orch = Orchestrator(cfg, ex)
    await orch.start()  # boucle infinie (scheduler interne)

def run_app() -> None:
    asyncio.run(_main_async())--- [149/191] ./engine/bitget_client.py ---
import json
import logging
import time
import hmac
import hashlib
import base64
import uuid
from typing import Any, Dict, List, Optional

import requests


# Mapping of deprecated v1 product type identifiers to the new v2 names
[REDACTED] = {
    "UMCBL": "USDT-FUTURES",
    "DMCBL": "USDC-FUTURES",
    "CMCBL": "COIN-FUTURES",
}

# Granularity aliases from v1 to v2 nomenclature
[REDACTED] = {
    "MIN1": "1m",
    "MIN3": "3m",
    "MIN5": "5m",
    "MIN15": "15m",
    "MIN30": "30m",
    "HOUR1": "1H",
    "HOUR4": "4H",
    "HOUR12": "12H",
    "DAY1": "1D",
    "WEEK1": "1W",
}


# Default margin coin for each product type. Some authenticated endpoints
# require ``marginCoin`` in addition to ``productType``; supplying a sensible
# default avoids ``400 Bad Request`` responses when the caller does not provide
# it explicitly.
[REDACTED] = {
    "USDT-FUTURES": "USDT",
    "USDC-FUTURES": "USDC",
}


class BitgetFuturesClient:
    """Lightweight REST client for Bitget LAPI v2 futures endpoints."""

    def __init__(
        self,
        access_key: [REDACTED]
        secret_key: [REDACTED]
        base_url: str,
        *,
        product_type: str = "USDT-FUTURES",
        recv_window: int = 30,
        paper_trade: bool = True,
        requests_module: Any = requests,
        log_event: Optional[Any] = None,
        passphrase: [REDACTED] = None,
    ) -> None:
        self.ak = access_key
        self.sk = secret_key
        self.base = base_url.rstrip("/")
        pt = product_type.upper()
        self.product_type = [REDACTED].get(pt, pt)
        self.recv_window = recv_window
        self.paper_trade = paper_trade
        self.requests = requests_module
        self.log_event = log_event or (lambda *a, **k: None)
        self.passphrase = [REDACTED]
        if not self.ak or not self.sk or self.ak == "A_METTRE" or self.sk == "B_METTRE":
            logging.warning(
                "\u26a0\ufe0f Cl\u00e9s API non d\u00e9finies. Le mode r\u00e9el ne fonctionnera pas.",
            )
        # Cache for contract precision details to avoid repeated network calls
        self._contract_cache: Dict[str, Dict[str, Any]] = {}

    # ------------------------------------------------------------------
    # Helpers
    # ------------------------------------------------------------------
    @staticmethod
    def _ms() -> int:
        return int(time.time() * 1000)

    @staticmethod
    def _urlencode_sorted(params: Dict[str, Any]) -> str:
        if not params:
            return ""
        items = []
        for k in sorted(params.keys()):
            v = "" if params[k] is None else str(params[k])
            items.append(f"{k}={v}")
        return "&".join(items)

    def _sign(self, prehash: str) -> str:
        """Return a base64-encoded HMAC SHA256 signature."""
        digest = hmac.new(self.sk.encode(), prehash.encode(), hashlib.sha256).digest()
        return base64.b64encode(digest).decode()

    def _headers(self, signature: str, timestamp: int) -> Dict[str, str]:
        headers = {
            "ACCESS-KEY=[REDACTED] self.ak,
            "ACCESS-SIGN": signature,
            "ACCESS-TIMESTAMP": str(timestamp),
            "ACCESS-RECV-WINDOW": str(self.recv_window),
            "Content-Type": "application/json",
        }
        if self.passphrase:
            headers["ACCESS-PASS=[REDACTED] = self.passphrase
        return headers

    def _format_symbol(self, symbol: str) -> str:
        """Return ``symbol`` formatted for Bitget API.

        The v2 endpoints expect the trading pair without any product type
        suffix (``BTCUSDT``). Older configurations may provide symbols like
        ``BTC_USDT`` or ``BTCUSDT_UMCBL``; these are normalised by removing the
        separators and any trailing product type string (legacy or v2).
        """

        if not symbol:
            return symbol

        sym = symbol.replace("_", "").upper()
        # Strip product type suffix if present (e.g. BTCUSDTUMCBL)
        if sym.endswith(self.product_type):
            sym = sym[: -len(self.product_type)]
        else:
            for old in [REDACTED].keys():
                if sym.endswith(old):
                    sym = sym[: -len(old)]
                    break
        return sym

    def _product_type(self, pt: Optional[str] = None) -> str:
        """Normalise ``pt`` to a valid v2 product type identifier."""
        key = [REDACTED] or self.product_type or "").upper()
        return [REDACTED].get(key, key)

    # ------------------------------------------------------------------
    # Public endpoints
    # ------------------------------------------------------------------
    def get_contract_detail(self, symbol: Optional[str] = None) -> Dict[str, Any]:
        """Return futures contract information.

        The previous implementation queried ``/contract-detail`` which does not
        exist on Bitget's v2 API and resulted in a 404 error.  The correct
        endpoint is ``/contracts`` with the symbol supplied as a query
        parameter."""

        url = f"{self.base}/api/v2/mix/market/contracts"
        params: Dict[str, Any] = {"productType": self.product_type}
        if symbol:
            params["symbol"] = self._format_symbol(symbol)
        r = self.requests.get(url, params=params, timeout=15)
        if r.status_code == 404:  # pragma: no cover - depends on network
            logging.error("Contract detail introuvable pour %s", symbol)
            return {"success": False, "code": 404, "data": None}
        r.raise_for_status()
        return r.json()

    # ------------------------------------------------------------------
    def [REDACTED](self, symbol: str) -> tuple[int, int]:
        """Return price and volume precision for ``symbol``.

        Results are cached to minimise HTTP requests. If the contract
        information cannot be retrieved, ``(0, 0)`` is returned.
        """
        sym = self._format_symbol(symbol)
        info = self._contract_cache.get(sym)
        if info is None:
            detail = self.get_contract_detail(sym)
            try:
                data = detail.get("data", [])
                if isinstance(data, list) and data:
                    info = data[0]
                else:
                    info = {}
            except Exception:
                info = {}
            self._contract_cache[sym] = info
        price_place = int(info.get("pricePlace") or 0)
        volume_place = int(info.get("volumePlace") or 0)
        return price_place, volume_place

    def get_kline(
        self,
        symbol: str,
        interval: str = "1m",
        start: Optional[int] = None,
        end: Optional[int] = None,
    ) -> Dict[str, Any]:
        # Endpoint expects the trading pair in query parameters rather than
        # encoded in the path. Using ``/candles/{symbol}`` results in a 404
        # response from Bitget. See: https://api.bitget.com/api/v2/mix/market/candles
        url = f"{self.base}/api/v2/mix/market/candles"
        interval_norm = [REDACTED].get(interval.replace("_", "").upper(), interval)
        params: Dict[str, Any] = {
            "symbol": self._format_symbol(symbol),
            "productType": self.product_type,
            "granularity": interval_norm,
        }
        if start is not None:
            params["startTime"] = int(start)
        if end is not None:
            params["endTime"] = int(end)
        r = self.requests.get(url, params=params, timeout=15)
        r.raise_for_status()
        data = r.json()

        rows = data.get("data") if isinstance(data, dict) else None
        if isinstance(rows, list) and rows and isinstance(rows[0], list):
            cols = {"ts": [], "open": [], "high": [], "low": [], "close": [], "volume": [], "quoteVolume": []}
            for row in rows:
                if len(row) < 7:
                    continue
                try:
                    ts, op, hi, lo, cl, vol, qv = row[:7]
                    cols["ts"].append(int(ts))
                    cols["open"].append(float(op))
                    cols["high"].append(float(hi))
                    cols["low"].append(float(lo))
                    cols["close"].append(float(cl))
                    cols["volume"].append(float(vol))
                    cols["quoteVolume"].append(float(qv))
                except (TypeError, ValueError):
                    continue
            data["data"] = cols
        elif isinstance(rows, list):
            data["data"] = {"ts": [], "open": [], "high": [], "low": [], "close": [], "volume": [], "quoteVolume": []}
        return data

    def get_ticker(self, symbol: Optional[str] = None) -> Dict[str, Any]:
        if symbol:
            url = f"{self.base}/api/v2/mix/market/ticker"
            params = {
                "symbol": self._format_symbol(symbol),
                "productType": self.product_type,
            }
        else:
            url = f"{self.base}/api/v2/mix/market/tickers"
            params = {"productType": self.product_type}
        r = self.requests.get(url, params=params, timeout=15)
        r.raise_for_status()
        return r.json()

    # ------------------------------------------------------------------
    # Private endpoints
    # ------------------------------------------------------------------
    def _private_request(
        self,
        method: str,
        path: str,
        *,
        params: Optional[Dict[str, Any]] = None,
        body: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        method = method.upper()
        ts = self._ms()

        if method in ("GET", "DELETE"):
            qs = self._urlencode_sorted(params or {})
            req_path = path + (f"?{qs}" if qs else "")
            sig = self._sign(f"{ts}{method}{req_path}")
            headers = self._headers(sig, ts)
            url = f"{self.base}{req_path}"
            r = self.requests.request(method, url, headers=headers, timeout=20)
        elif method == "POST":
            qs = self._urlencode_sorted(params or {})
            req_path = path + (f"?{qs}" if qs else "")
            body_str = json.dumps(body or {}, separators=(",", ":"), ensure_ascii=False)
            sig = self._sign(f"{ts}{method}{req_path}{body_str}")
            headers = self._headers(sig, ts)
            url = f"{self.base}{req_path}"
            r = self.requests.post(
                url,
                data=body_str.encode("utf-8"),
                headers=headers,
                timeout=20,
            )
        else:
            raise ValueError("M\u00e9thode non support\u00e9e")

        resp_text = getattr(r, "text", "")
        try:
            data = r.json()
        except Exception:
            data = {
                "success": False,
                "error": resp_text,
                "status_code": getattr(r, "status_code", None),
            }

        status = getattr(r, "status_code", 0)
        if status >= 400:
            code = str(data.get("code")) if isinstance(data, dict) else ""
            if code == "22001":
                logging.info("Aucun ordre à annuler (%s %s)", method, path)
            else:
                try:
                    r.raise_for_status()
                except Exception as e:
                    if not resp_text:
                        resp_text = getattr(r, "text", "") or str(e)
                logging.error(
                    "Erreur HTTP/JSON %s %s -> %s %s",
                    method,
                    path,
                    status,
                    resp_text,
                )
                if isinstance(data, dict):
                    data.setdefault("success", False)
                    data.setdefault("status_code", status)
                    data.setdefault("error", resp_text)

        self.log_event(
            "http_private",
            {"method": method, "path": path, "params": params, "body": body, "response": data},
        )
        return data

    # Accounts & positions -------------------------------------------------
    def get_assets(self, margin_coin: Optional[str] = None) -> Dict[str, Any]:
        if self.paper_trade:
            return {
                "success": True,
                "code": 0,
                "data": [
                    {
                        "currency": "USDT",
                        "equity": 100.0,
                    }
                ],
            }

        params = {"productType": self.product_type}
        if margin_coin is None:
            margin_coin = [REDACTED].get(self.product_type)
        if margin_coin:
            params["marginCoin"] = margin_coin
        data = self._private_request(
            "GET", "/api/v2/mix/account/accounts", params=params
        )
        if isinstance(data, dict):
            data.setdefault("success", str(data.get("code")) == "00000")
        try:
            for row in data.get("data", []):
                if "currency" not in row and row.get("marginCoin"):
                    row["currency"] = str(row["marginCoin"]).upper()
                chosen = None
                for key in ("available", "cashBalance", "equity", "usdtEquity"):
                    val = row.get(key)
                    if val is not None:
                        chosen = val
                        break
                if chosen is not None:
                    row["equity"] = chosen
                try:
                    row["equity"] = float(row["equity"])
                except Exception:
                    pass
        except Exception:  # pragma: no cover - best effort
            pass
        return data

    def get_positions(self, product_type: Optional[str] = None) -> Dict[str, Any]:
        if self.paper_trade:
            return {"success": True, "code": 0, "data": []}
        data = self._private_request(
            "GET",
            "/api/v2/mix/position/all-position",
            params={"productType": self._product_type(product_type)},
        )
        try:
            positions = data.get("data", [])
            filtered = []
            for pos in positions:
                vol = pos.get("vol")
                try:
                    if vol is not None and float(vol) > 0:
                        filtered.append(pos)
                except (TypeError, ValueError):
                    continue
            data["data"] = filtered
        except Exception:  # pragma: no cover - best effort
            pass
        return data

    def get_open_orders(self, symbol: Optional[str] = None) -> Dict[str, Any]:
        if self.paper_trade:
            return {"success": True, "code": 0, "data": []}
        params: Dict[str, Any] = {"productType": self.product_type}
        if symbol:
            params["symbol"] = self._format_symbol(symbol)
        return self._private_request("GET", "/api/v2/mix/order/orders-pending", params=params)

    # Account configuration -------------------------------------------------
    def [REDACTED](self, symbol: str, product_type: Optional[str] = None) -> Dict[str, Any]:
        body = {
--- [150/191] ./engine/metrics.py ---
"""Utility metrics for trading calculations."""

from __future__ import annotations


from typing import Iterable

__all__ = ["calc_pnl_pct", "calc_rsi", "calc_atr", "calc_macd", "backtest_position"]


def calc_pnl_pct(
    entry_price: float, exit_price: float, side: int, fee_rate: float = 0.0
) -> float:
    """Return percentage PnL between entry and exit prices minus fees.


    Parameters
    ----------
    entry_price: float
        Trade entry price (>0).
    exit_price: float
        Trade exit price (>0).
    side: int
        +1 for long, -1 for short.
    fee_rate: float, optional
        Trading fee rate per operation (e.g., 0.0006 for 0.06%). The fee is
        applied twice (entry + exit).
    """
    if entry_price <= 0 or exit_price <= 0:
        raise ValueError("Prices must be positive")
    if side not in (1, -1):
        raise ValueError("side must be +1 (long) or -1 (short)")

    pnl = (exit_price - entry_price) / entry_price * 100.0 * side
    fee_pct = fee_rate * 2 * 100.0  # entrée + sortie
    return pnl - fee_pct


def calc_rsi(prices: Iterable[float], period: int = 14) -> float:
    """Compute the Relative Strength Index (RSI) using Wilder's smoothing.


    Parameters
    ----------
    prices:
        Ordered sequence of closing prices.
    period:
        Number of periods to use for the calculation. Must be positive and the
        length of ``prices`` must be at least ``period + 1``.
    """

    prices_list = [float(p) for p in prices]

    if period <= 0:
        raise ValueError("period must be positive")
    if len(prices_list) < period + 1:

        raise ValueError("len(prices) must be >= period + 1")

    gains: list[float] = []
    losses: list[float] = []
    for i in range(1, period + 1):

        diff = prices_list[i] - prices_list[i - 1]

        if diff >= 0:
            gains.append(diff)
            losses.append(0.0)
        else:
            gains.append(0.0)
            losses.append(-diff)

    avg_gain = sum(gains) / period
    avg_loss = sum(losses) / period

    for i in range(period + 1, len(prices_list)):
        diff = prices_list[i] - prices_list[i - 1]

        gain = max(diff, 0.0)
        loss = max(-diff, 0.0)
        avg_gain = (avg_gain * (period - 1) + gain) / period
        avg_loss = (avg_loss * (period - 1) + loss) / period

    if avg_gain == 0 and avg_loss == 0:
        return 50.0
    if avg_loss == 0:
        return 100.0
    if avg_gain == 0:
        return 0.0
    rs = avg_gain / avg_loss
    return 100.0 - (100.0 / (1.0 + rs))


def calc_atr(
    highs: Iterable[float],
    lows: Iterable[float],
    closes: Iterable[float],
    period: int = 14,
) -> float:
    """Compute the Average True Range (ATR) using Wilder's smoothing.


    Parameters
    ----------
    highs, lows, closes:
        Ordered sequences of high, low and close prices. All sequences must
        have the same length and contain at least ``period + 1`` elements.
    period:
        Number of periods to use for the calculation. Must be positive.
    """

    highs_list = [float(h) for h in highs]
    lows_list = [float(low) for low in lows]
    closes_list = [float(c) for c in closes]

    length = len(highs_list)
    if length != len(lows_list) or length != len(closes_list):

        raise ValueError("Input sequences must have the same length")
    if period <= 0:
        raise ValueError("period must be positive")
    if length < period + 1:
        raise ValueError("Input sequences must have at least period + 1 elements")

    trs: list[float] = []
    for i in range(1, len(highs_list)):
        tr = max(
            highs_list[i] - lows_list[i],
            abs(highs_list[i] - closes_list[i - 1]),
            abs(lows_list[i] - closes_list[i - 1]),
        )
        trs.append(tr)

    atr = sum(trs[:period]) / period
    for tr in trs[period:]:
        atr = (atr * (period - 1) + tr) / period
    return atr


def calc_macd(
    prices: Sequence[float],
    fast: int = 12,
    slow: int = 26,
    signal: int = 9,
) -> tuple[float, float, float]:
    """Return MACD, signal line and histogram values.

    The implementation computes exponential moving averages using Wilder's
    smoothing. ``prices`` must contain at least ``slow + signal`` elements.
    """

    prices_list = [float(p) for p in prices]
    if fast <= 0 or slow <= 0 or signal <= 0:
        raise ValueError("periods must be positive")
    min_len = max(fast, slow) + signal
    if len(prices_list) < min_len:
        raise ValueError("len(prices) must be >= slow + signal")

    def _ema_series(series: Sequence[float], window: int) -> list[float]:
        k = 2.0 / (window + 1.0)
        out = [float(series[0])]
        for x in series[1:]:
            out.append(float(x) * k + out[-1] * (1.0 - k))
        return out

    fast_ema = _ema_series(prices_list, fast)
    slow_ema = _ema_series(prices_list, slow)
    macd_series = [f - s for f, s in zip(fast_ema, slow_ema)]
    signal_series = _ema_series(macd_series, signal)
    macd_val = macd_series[-1]
    signal_val = signal_series[-1]
    hist = macd_val - signal_val
    return macd_val, signal_val, hist


def backtest_position(
    prices: list[float], entry_idx: int, exit_idx: int, side: int
) -> bool:
    """Run a basic backtest to verify a position's coherence.

    Parameters
    ----------
    prices: list[float]
        Sequential list of prices to evaluate.
    entry_idx: int
        Index in ``prices`` where the position is opened.
    exit_idx: int
        Index in ``prices`` where the position is closed (must be > ``entry_idx``).
    side: int
        +1 for long, -1 for short.

    Returns
    -------
    bool
        ``True`` if the resulting PnL is non-negative, meaning the position is
        coherent with the direction of price movement. ``False`` otherwise.
    """
    if side not in (1, -1):
        raise ValueError("side must be +1 (long) or -1 (short)")
    if not (0 <= entry_idx < exit_idx < len(prices)):
        raise ValueError(
            "entry_idx and exit_idx must be valid and entry_idx < exit_idx"
        )

    entry_price = float(prices[entry_idx])
    exit_price = float(prices[exit_idx])
    pnl = calc_pnl_pct(entry_price, exit_price, side)
    return pnl >= 0.0
--- [151/191] ./engine/selection/momentum.py ---
"""Utilities to select pairs exhibiting strong momentum."""

from __future__ import annotations

from typing import Any, Dict, List, Sequence

from ..metrics import calc_atr


def ema(series: Sequence[float], window: int) -> List[float]:
    """Simple exponential moving average implementation."""

    if window <= 1 or not series:
        return list(series)
    k = 2.0 / (window + 1.0)
    out: List[float] = [float(series[0])]
    prev = out[0]
    for x in series[1:]:
        prev = float(x) * k + prev * (1.0 - k)
        out.append(prev)
    return out


def cross(last_fast: float, last_slow: float, prev_fast: float, prev_slow: float) -> int:
    """Return 1 if a bullish cross occurred, -1 for bearish, 0 otherwise."""

    if prev_fast <= prev_slow and last_fast > last_slow:
        return 1
    if prev_fast >= prev_slow and last_fast < last_slow:
        return -1
    return 0


def _quantile(values: Sequence[float], q: float) -> float:
    """Return the *q* quantile of *values* (0 <= q <= 1)."""

    if not values:
        return 0.0
    q = min(max(q, 0.0), 1.0)
    vals = sorted(values)
    idx = int((len(vals) - 1) * q)
    return vals[idx]


def select_active_pairs(
    client: Any,
    pairs: Sequence[Dict[str, Any]],
    *,
    interval: str = "Min5",
    ema_fast: int = 20,
    ema_slow: int = 50,
    atr_period: int = 14,
    atr_quantile: float = 0.5,
    top_n: int = 5,
) -> List[Dict[str, Any]]:
    """Return pairs with an EMA crossover and high ATR.

    Only pairs where ``EMA20`` crosses ``EMA50`` on the latest candle are kept.
    Among those candidates, the Average True Range is computed and only pairs
    whose ATR is above the provided quantile are returned.  The resulting
    dictionaries include an ``atr`` key for convenience.
    """

    candidates: List[Dict[str, Any]] = []
    atrs: List[float] = []

    for info in pairs:
        sym = info.get("symbol")
        if not sym:
            continue
        k = client.get_kline(sym, interval=interval)
        kdata = k.get("data") if isinstance(k, dict) else {}
        closes = kdata.get("close", [])
        highs = kdata.get("high", [])
        lows = kdata.get("low", [])
        if len(closes) < max(ema_slow, atr_period) + 2:
            continue
        efast = ema(closes, ema_fast)
        eslow = ema(closes, ema_slow)
        if cross(efast[-1], eslow[-1], efast[-2], eslow[-2]) == 0:
            continue
        atr_val = calc_atr(highs, lows, closes, atr_period)
        row = dict(info)
        row["atr"] = atr_val
        candidates.append(row)
        atrs.append(atr_val)

    if not candidates:
        return []

    threshold = _quantile(atrs, atr_quantile)
    selected = [row for row in candidates if row["atr"] >= threshold]
    selected.sort(key=[REDACTED] r: r["atr"], reverse=True)
    return selected[:top_n]


__all__ = ["select_active_pairs"]

--- [152/191] ./engine/selection/scanner.py ---
"""Utilities for scanning tradable pairs on the exchange."""

from __future__ import annotations

from typing import Any, Dict, List


def scan_pairs(
    client: Any,
    *,
    volume_min: float = 5_000_000,
    max_spread_bps: float = 5.0,
    min_hourly_vol: float = 0.0,
    top_n: int = 40,
) -> List[Dict[str, Any]]:
    """Return pairs satisfying basic liquidity and volatility filters.

    Parameters
    ----------
    client: Any
        Client instance exposing ``get_ticker`` and ``get_kline`` methods.
    volume_min: float, optional
        Minimum 24h volume required to keep a pair.
    max_spread_bps: float, optional
        Maximum allowed bid/ask spread expressed in basis points.
    min_hourly_vol: float, optional
        Minimum volatility over the last hour expressed as ``(high - low) /
        close``.  When set to ``0`` the filter is disabled.
    top_n: int, optional
        Limit the number of returned pairs.
    """

    tick = client.get_ticker()
    data = tick.get("data") if isinstance(tick, dict) else []
    if not isinstance(data, list):
        data = [data]

    eligible: List[Dict[str, Any]] = []

    for row in data:
        sym = row.get("symbol")
        if not sym:
            continue
        try:
            vol = float(row.get("volume", 0))
            bid = float(row.get("bidPrice", 0))
            ask = float(row.get("askPrice", 0))
        except (TypeError, ValueError):
            continue
        if vol < volume_min or bid <= 0 or ask <= 0:
            continue
        spread_bps = (ask - bid) / ((ask + bid) / 2.0) * 10_000
        if spread_bps >= max_spread_bps:
            continue

        if min_hourly_vol > 0:
            k = client.get_kline(sym, interval="Min60")
            kdata = k.get("data") if isinstance(k, dict) else {}
            highs = kdata.get("high", [])
            lows = kdata.get("low", [])
            closes = kdata.get("close", [])
            if not highs or not lows or not closes:
                continue
            try:
                h = float(highs[-1])
                l = float(lows[-1])
                c = float(closes[-1])
            except (TypeError, ValueError):
                continue
            hourly_vol = (h - l) / c if c else 0.0
            if hourly_vol < min_hourly_vol:
                continue

        eligible.append(row)

    eligible.sort(key=[REDACTED] r: float(r.get("volume", 0)), reverse=True)
    return eligible[:top_n]


__all__ = ["scan_pairs"]

--- [153/191] ./engine/selection/__init__.py ---
"""Pair selection helpers for the Scalp bot.

This package exposes two utilities used during the preparation phase of the
trading strategy:

``scan_pairs``
    Performs the first level market scan by filtering pairs based on volume,
    spread and hourly volatility.

``select_active_pairs``
    Refines a list of pairs by keeping only those showing an EMA20/EMA50
    crossover and a sufficiently high ATR.
"""

from .scanner import scan_pairs
from .momentum import select_active_pairs

__all__ = ["scan_pairs", "select_active_pairs"]

--- [154/191] ./engine/selfcheck.py ---
# scalper/selfcheck.py
from __future__ import annotations
import os, sys, importlib, traceback
from pathlib import Path

NOTEBOOKS = Path("/notebooks")
REPO = (NOTEBOOKS / "scalp") if NOTEBOOKS.exists() else Path(__file__).resolve().parents[2]

def _mask(val: str) -> str:
    if not val: return ""
    return (val[:3] + "…" + val[-3:]) if len(val) > 6 else "********"

def _try_import(modname: str):
    try:
        m = importlib.import_module(modname)
        return True, m
    except Exception:
        return False, traceback.format_exc()

def preflight(verbose: bool = False) -> list[str]:
    """
    Retourne la liste des 'issues' trouvées (vide si tout est OK).
    Ne lève pas d'exception. N'écrit que de l'info lisible.
    """
    issues: list[str] = []
    # s'assurer que le repo est bien dans sys.path
    if str(REPO) not in sys.path:
        sys.path.insert(0, str(REPO))

    print("=== SCALPER PREFLIGHT ===")
    print(f"[i] Repo: {REPO}")
    print(f"[i] Python: {sys.version.split()[0]}")

    # backtest API
    ok, mod = _try_import("engine.backtest")
    if not ok:
        print("[✗] Import engine.backtest KO")
        if verbose: print(mod)  # ici 'mod' contient la trace
        issues.append("backtest import")
    else:
        has_single = hasattr(mod, "run_single")
        has_multi  = hasattr(mod, "run_multi")
        print(f"[✓] engine.backtest: run_single={has_single} run_multi={has_multi}")
        if not (has_single and has_multi):
            issues.append("backtest API incomplète")

    # trade_utils
    ok, mod = _try_import("engine.trade_utils")
    if not ok:
        print("[✗] Import engine.trade_utils KO")
        if verbose: print(mod)
        issues.append("trade_utils import")
    else:
        print(f"[✓] engine.trade_utils: [REDACTED]={'[REDACTED]' in dir(mod)}")

    # fees
    ok, mod = _try_import("engine.exchange.fees")
    if not ok:
        print("[✗] Import engine.exchange.fees KO")
        if verbose: print(mod)
        issues.append("fees import")
    else:
        need = {"get_fee", "load_bitget_fees"}
        miss = [n for n in need if not hasattr(mod, n)]
        if miss: issues.append("fees API manquante: " + ",".join(miss))
        print("[✓] engine.exchange.fees OK")

    # notify/commands/backtest_telegram/orchestrator
    for name, required in [
        ("engine.live.notify", ("[REDACTED]",)),
        ("engine.live.commands", ("CommandHandler",)),
        ("engine.live.backtest_telegram", ("[REDACTED]",)),
        ("engine.live.orchestrator", ("run_orchestrator", "Orchestrator")),
    ]:
        ok, mod = _try_import(name)
        if not ok:
            print(f"[✗] Import {name} KO")
            if verbose: print(mod)
            issues.append(f"{name} import")
        else:
            miss = [a for a in required if not hasattr(mod, a)]
            if miss: issues.append(f"{name} API manquante: {','.join(miss)}")
            print(f"[✓] {name} OK")

    # ENV (masqué)
    tg_t = os.getenv("TELEGRAM_BOT_TOKEN=[REDACTED] "")
    tg_c = os.getenv("TELEGRAM_CHAT_ID", "")
    gu   = os.getenv("GIT_USER", "")
    gt   = os.getenv("GIT_TOKEN=[REDACTED] "")
    print("\n-- ENV --")
    print(f"  TELEGRAM_BOT_TOKEN=[REDACTED] {_mask(tg_t)} {'(ABSENT)' if not tg_t else ''}")
    print(f"  TELEGRAM_CHAT_ID  : {_mask(tg_c)} {'(ABSENT)' if not tg_c else ''}")
    print(f"  GIT_USER          : {gu or '(ABSENT)'}")
    print(f"  GIT_TOKEN=[REDACTED]         : {_mask(gt)} {'(ABSENT)' if not gt else ''}")

    # Data
    data_dir = (REPO / "data")
    print("\n-- DATA --")
    if data_dir.exists():
        csvs = list(data_dir.glob("*.csv"))
        print(f"  {len(csvs)} CSV trouvé(s) dans data/ (OK si tu backtestes via CSV)")
    else:
        print("  data/ absent (OK si loader API)")

    return issues

def preflight_or_die(verbose: bool = False) -> None:
    issues = preflight(verbose=verbose)
    if issues:
        print("\n[✗] Préflight a détecté des problèmes :")
        for it in issues: print("   -", it)
        print("\nConseils :")
        print(" - Vérifie les fichiers remplacés (backtest/__init__.py, trade_utils.py, exchange/fees.py).")
        print(" - Évite d'importer optimize/walkforward dans backtest/__init__.py.")
        print(" - Charge /notebooks/.env si TELEGRAM/GIT sont absents (source /notebooks/.env).")
        raise SystemExit(1)
    print("\n[✓] Préflight OK — démarrage du bot.")--- [155/191] ./engine/signals/current.py ---
# scalper/signals/current.py
from __future__ import annotations

# Wrapper pour utiliser la stratégie live actuelle en mode "plugin"
from engine.strategy import generate_signal as _generate_signal

def generate_signal(**kwargs):
    """
    Expose la même signature que engine.strategy.generate_signal.
    Sert d’adaptateur pour la factory.
    """
    return _generate_signal(**kwargs)--- [156/191] ./engine/signals/generator.py ---
from __future__ import annotations

from typing import Any, Dict, List, Optional

import pandas as pd

from data.indicators import compute_all

__all__ = ["generate_signal"]


def _quality_from_score(score: float) -> str:
    if score >= 0.8:
        return "A"
    if score >= 0.5:
        return "B"
    return "C"


def generate_signal(
    df: pd.DataFrame,
    *,
    trend_tf: Optional[pd.DataFrame] = None,
    confirm_tf: Optional[pd.DataFrame] = None,
    atr_mult: float = 1.0,
    trailing: bool = False,
    **_: Any,
) -> Optional[Dict[str, Any]]:
    """Generate a trading signal with confluence scoring.

    Parameters
    ----------
    df: pd.DataFrame
        Primary timeframe OHLCV data.
    trend_tf: pd.DataFrame, optional
        Higher timeframe used for trend filtering.
    confirm_tf: pd.DataFrame, optional
        Lower timeframe used for confirmation.
    atr_mult: float, optional
        Multiplier applied to ATR for stop/target calculation.
    trailing: bool, optional
        When ``True`` include a ``trail`` distance (ATR * ``atr_mult``).

    Returns
    -------
    dict | None
        Dictionary describing the signal or ``None`` if no trade setup exists.
    """

    if df is None or len(df) < 2:
        return None

    df = compute_all(df)
    last = df.iloc[-1]

    conditions: List[bool] = []
    reasons: List[str] = []
    direction: Optional[str] = None

    # --- Basic trend via EMAs ----------------------------------------------
    if last["close"] > last["ema20"] > last["ema50"]:
        direction = "long"
        reasons.append("price_above_ema")
        conditions.append(True)
    elif last["close"] < last["ema20"] < last["ema50"]:
        direction = "short"
        reasons.append("price_below_ema")
        conditions.append(True)
    else:
        conditions.append(False)
        return None

    # --- RSI ---------------------------------------------------------------
    if direction == "long":
        cond = last["rsi"] > 55
        if cond:
            reasons.append("rsi_bullish")
        conditions.append(cond)
    else:
        cond = last["rsi"] < 45
        if cond:
            reasons.append("rsi_bearish")
        conditions.append(cond)

    # --- MACD --------------------------------------------------------------
    if direction == "long":
        cond = last["macd"] > last["macd_signal"]
        if cond:
            reasons.append("macd_bullish")
        conditions.append(cond)
    else:
        cond = last["macd"] < last["macd_signal"]
        if cond:
            reasons.append("macd_bearish")
        conditions.append(cond)

    # --- OBV momentum ------------------------------------------------------
    if len(df) >= 2:
        obv_up = df["obv"].iloc[-1] > df["obv"].iloc[-2]
        if obv_up:
            reasons.append("obv_trending")
        conditions.append(obv_up)

    # --- Trend timeframe filter -------------------------------------------
    if trend_tf is not None and len(trend_tf) >= 2:
        tdf = compute_all(trend_tf)
        ema50 = tdf["ema50"]
        slope = ema50.iloc[-1] - ema50.iloc[-2]
        if direction == "long":
            cond = slope > 0
            if cond:
                reasons.append("trend_up")
            conditions.append(cond)
        else:
            cond = slope < 0
            if cond:
                reasons.append("trend_down")
            conditions.append(cond)

    # --- Confirmation timeframe filter ------------------------------------
    if confirm_tf is not None and len(confirm_tf) > 0:
        cdf = compute_all(confirm_tf)
        rsi = cdf["rsi"].iloc[-1]
        if direction == "long":
            cond = rsi > 50
            if cond:
                reasons.append("confirm_rsi_bullish")
            conditions.append(cond)
        else:
            cond = rsi < 50
            if cond:
                reasons.append("confirm_rsi_bearish")
            conditions.append(cond)

    score = (
        sum(1 for c in conditions if c) / len(conditions) if conditions else 0.0
    )
    quality = _quality_from_score(score)

    atr = last.get("atr")
    if pd.isna(atr) or atr == 0:
        return None

    entry = float(last["close"])
    if direction == "long":
        sl = entry - atr * atr_mult
        tp = entry + atr * atr_mult * 2
    else:
        sl = entry + atr * atr_mult
        tp = entry - atr * atr_mult * 2

    result: Dict[str, Any] = {
        "direction": direction,
        "entry": entry,
        "sl": sl,
        "tp": tp,
        "score": round(score, 3),
        "reasons": reasons,
        "quality": quality,
    }

    if trailing:
        result["trail"] = atr * atr_mult

    return result
--- [157/191] ./engine/signals/factory.py ---
# scalper/signals/factory.py
from __future__ import annotations
from typing import Callable, Dict, Any
import importlib
import os
import json

try:
    import yaml  # type: ignore
except Exception:
    yaml = None  # type: ignore

SignalFn = Callable[..., Any]

# IMPORTANT : on pointe par défaut sur TA stratégie actuelle dans scalper/strategy.py
_REGISTRY: Dict[str, str] = {
    "current": "engine.strategy:generate_signal",
    # Tu pourras ajouter d'autres stratégies ici, par ex :
    # "ema_cross": "engine.strategies.ema_cross:generate_signal",
}

def _load_callable(path: str) -> SignalFn:
    if ":" not in path:
        raise ValueError(f"Chemin callable invalide: {path}")
    module_name, attr = path.split(":", 1)
    mod = importlib.import_module(module_name)
    fn = getattr(mod, attr, None)
    if not callable(fn):
        raise ValueError(f"{attr} n'est pas callable dans {module_name}")
    return fn  # type: ignore

def load_signal(name: str) -> SignalFn:
    key = [REDACTED] or "").strip().lower()
    if key not in _REGISTRY:
        raise KeyError(f"Stratégie inconnue: '{name}'. Registre: {list(_REGISTRY)}")
    return _load_callable(_REGISTRY[key])

def _read_yaml(path: str) -> dict:
    if yaml is None:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f) or {}

def load_strategies_cfg(path: str | None) -> dict:
    """
    Charge le mapping (symbole, timeframe) -> nom de stratégie.
    Si le fichier n'existe pas, retourne une config par défaut fonctionnelle.
    """
    default_cfg = {"default": "current", "by_timeframe": {}, "by_symbol": {}}
    if not path:
        return default_cfg
    if not os.path.isfile(path):
        # Pas de fichier ? On continue avec les valeurs par défaut.
        return default_cfg
    cfg = _read_yaml(path)
    cfg.setdefault("default", "current")
    cfg.setdefault("by_timeframe", {})
    cfg.setdefault("by_symbol", {})
    return cfg

def [REDACTED](symbol: str, timeframe: str, cfg: dict) -> str:
    symbol = (symbol or "").upper()
    timeframe = (timeframe or "").lower()
    return (
        cfg.get("by_symbol", {}).get(symbol, {}).get(timeframe)
        or cfg.get("by_timeframe", {}).get(timeframe)
        or cfg.get("default", "current")
    )

def resolve_signal_fn(symbol: str, timeframe: str, cfg: dict) -> SignalFn:
    return load_signal([REDACTED](symbol, timeframe, cfg))--- [158/191] ./engine/signals/__init__.py ---
__all__ = ["factory"]--- [159/191] ./engine/services/utils.py ---
# scalper/services/utils.py
from __future__ import annotations
import asyncio
from typing import Callable, Any


class NullNotifier:
    async def send(self, _msg: str) -> None:
        return


async def heartbeat_task(running_getter: Callable[[], bool], notifier: Any, period: float = 30.0) -> None:
    if notifier is None:
        notifier = NullNotifier()
    try:
        while running_getter():
            await notifier.send("heartbeat alive")
            await asyncio.sleep(period)
    except asyncio.CancelledError:
        pass


async def log_stats_task(
    notifier: Any,
    ticks_getter: Callable[[], int],
    symbols_getter: Callable[[], list[str]],
    period: float = 30.0,
) -> None:
    if notifier is None:
        notifier = NullNotifier()
    last = 0
    try:
        while True:
            total = int(ticks_getter() or 0)
            delta = total - last
            last = total
            syms = symbols_getter() or []
            msg = f"[stats] ticks_total={total} (+{delta} /30s) | pairs=" + ",".join(syms)
            print(msg)
            await notifier.send(msg)
            await asyncio.sleep(period)
    except asyncio.CancelledError:
        pass--- [160/191] ./engine/services/order_service.py ---
from __future__ import annotations
from dataclasses import dataclass
from typing import Any, Dict, Optional, Protocol
from engine.trade_utils import [REDACTED]


@dataclass
class OrderCaps:
    min_trade_usdt: float = 5.0
    leverage: float = 1.0


@dataclass
class OrderRequest:
    symbol: str
    side: str
    price: float
    sl: float
    tp: Optional[float]
    risk_pct: float


@dataclass
class OrderResult:
    accepted: bool
    reason: str = ""
    payload: Dict[str, Any] = None
    order_id: Optional[str] = None
    status: Optional[str] = None
    avg_price: Optional[float] = None
    filled_qty: Optional[float] = None


class Exchange(Protocol):
    def get_assets(self) -> Dict[str, Any]: ...
    def get_ticker(self, symbol: Optional[str] = None) -> Dict[str, Any]: ...
    def place_order(
        self,
        symbol: str,
        side: str,
        quantity: float,
        order_type: str,
        price: Optional[float] = None,
        stop_loss: Optional[float] = None,
        take_profit: Optional[float] = None,
    ) -> Dict[str, Any]: ...


class OrderService:
    def __init__(self, exchange: Exchange, caps: OrderCaps = OrderCaps()):
        self.exchange = exchange
        self.caps = caps

    @staticmethod
    def _abs(x: float) -> float:
        return -x if x < 0 else x

    def _calc_qty(self, equity_usdt: float, price: float, sl: float, risk_pct: float) -> float:
        dist = self._abs(price - sl)
        if dist <= 0:
            return 0.0
        risk_usdt = max(0.0, equity_usdt * risk_pct)
        return 0.0 if price <= 0 else (risk_usdt / dist)

    def prepare_and_place(self, equity_usdt: float, req: OrderRequest) -> OrderResult:
        qty = self._calc_qty(equity_usdt, req.price, req.sl, req.risk_pct)
        if qty <= 0:
            return OrderResult(False, "invalid_size")
        notional = qty * req.price
        if notional < self.caps.min_trade_usdt:
            return OrderResult(False, "under_min_notional")
        assets = self.exchange.get_assets()
        available = [REDACTED](assets)
        required_margin = notional / max(1.0, self.caps.leverage)
        if available < required_margin:
            return OrderResult(False, "insufficient_margin")
        side = "BUY" if req.side == "long" else "SELL"
        out = self.exchange.place_order(
            symbol=req.symbol, side=side, quantity=qty,
            order_type="limit", price=req.price,
            stop_loss=req.sl, take_profit=req.tp
        )
        # extraire infos utiles
        oid = None; status = None; avg = None; filled = None
        try:
            data = out.get("data") if isinstance(out, dict) else out
            if isinstance(data, dict):
                oid = str(data.get("orderId") or data.get("ordId") or data.get("id") or "")
                status = (data.get("status") or data.get("state") or "new").lower()
                avg = float(data.get("avgPrice", data.get("avgPx", 0)) or 0)
                filled = float(data.get("filledQty", data.get("fillSz", 0)) or 0)
        except Exception:
            pass
        return OrderResult(True, "", out, oid, status, avg, filled)
--- [161/191] ./engine/services/data_cache.py ---
# scalper/services/data_cache.py
from __future__ import annotations

import asyncio
import csv
import os
import time
from typing import Iterable, List, Optional, Tuple, Dict

# ---------------------------------------------------------------------
# Réglages via env (valeurs sûres par défaut)
# ---------------------------------------------------------------------
DATA_DIR = os.getenv("DATA_DIR", "/notebooks/data")           # dossier PERSISTANT (hors-git)
CSV_MAX_AGE = int(os.getenv("CSV_MAX_AGE_SECONDS", "0"))      # 0 = auto (en fonction du TF)
CSV_MIN_ROWS = int(os.getenv("CSV_MIN_ROWS", "200"))          # minimum de lignes attendues
STALE_FACTOR = float(os.getenv("CSV_STALE_FACTOR", "6"))      # âge max = STALE_FACTOR * tf_sec
PREFETCH_CONC = int(os.getenv("CSV_PREFETCH_CONC", "4"))      # concurrence préchauffage

os.makedirs(DATA_DIR, exist_ok=True)


# ---------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------
def [REDACTED](tf: str) -> int:
    tf = tf.strip().lower()
    unit = tf[-1]
    try:
        n = int(tf[:-1])
    except Exception as e:
        raise ValueError(f"timeframe invalide: {tf}") from e
    if unit == "m":
        return n * 60
    if unit == "h":
        return n * 3600
    if unit == "d":
        return n * 86400
    raise ValueError(f"timeframe invalide: {tf}")


def csv_path(symbol: str, timeframe: str) -> str:
    return os.path.join(DATA_DIR, f"{symbol}-{timeframe}.csv")


def read_csv_ohlcv(path: str) -> List[Tuple[int, float, float, float, float, float]]:
    rows: List[Tuple[int, float, float, float, float, float]] = []
    if not os.path.exists(path):
        return rows
    with open(path, "r", newline="") as f:
        r = csv.reader(f)
        header = next(r, None)  # accepte avec ou sans header
        for line in r:
            if not line:
                continue
            ts, o, h, l, c, v = line[:6]
            rows.append((int(ts), float(o), float(h), float(l), float(c), float(v)))
    return rows


def write_csv_ohlcv(path: str, data: Iterable[Tuple[int, float, float, float, float, float]]) -> None:
    first = not os.path.exists(path)
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "a", newline="") as f:
        w = csv.writer(f)
        if first:
            w.writerow(["timestamp", "open", "high", "low", "close", "volume"])
        for row in data:
            w.writerow(row)


def last_ts(rows: List[Tuple[int, float, float, float, float, float]]) -> Optional[int]:
    return rows[-1][0] if rows else None


# ---------------------------------------------------------------------
# Fetch CCXT paginé
# ---------------------------------------------------------------------
async def [REDACTED](
    exchange,
    symbol: str,
    timeframe: str,
    since_ms: Optional[int],
    limit: int = 1000,
) -> List[Tuple[int, float, float, float, float, float]]:
    """
    Récupère OHLCV par pages (limit 1000) depuis since_ms jusqu'à ~now.
    Retourne une liste triée/dédupliquée.
    """
    out: List[Tuple[int, float, float, float, float, float]] = []
    tf_ms = [REDACTED](timeframe) * 1000
    now_ms = exchange.milliseconds() if hasattr(exchange, "milliseconds") else int(time.time() * 1000)

    cursor = since_ms or (now_ms - 200 * tf_ms)
    while True:
        batch = await exchange.fetch_ohlcv(symbol, timeframe=timeframe, since=cursor, limit=limit)
        if not batch:
            break
        for ts, o, h, l, c, v in batch:
            out.append((int(ts), float(o), float(h), float(l), float(c), float(v)))
        next_cursor = batch[-1][0] + tf_ms
        if next_cursor <= cursor:
            break
        cursor = next_cursor
        if cursor >= now_ms + (2 * tf_ms):
            break
        await asyncio.sleep(getattr(exchange, "rateLimit", 200) / 1000)

    out.sort(key=[REDACTED] x: x[0])
    dedup: List[Tuple[int, float, float, float, float, float]] = []
    seen = set()
    for row in out:
        if row[0] in seen:
            continue
        seen.add(row[0])
        dedup.append(row)
    return dedup


# ---------------------------------------------------------------------
# Cache manager
# ---------------------------------------------------------------------
async def [REDACTED](
    exchange,
    symbol: str,
    timeframe: str,
    min_rows: int = CSV_MIN_ROWS,
) -> str:
    """
    Garantit qu'un CSV OHLCV récent existe pour (symbol, timeframe).
    Crée/append si nécessaire. Retourne le chemin.
    """
    path = csv_path(symbol, timeframe)
    rows = read_csv_ohlcv(path)
    tf_sec = [REDACTED](timeframe)
    tf_ms = tf_sec * 1000
    now_ms = int(time.time() * 1000)

    # âge max
    max_age = CSV_MAX_AGE if CSV_MAX_AGE > 0 else int(tf_sec * STALE_FACTOR)

    need_full = False
    need_append = False

    if not rows:
        need_full = True
    else:
        last = last_ts(rows) or 0
        age_sec = max(0, (now_ms - last) // 1000)
        if age_sec > max_age or len(rows) < min_rows:
            need_append = True

    if need_full:
        since = now_ms - (tf_ms * 2000)  # ~2000 bougies
        fresh = await [REDACTED](exchange, symbol, timeframe, since_ms=since)
        if len(fresh) < min_rows:
            since = now_ms - (tf_ms * 5000)
            fresh = await [REDACTED](exchange, symbol, timeframe, since_ms=since)
        if os.path.exists(path):
            os.remove(path)
        write_csv_ohlcv(path, fresh)
        return path

    if need_append:
        since = (last_ts(rows) or now_ms - (tf_ms * 2000)) + tf_ms
        fresh = await [REDACTED](exchange, symbol, timeframe, since_ms=since)
        if fresh:
            write_csv_ohlcv(path, fresh)

    return path


async def prewarm_csv_cache(exchange, symbols: Iterable[str], timeframe: str) -> Dict[str, str]:
    """
    Prépare le cache pour plusieurs symboles (concurrence limitée).
    Retourne {symbol: path}.
    """
    sem = asyncio.Semaphore(PREFETCH_CONC)
    result: Dict[str, str] = {}

    async def _one(sym: str):
        async with sem:
            p = await [REDACTED](exchange, sym, timeframe)
            result[sym] = p

    await asyncio.gather(*[_one(s) for s in symbols])
    return result--- [162/191] ./engine/services/__init__.py ---
--- [163/191] ./engine/__init__.py ---
# Rend le dossier 'scalper' importable comme package.
__all__ = ["live", "signals", "core"]--- [164/191] ./engine/pairs/selector.py ---
# engine/pairs/selector.py
from __future__ import annotations
import math, statistics as stats, time
from dataclasses import dataclass
from typing import Any, Iterable, List, Dict, Tuple

@dataclass
class PairMetrics:
    symbol: str
    vol_usd_24h: float     # volume $ 24h (proxy)
    atr_pct_24h: float     # volatilité (ATR% sur close)
    score: float           # score combiné

def _ohlcv_to_atr_pct(rows: List[List[float]]) -> float:
    """rows: [ts,o,h,l,c,v]. Renvoie ATR% moyen ~ 24h (proxy simple)."""
    if not rows:
        return 0.0
    atr_vals: List[float] = []
    for i in range(1, len(rows)):
        o,h,l,c,_ = rows[i][1], rows[i][2], rows[i][3], rows[i][4], rows[i][5] if len(rows[i])>5 else 0.0
        pc = rows[i-1][4]
        tr = max(h-l, abs(h-pc), abs(l-pc))
        if c:
            atr_vals.append(tr / c)
    if not atr_vals:
        return 0.0
    return float(sum(atr_vals)/len(atr_vals))

def _norm(vals: List[float]) -> List[float]:
    if not vals: return []
    lo, hi = min(vals), max(vals)
    if hi <= 0 or hi == lo:
        return [0.0 for _ in vals]
    return [(v - lo)/(hi - lo) for v in vals]

def select_top_pairs(
    exchange: Any,
    *,
    universe: Iterable[str] | None = None,
    timeframe: str = "5m",
    lookback_candles: int = 300,   # ~ 24h en 5m
    top_n: int = 10,
    vol_weight: float = 0.6,
    volat_weight: float = 0.4,
) -> List[PairMetrics]:
    """
    Récupère OHLCV pour chaque symbole du 'universe' (sinon via tickers),
    calcule volume USD et ATR% 24h, puis score = 0.6*norm(volume) + 0.4*norm(volatilité).
    Retourne le Top N.
    """
    # 1) Construire l’univers
    symbols: List[str]
    if universe:
        symbols = list(dict.fromkeys([s.replace("_","").upper() for s in universe]))
    else:
        # essaie via exchange.get_ticker() (liste complète)
        try:
            data = exchange.get_ticker().get("data") or []
            symbols = [str(d.get("symbol","")).replace("_","").upper() for d in data if d.get("symbol")]
        except Exception:
            symbols = ["BTCUSDT","ETHUSDT","SOLUSDT","BNBUSDT","XRPUSDT","ADAUSDT","DOGEUSDT","LTCUSDT","LINKUSDT","MATICUSDT"]

    # 2) Collecte OHLCV + proxies volume
    metrics: List[Tuple[str, float, float]] = []  # (symbol, vol_usd, atr_pct)
    for sym in symbols:
        try:
            ohlcv = exchange.get_klines(sym, interval=timeframe, limit=lookback_candles).get("data") or []
            if len(ohlcv) < 50:
                continue
            atr_pct = _ohlcv_to_atr_pct(ohlcv)
            # proxy volume $ : somme(close*volume)
            vol_usd = 0.0
            for r in ohlcv[-288:]:  # ~ dernier jour
                close = float(r[4]); vol = float(r[5]) if len(r)>5 else 0.0
                vol_usd += close * vol
            metrics.append((sym, vol_usd, atr_pct))
        except Exception:
            continue

    if not metrics:
        return []

    vols = [m[1] for m in metrics]
    vols_norm = _norm(vols)
    atrs = [m[2] for m in metrics]
    atrs_norm = _norm(atrs)

    scored: List[PairMetrics] = []
    for (sym, vol_usd, atr_pct), nv, na in zip(metrics, vols_norm, atrs_norm):
        s = vol_weight*nv + volat_weight*na
        scored.append(PairMetrics(symbol=sym, vol_usd_24h=vol_usd, atr_pct_24h=atr_pct, score=s))

    scored.sort(key=[REDACTED] x: x.score, reverse=True)
    return scored[:top_n]--- [165/191] ./engine/pairs/__init__.py ---
# engine/pairs/__init__.py
from .selector import PairMetrics, select_top_pairs  # re-export
__all__ = ["PairMetrics", "select_top_pairs"]--- [166/191] ./engine/backtest/runner.py ---
# engine/backtest/runner.py
from __future__ import annotations

import json
import time
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, Iterable, List

from engine.config.loader import load_config
from engine.config.watchlist import load_watchlist
from engine.signals.factory import load_strategies_cfg          # FIX d'import
from engine.backtest.loader_csv import load_csv_ohlcv           # lecteur OHLCV CSV

@dataclass
class DraftStrategy:
    name: str = "ema_cross_atr"
    ema_fast: int = 9
    ema_slow: int = 21
    atr_period: int = 14
    trail_atr_mult: float = 2.0
    risk_pct_equity: float = 0.5
    created_at: int = 0        # ms epoch
    ttl_bars: int = 240        # durée de vie par défaut (barres)
    expired: bool = False

def _now_ms() -> int:
    return int(time.time() * 1000)

def [REDACTED](top: int | None = None) -> List[str]:
    wl = load_watchlist()
    items = wl.get("top") or []
    syms = [(d.get("symbol") or "").replace("_", "").upper() for d in items if d.get("symbol")]
    return syms[:top] if top and top > 0 else syms

def _load_universe(from_watchlist: bool, top: int | None) -> List[str]:
    if from_watchlist:
        syms = [REDACTED](top)
        if syms:
            return syms
    return ["BTCUSDT", "ETHUSDT", "SOLUSDT"]

def _baseline_params(tf: str) -> DraftStrategy:
    if tf == "1m":
        return DraftStrategy(ema_fast=9,  ema_slow=21, atr_period=14, trail_atr_mult=2.2, risk_pct_equity=0.4)
    if tf == "5m":
        return DraftStrategy(ema_fast=12, ema_slow=26, atr_period=14, trail_atr_mult=2.0, risk_pct_equity=0.5)
    if tf == "15m":
        return DraftStrategy(ema_fast=20, ema_slow=50, atr_period=14, trail_atr_mult=1.8, risk_pct_equity=0.6)
    if tf == "1h":
        return DraftStrategy(ema_fast=34, ema_slow=89, atr_period=14, trail_atr_mult=1.6, risk_pct_equity=0.7)
    return DraftStrategy()

def _has_enough_data(data_dir: str, symbol: str, tf: str, min_rows: int = 200) -> bool:
    try:
        rows = load_csv_ohlcv(data_dir, symbol, tf, max_rows=min_rows)
        return len(rows) >= min_rows
    except Exception:
        return False

def run_backtests(*, from_watchlist: bool, tfs: Iterable[str]) -> Path:
    """
    Produit un draft minimal 'strategies.yml.next' sous reports/.
    Génère des stratégies baseline pour chaque (symbol, tf) avec assez d'historique.
    """
    cfg = load_config()
    rt = cfg.get("runtime", {})
    data_dir = str(rt.get("data_dir") or "/notebooks/scalp_data/data")
    reports_dir = str(rt.get("reports_dir") or "/notebooks/scalp_data/reports")
    Path(reports_dir).mkdir(parents=True, exist_ok=True)

    top = int(cfg.get("watchlist", {}).get("top", 10))
    symbols = _load_universe(from_watchlist=from_watchlist, top=top)
    tfs_list = [str(tf) for tf in tfs]
    created = _now_ms()

    draft: Dict[str, Dict] = {"strategies": {}}

    # overrides utilisateur (facultatifs)
    try:
        user_overrides = load_strategies_cfg() or {}
    except Exception:
        user_overrides = {}

    defaults = user_overrides.get("defaults") or {}

    for s in symbols:
        for tf in tfs_list:
            if not _has_enough_data(data_dir, s, tf, min_rows=200):
                continue
            params = _baseline_params(tf)
            params.created_at = created
            key = [REDACTED]"{s}:{tf}"
            specific = user_overrides.get(key) or {}
            draft["strategies"][key] = {**asdict(params), **defaults, **specific}

    out_path = Path(reports_dir) / "strategies.yml.next"
    out_path.write_text(json.dumps(draft, indent=2), encoding="utf-8")
    return out_path--- [167/191] ./engine/backtest/walkforward.py ---
from __future__ import annotations

from itertools import product
from statistics import mean, stdev
from typing import Dict, Iterable, Optional

from ..strategy import max_drawdown


def _sharpe(returns: Iterable[float]) -> float:
    vals = list(returns)
    if not vals:
        return 0.0
    mu = mean(vals)
    if len(vals) > 1:
        sd = stdev(vals)
    else:
        sd = 0.0
    return mu / sd if sd > 0 else 0.0


def _stability(equity: Iterable[float]) -> float:
    curve = list(equity)
    n = len(curve)
    if n < 2:
        return 0.0
    x = list(range(n))
    x_mean = sum(x) / n
    y_mean = sum(curve) / n
    ss_tot = sum((y - y_mean) ** 2 for y in curve)
    denom = sum((xi - x_mean) ** 2 for xi in x)
    if denom == 0 or ss_tot == 0:
        return 0.0
    b = sum((xi - x_mean) * (yi - y_mean) for xi, yi in zip(x, curve)) / denom
    a = y_mean - b * x_mean
    ss_res = sum((yi - (a + b * xi)) ** 2 for xi, yi in zip(x, curve))
    return 1 - ss_res / ss_tot


def walk_forward(
    df,
    splits: int = 5,
    train_ratio: float = 0.7,
    params: Optional[Dict[str, Iterable]] = None,
) -> Dict[str, float]:
    """Perform walk-forward optimisation and evaluation.

    Parameters
    ----------
    df:
        DataFrame containing per-period percentage returns. The first column is
        used when a dedicated ``"returns"`` column is not found.
    splits:
        Number of walk-forward test windows.
    train_ratio:
        Proportion of the data used for training in the initial window.
    params:
        Optional parameter grid. If provided, columns in ``df`` matching each
        parameter combination are evaluated and the best Sharpe ratio on the
        training window is selected. When ``None``, the first column is used.
    """

    if df.empty:
        return {"sharpe": 0.0, "mdd": 0.0, "pnl": 0.0, "stability": 0.0}

    returns_col = "returns" if "returns" in df.columns else df.columns[0]
    data = df.copy()

    n = len(data)
    train_len = max(1, int(n * train_ratio))
    test_len = max(1, (n - train_len) // splits) if splits else max(1, n - train_len)

    sharpe_list = []
    mdd_list = []
    pnl_list = []
    stability_list = []

    from . import [REDACTED]

    indices = list(range(n))

    for tr_idx, te_idx in [REDACTED](indices, train_len, test_len):
        train_df = data.iloc[tr_idx]
        test_df = data.iloc[te_idx]

        # Parameter optimisation based on Sharpe ratio
        if params:
            best_col = None
            best_score = float("-inf")
            keys, values = zip(*params.items()) if params else ([], [])
            for combo in product(*values):
                col_name = "_".join(f"{k}={v}" for k, v in zip(keys, combo))
                if col_name not in data.columns:
                    continue
                score = _sharpe(train_df[col_name])
                if score > best_score:
                    best_score = score
                    best_col = col_name
            series = test_df[best_col] if best_col else test_df[returns_col]
        else:
            series = test_df[returns_col]

        sharpe_list.append(_sharpe(series))
        equity = (1 + series / 100.0).cumprod()
        mdd_list.append(max_drawdown(equity))
        pnl_list.append((equity.iloc[-1] - 1) * 100 if len(equity) else 0.0)
        stability_list.append(_stability(equity))

    count = len(sharpe_list) or 1
    mean_sharpe = sum(sharpe_list) / count
    mean_mdd = sum(mdd_list) / count
    mean_pnl = sum(pnl_list) / count
    mean_stability = sum(stability_list) / count

    return {
        "sharpe": mean_sharpe,
        "mdd": mean_mdd,
        "pnl": mean_pnl,
        "stability": mean_stability,
    }
--- [168/191] ./engine/backtest/loader_csv.py ---
# engine/backtest/loader_csv.py
from __future__ import annotations
import csv, glob
from pathlib import Path
from typing import List, Tuple, Optional

Row = Tuple[int, float, float, float, float, float]

def _cands(data_dir: str, symbol: str, tf: str) -> List[Path]:
    s = symbol.upper().replace("/", "").replace("_", "")
    t = tf.lower()
    root = Path(data_dir)

    patterns = [
        # ✅ tes fichiers (ohlcv/SYMBOL/TF.csv)
        root / "ohlcv" / s / f"{t}.csv",
        # autres conventions possibles (on garde)
        root / "live" / f"{s}_{t}.csv",
        root / "ohlcv" / f"{s}_{t}.csv",
        root / s / f"{t}.csv",
        root / s / t / "ohlcv.csv",
        root / f"{s}_{t}.csv",
        root / f"{s}-{t}.csv",
        root / f"{s}{t}.csv",
        root / "live" / s / f"{t}.csv",
        # globs de secours
        *[Path(p) for p in glob.glob(str(root / "ohlcv" / s / f"{t}.csv"))],
        *[Path(p) for p in glob.glob(str(root / "**" / f"{s}_{t}.csv"), recursive=True)],
        *[Path(p) for p in glob.glob(str(root / "**" / f"{s}-{t}.csv"), recursive=True)],
        *[Path(p) for p in glob.glob(str(root / "**" / s / f"{t}.csv"), recursive=True)],
    ]
    out, seen = [], set()
    for p in patterns:
        if p not in seen:
            out.append(p); seen.add(p)
    return out

def _read_csv_tail(path: Path, max_rows: int) -> List[Row]:
    if not path.exists(): return []
    rows: List[Row] = []
    with path.open("r", encoding="utf-8", errors="ignore") as f:
        r = csv.reader(f); first = True
        for line in r:
            if not line: continue
            if first:
                first = False
                try: int(float(line[0]))
                except Exception:  # header
                    continue
            try:
                ts = int(float(line[0]))
                o,h,l,c,v = map(float, line[1:6])
                rows.append((ts,o,h,l,c,v))
            except Exception:
                continue
    return rows[-max_rows:] if max_rows and len(rows) > max_rows else rows

def find_csv_path(data_dir: str, symbol: str, tf: str) -> Optional[Path]:
    for p in _cands(data_dir, symbol, tf):
        if p.exists(): return p
    return None

def load_csv_ohlcv(data_dir: str, symbol: str, tf: str, max_rows: int = 0) -> List[Row]:
    p = find_csv_path(data_dir, symbol, tf)
    if not p: return []
    return _read_csv_tail(p, max_rows or 0)--- [169/191] ./engine/backtest/optimize.py ---
from __future__ import annotations

"""Parameter sweep utilities for strategy optimisation.

This module performs a grid search over a parameter space in parallel.  It
tries to use :mod:`ray` for distributed execution when available and falls
back to :mod:`multiprocessing` otherwise.
"""

import itertools
import json
import multiprocessing as mp
import os
from typing import Any, Dict, Iterable, List, Sequence

try:  # Optional dependency
    import ray  # type: ignore
except Exception:  # pragma: no cover - ray is optional
    ray = None

from engine.backtest import backtest_trades


# ---------------------------------------------------------------------------
# Parameter space
# ---------------------------------------------------------------------------

def param_space_default() -> Dict[str, Sequence[Any]]:
    """Return the default parameter search space.

    The keys correspond to strategy parameters while the values are iterables
    of possible settings.  The defaults represent a small but representative
    grid and can be overridden by callers.
    """

    return {
        "ema_fast": [10, 20, 30],
        "ema_slow": [50, 100, 200],
        "rsi_period": [14, 21],
        "atr_period": [14, 21],
    }


def _param_grid(space: Dict[str, Iterable[Any]]) -> List[Dict[str, Any]]:
    """Expand *space* into a list of parameter combinations."""

    keys = list(space)
    values = [space[k] for k in keys]
    return [dict(zip(keys, combo)) for combo in itertools.product(*values)]


# ---------------------------------------------------------------------------
# Evaluation
# ---------------------------------------------------------------------------

def eval_params_one(grid_item: Dict[str, Any]) -> Dict[str, Any]:
    """Run a backtest for a single parameter combination.

    ``grid_item`` contains the parameter values along with optional ``trades``
    to evaluate.  The function returns a copy of the parameters augmented with
    the computed PnL under the key ``pnl``.
    """

    params = dict(grid_item)
    trades = params.pop("trades", [])
    fee_rate = params.pop("fee_rate", None)
    pnl = backtest_trades(trades, fee_rate=fee_rate)
    params["pnl"] = pnl
    return params


# ---------------------------------------------------------------------------
# Orchestration
# ---------------------------------------------------------------------------

def run_param_sweep(space: Dict[str, Iterable[Any]] | None = None, *, jobs: int | None = None) -> List[Dict[str, Any]]:
    """Evaluate the full parameter grid in parallel and return results."""

    space = space or param_space_default()
    grid = _param_grid(space)

    # Determine execution backend
    use_ray = False
    if ray is not None:
        try:  # pragma: no cover - depends on ray
            ray.init(ignore_reinit_error=True)
            use_ray = True
        except Exception:
            use_ray = False

    if use_ray:
        remote_eval = ray.remote(eval_params_one)  # type: ignore
        futures = [remote_eval.remote(g) for g in grid]
        results = ray.get(futures)
    else:
        jobs = jobs or int(os.getenv("OPT_JOBS", "0")) or mp.cpu_count()
        with mp.Pool(processes=jobs) as pool:
            results = pool.map(eval_params_one, grid)

    return results


def optimize(space: Dict[str, Iterable[Any]] | None = None, *, outfile: str = "opt_results.json", jobs: int | None = None) -> List[Dict[str, Any]]:
    """High level helper executing the sweep and saving aggregated results."""

    results = run_param_sweep(space, jobs=jobs)
    with open(outfile, "w", encoding="utf8") as fh:
        json.dump(results, fh, indent=2, sort_keys=True)
    return results


def main() -> None:  # pragma: no cover - convenience CLI
    optimize()


if __name__ == "__main__":  # pragma: no cover
    main()
--- [170/191] ./engine/backtest/market_data.py ---
from __future__ import annotations

import json
import os
import time
from pathlib import Path
from typing import Any, Iterable, Optional, Sequence, Tuple
from urllib.request import Request, urlopen
from urllib.error import URLError, HTTPError

import pandas as pd

# ============================================================================
# Logs & utilitaires
# ============================================================================
BT_DEBUG = int(os.getenv("BT_DEBUG", "0") or "0")
def _log(msg: str) -> None:
    if BT_DEBUG:
        print(f"[bt.debug] {msg}", flush=True)

def _now_ms() -> int:
    return int(time.time() * 1000)

def _tf_to_seconds(tf: str) -> int:
    tf = tf.lower().strip()
    table = {"1m":60,"3m":180,"5m":300,"15m":900,"30m":1800,"1h":3600,"4h":14400,"1d":86400}
    if tf not in table:
        raise ValueError(f"Timeframe non supporté: {tf}")
    return table[tf]

def _parse_duration(s: str) -> int:
    """
    '90s','15m','2h','3d' -> secondes
    """
    s = s.strip().lower()
    if s.endswith("s"): return int(float(s[:-1]))
    if s.endswith("m"): return int(float(s[:-1])*60)
    if s.endswith("h"): return int(float(s[:-1])*3600)
    if s.endswith("d"): return int(float(s[:-1])*86400)
    return int(float(s))  # secondes

# ============================================================================
# Politique de fraîcheur (par défaut + overrides via ENV)
# ============================================================================
def [REDACTED](tf: str) -> int:
    """
    Règles par défaut (conservatrices) :
      - 1m..15m : 2 × TF  (ex: 5m -> 10m)
      - 30m     : 1h
      - 1h      : 6h
      - 4h      : 24h
      - 1d      : 3d
    """
    tf = tf.lower()
    if tf in ("1m","3m","5m","15m"):
        return 2 * _tf_to_seconds(tf)
    if tf == "30m":
        return 3600
    if tf == "1h":
        return 6*3600
    if tf == "4h":
        return 24*3600
    if tf == "1d":
        return 3*86400
    raise ValueError(tf)

def _max_age_seconds(tf: str) -> int:
    """
    Overrides possibles (au choix) :
      - CSV_MAX_AGE_MULT=NN → NN × TF  (ex: 50 pour 1m => 50 minutes)
      - CSV_MAX_AGE_5m="45m" (prioritaire si présent)
      - CSV_MAX_AGE_DEFAULT="2h" (fallback global)
    """
    tfk = tf.lower().replace(":", "")
    env_spec = os.getenv(f"CSV_MAX_AGE_{tfk}")
    if env_spec:
        return _parse_duration(env_spec)
    mult = os.getenv("CSV_MAX_AGE_MULT")
    if mult:
        return int(float(mult) * _tf_to_seconds(tf))
    g = os.getenv("CSV_MAX_AGE_DEFAULT")
    if g:
        return _parse_duration(g)
    return [REDACTED](tf)

# ============================================================================
# CSV helpers + validation
# ============================================================================
def _data_dir(default: str = "data") -> Path:
    root = Path(os.getenv("DATA_DIR", default))
    root.mkdir(parents=True, exist_ok=True)
    return root

def _csv_path(symbol: str, timeframe: str) -> Path:
    tf = timeframe.replace(":", "")
    return _data_dir() / f"{symbol}-{tf}.csv"

def _rows_to_df(rows: Iterable[Iterable[float]]) -> pd.DataFrame:
    rows = list(rows)
    if not rows:
        raise ValueError("OHLCV vide")
    unit = "ms" if rows[0][0] > 10_000_000_000 else "s"
    df = pd.DataFrame(rows, columns=["ts","open","high","low","close","volume"])
    df["timestamp"] = pd.to_datetime(df["ts"], unit=unit, utc=True)
    return df.drop(columns=["ts"]).set_index("timestamp").sort_index()

def _read_csv(path: Path) -> pd.DataFrame:
    df = pd.read_csv(path)
    # tolère quelques variations de colonnes
    cols = {c.lower(): c for c in df.columns}
    ts_col = cols.get("timestamp") or cols.get("time") or cols.get("date") or cols.get("ts")
    if not ts_col:
        raise ValueError("Colonne temps absente (timestamp/time/date/ts)")
    rename = {ts_col: "timestamp"}
    for c in ("open","high","low","close","volume"):
        if c not in cols:
            raise ValueError(f"Colonne manquante: {c}")
        rename[cols[c]] = c
    df = df.rename(columns=rename)
    df["timestamp"] = pd.to_datetime(df["timestamp"], utc=True, [REDACTED]=True)
    df = df[["timestamp","open","high","low","close","volume"]].sort_values("timestamp")
    df = df.drop_duplicates("timestamp")
    df = df.set_index("timestamp")
    return df

def _write_csv(path: Path, df: pd.DataFrame) -> None:
    out = df.reset_index().rename(columns={"index": "timestamp"})
    out.to_csv(path, index=False)

def [REDACTED](path: Path, timeframe: str, *, min_rows: int = 100) -> Tuple[bool, str]:
    """
    Retourne (ok, reason). ok=True si le CSV est utilisable:
      - schéma valide
      - assez de lignes
      - fraîcheur < seuil selon TF
    """
    if not path.exists():
        return False, "absent"
    try:
        df = _read_csv(path)
    except Exception as e:
        return False, f"invalid({e})"
    if len(df) < min_rows:
        return False, f"too_few_rows({len(df)}<{min_rows})"
    # Fraîcheur
    last_ts = int(df.index.max().timestamp())
    age_s = int(time.time()) - last_ts
    max_age = _max_age_seconds(timeframe)
    if age_s > max_age:
        return False, f"stale({age_s}s>{max_age}s)"
    # Monotonicité (échantillon)
    if not df.index.[REDACTED]:
        return False, "not_monotonic"
    return True, "ok"

# ============================================================================
# Fallback réseau (CCXT d'abord, HTTP sinon)
# ============================================================================
def _ensure_ccxt() -> Any | None:
    try:
        import ccxt  # type: ignore
        return ccxt
    except Exception:
        return None

def _fetch_via_ccxt(symbol: str, timeframe: str, limit: int = 1000) -> Optional[pd.DataFrame]:
    ccxt = _ensure_ccxt()
    if not ccxt:
        _log("ccxt indisponible")
        return None
    ex = ccxt.bitget({"enableRateLimit": True, "options": {"defaultType": "swap"}})
    ex.load_markets()
    base = symbol.upper()
    if not base.endswith("USDT"):
        raise ValueError("symbol doit finir par USDT (ex: BTCUSDT)")
    coin = base[:-4]
    candidates = [f"{coin}/USDT:USDT", f"{coin}/USDT"]  # perp puis spot
    for ccxt_sym in candidates:
        try:
            rows = ex.fetch_ohlcv(ccxt_sym, timeframe=timeframe, limit=limit)
            if rows:
                return _rows_to_df(sorted(rows, key=[REDACTED] r: r[0]))
        except Exception as e:
            _log(f"ccxt fail {ccxt_sym}: {e}")
            continue
    return None

# === (facultatif) HTTP Bitget v1 minimal ===
_GRAN_MIX = {"1m":"1min","3m":"3min","5m":"5min","15m":"15min","30m":"30min","1h":"1h","4h":"4h","1d":"1day"}
_PERIOD_SPOT = {"1m":"1min","3m":"3min","5m":"5min","15m":"15min","30m":"30min","1h":"1hour","4h":"4hour","1d":"1day"}

def _http_get(url: str, timeout: int = 20) -> dict | list:
    req = Request(url, headers={"User-Agent":"backtest-marketdata/1.0"})
    with urlopen(req, timeout=timeout) as resp:
        return json.loads(resp.read().decode("utf-8"))

def [REDACTED](payload: dict | list) -> list[list[float]]:
    rows = payload.get("data") if isinstance(payload, dict) else payload
    if not isinstance(rows, list):
        raise ValueError(f"Réponse inattendue: {payload}")
    out = []
    for r in rows:
        ts = int(str(r[0])); o,h,l,c,v = map(float,(r[1],r[2],r[3],r[4],r[5]))
        out.append([ts,o,h,l,c,v])
    out.sort(key=[REDACTED] x:x[0])
    return out

def _fetch_via_http(symbol: str, timeframe: str, limit: int = 1000) -> Optional[pd.DataFrame]:
    tf = timeframe.lower()
    g = _GRAN_MIX.get(tf); p = _PERIOD_SPOT.get(tf)
    if not (g and p):
        return None
    # mix umcbl puis spot spbl, paramètres minimum (v1)
    trials = [
        f"https://api.bitget.com/api/mix/v1/market/candles?symbol={symbol}_UMCBL&granularity={g}&limit={limit}",
        f"https://api/bitget.com/api/mix/v1/market/candles?symbol={symbol}&granularity={g}&limit={limit}",
        f"https://api.bitget.com/api/spot/v1/market/candles?symbol={symbol}_SPBL&period={p}&limit={limit}",
        f"https://api.bitget.com/api/spot/v1/market/candles?symbol={symbol}&period={p}&limit={limit}",
    ]
    for url in trials:
        try:
            payload = _http_get(url)
            if isinstance(payload, dict) and "code" in payload and str(payload["code"]) != "00000" and "data" not in payload:
                raise RuntimeError(f"Bitget error {payload.get('code')}: {payload.get('msg')}")
            rows = [REDACTED](payload)
            if rows:
                return _rows_to_df(rows)
        except Exception as e:
            _log(f"HTTP fail: {url} -> {e}")
            continue
    return None

# ============================================================================
# API publique utilisée par l’orchestrateur/backtest
# ============================================================================
def fetch_ohlcv_best(symbol: str, timeframe: str, *, limit: int = 1000) -> pd.DataFrame:
    """
    Tente d’abord CCXT (si présent), sinon HTTP v1. Lève si tout échoue.
    """
    df = _fetch_via_ccxt(symbol, timeframe, limit=limit)
    if df is not None:
        _log(f"source=ccxt  n={len(df)}")
        return df
    df = _fetch_via_http(symbol, timeframe, limit=limit)
    if df is not None:
        _log(f"source=http  n={len(df)}")
        return df
    raise RuntimeError(f"Aucune source OHLCV pour {symbol} {timeframe}")

def hybrid_loader(
    data_dir: str = "data",
    *,
    use_cache_first: bool = True,
    min_rows: int = 100,
    refill_if_stale: bool = True,
    network_limit: int = 1000,
):
    """
    Loader smart :
      1) si CSV présent ET frais/valide → le renvoie
      2) sinon, si refill_if_stale → recharge (CCXT>HTTP) puis écrit CSV
      3) sinon → lève
    """
    os.environ.setdefault("DATA_DIR", data_dir)

    def load(symbol: str, timeframe: str, start: str | None, end: str | None) -> pd.DataFrame:
        path = _csv_path(symbol, timeframe)

        if use_cache_first:
            ok, why = [REDACTED](path, timeframe, min_rows=min_rows)
            if ok:
                _log(f"CSV OK: {path}")
                df = _read_csv(path)
            else:
                _log(f"CSV non utilisable ({why}): {path}")
                if not refill_if_stale:
                    raise RuntimeError(f"CSV invalide et recharge désactivée: {path} ({why})")
                df = fetch_ohlcv_best(symbol, timeframe, limit=network_limit)
                _write_csv(path, df)
        else:
            df = fetch_ohlcv_best(symbol, timeframe, limit=network_limit)
            _write_csv(path, df)

        # Fenêtrage temporel si demandé (timestamps UTC)
        if start:
            df = df.loc[pd.Timestamp(start, tz="UTC") :]
        if end:
            df = df.loc[: pd.Timestamp(end, tz="UTC")]
        return df

    return load--- [171/191] ./engine/backtest/cli.py ---
from __future__ import annotations

import argparse
from engine.backtest.runner import run_multi, csv_loader_factory

def create_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(prog="backtest", description="Backtest multi symboles / multi timeframes")
    p.add_argument("--symbols", required=True, help="Liste, ex: BTCUSDT,ETHUSDT,SOLUSDT")
    p.add_argument("--timeframes", required=True, help="Liste, ex: 1m,5m,15m")
    p.add_argument("--data-dir", default="data", help="Répertoire CSV OHLCV")
    p.add_argument("--out-dir", default="result", help="Répertoire de sortie")
    p.add_argument("--cash", type=float, default=10_000.0)
    p.add_argument("--risk", type=float, default=0.005, help="risk_pct par trade (0.005 = 0.5%)")
    p.add_argument("--slippage-bps", type=float, default=1.5)
    return p

def main(argv: list[str] | None = None) -> int:
    p = create_parser()
    a = p.parse_args(argv)
    symbols = [s.strip().upper() for s in a.symbols.split(",") if s.strip()]
    tfs = [t.strip() for t in a.timeframes.split(",") if t.strip()]
    loader = csv_loader_factory(a.data_dir)
    run_multi(
        symbols=symbols,
        timeframes=tfs,
        loader=loader,
        out_dir=a.out_dir,
        initial_cash=a.cash,
        risk_pct=a.risk,
        slippage_bps=a.slippage_bps,
    )
    print(f"✅ Backtests terminés → {a.out_dir}/ (equity_curve/trades/fills/metrics/summary)")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())--- [172/191] ./engine/backtest/grid_search.py ---
"""Grid-search express module to evaluate hyperparameter combinations.

This module builds combinations of strategy and engine parameters, runs the
existing multi symbol backtester for each combination, collects key metrics and
selects the best configuration according to:

1. Profit factor (descending)
2. Maximum drawdown percentage (ascending)
3. Net PnL in USDT (descending)
4. Number of trades (ascending)

Results are written under ``result/grid`` by default and a short summary is
printed to the console.
"""
from __future__ import annotations

from dataclasses import dataclass
import csv
import json
import os
import random
from itertools import product
from typing import Any, Callable, Dict, Iterable, List, Sequence

# ---------------------------------------------------------------------------
# Utilities
# ---------------------------------------------------------------------------


def parse_hours(hours: str) -> List[int]:
    """Parse hours specification like ``"7-11,13-17"`` into a list of ints.

    Each comma separated element can either be a single hour (``"8"``) or a
    range ``"7-11"`` which is inclusive.  Returned hours are sorted and unique.
    """

    if not hours:
        return []
    result: List[int] = []
    for part in hours.split(","):
        part = part.strip()
        if not part:
            continue
        if "-" in part:
            start_s, end_s = part.split("-", 1)
            start, end = int(start_s), int(end_s)
            result.extend(range(start, end + 1))
        else:
            result.append(int(part))
    return sorted(set(result))


# Order of parameters used throughout the module and in CSV output
PARAM_KEY=[REDACTED] = [
    "timeframe",
    "score_min",
    "atr_min_ratio",
    "rr_min",
    "risk_pct",
    "slippage_bps",
    "fee_rate",
    "cooldown_secs",
    "hours",
]

# Default values used if a parameter is not provided in the grid
DEFAULTS = {
    "score_min": 55,
    "atr_min_ratio": 0.002,
    "rr_min": 1.2,
    "risk_pct": 0.01,
    "slippage_bps": 2,
    "fee_rate": 0.001,
    "cooldown_secs": 300,
    "hours": "7-11,13-17",
}


@dataclass
class GridResult:
    params: Dict[str, Any]
    metrics: Dict[str, float]


def _ensure_list(val: Sequence[Any] | Any) -> List[Any]:
    if isinstance(val, (list, tuple, set)):
        return list(val)
    return [val]


def build_param_grid(param_lists: Dict[str, Sequence[Any]], grid_max: int) -> List[Dict[str, Any]]:
    """Return a list of parameter combinations.

    ``param_lists`` maps parameter names to a sequence of values.  Missing keys
    fall back to ``DEFAULTS``.  The resulting cartesian product is uniformly
    sampled to ``grid_max`` elements when necessary while trying to maintain a
    variety of timeframes and ``atr_min_ratio`` values.
    """

    lists: Dict[str, List[Any]] = {}
    for key in PARAM_KEY=[REDACTED]
        if key =[REDACTED] "timeframe":
            # timeframe must be explicitly provided; default empty -> "1m"
            vals = param_lists.get(key) or ["1m"]
        else:
            vals = param_lists.get(key)
            if not vals:
                default = DEFAULTS[key]
                vals = [default]
        lists[key] = _ensure_list(vals)

    combos: List[Dict[str, Any]] = [
        dict(zip(PARAM_KEY=[REDACTED] values)) for values in product(*(lists[k] for k in PARAM_KEY=[REDACTED]
    ]

    # Uniform sampling if exceeding grid_max
    if len(combos) > grid_max:
        step = len(combos) / float(grid_max)
        sampled = []
        for i in range(grid_max):
            idx = int(round(i * step))
            if idx >= len(combos):
                idx = len(combos) - 1
            sampled.append(combos[idx])
        # ensure each timeframe appears at least once
        wanted_tfs = set(lists["timeframe"])
        present_tfs = {c["timeframe"] for c in sampled}
        missing = list(wanted_tfs - present_tfs)
        if missing:
            for tf in missing:
                for c in combos:
                    if c["timeframe"] == tf and c not in sampled:
                        sampled.append(c)
                        break
            sampled = sampled[:grid_max]
        combos = sampled
    return combos


# ---------------------------------------------------------------------------
# Core runner
# ---------------------------------------------------------------------------


def run_grid_search(
    *,
    symbols: Sequence[str],
    exchange: str,
    base_params: Dict[str, Any],
    param_lists: Dict[str, Sequence[Any]],
    grid_max: int = 12,
    csv_dir: str | None = None,
    initial_equity: float = 1000.0,
    leverage: float = 1.0,
    paper_constraints: bool = True,
    seed: int | None = None,
    out_dir: str = "./result/grid",
    [REDACTED]: bool = False,  # placeholder for compatibility
    run_func: Callable[..., Any] | None = None,
) -> List[GridResult]:
    """Execute grid search across parameter combinations.

    ``base_params`` provides default single values for parameters. ``param_lists``
    contains the grid specifications from CLI (already parsed into sequences).
    ``run_func`` should have the same signature as :func:`run_backtest_multi`.
    """

    if seed is not None:
        random.seed(seed)

    if run_func is None:  # avoid circular import at module load
        from .run_multi import run_backtest_multi  # late import

        run_func = run_backtest_multi

    # merge lists with defaults
    full_lists: Dict[str, Sequence[Any]] = {}
    for k in PARAM_KEY=[REDACTED]
        if k == "timeframe":
            full_lists[k] = param_lists.get(k) or [base_params.get("timeframe", "1m")]
        else:
            if param_lists.get(k) is not None:
                full_lists[k] = param_lists[k]
            else:
                full_lists[k] = [base_params.get(k, DEFAULTS[k])]

    combos = build_param_grid(full_lists, grid_max)

    results: List[GridResult] = []
    os.makedirs(out_dir, exist_ok=True)

    for combo in combos:
        # Build parameters for backtester
        tf = combo["timeframe"]
        fee = float(combo["fee_rate"])
        slip = float(combo["slippage_bps"])
        risk = float(combo["risk_pct"])

        summary, _trades = run_func(
            symbols=list(symbols),
            exchange=exchange,
            timeframe=tf,
            csv_dir=csv_dir,
            fee_rate=fee,
            slippage_bps=slip,
            risk_pct=risk,
            initial_equity=initial_equity,
            leverage=leverage,
            paper_constraints=paper_constraints,
            seed=seed,
            out_dir=os.path.join(out_dir, "tmp"),
            plot=False,
            dry_run=True,
        )
        total = next((r for r in summary if r.get("symbol") == "TOTAL"), summary[-1])
        metrics = {
            "pnl_usdt": float(total.get("pnl_usdt", 0.0)),
            "profit_factor": float(total.get("profit_factor", 0.0)),
            "max_dd_pct": float(total.get("max_drawdown_pct", 0.0)),
            "winrate_pct": float(total.get("winrate_pct", 0.0)),
            "trades": float(total.get("trades", 0.0)),
            "final_equity": initial_equity + float(total.get("pnl_usdt", 0.0)),
        }
        results.append(GridResult(params=combo, metrics=metrics))

    # sort results
    results.sort(
        key=[REDACTED] r: (
            -r.metrics["profit_factor"],
            r.metrics["max_dd_pct"],
            -r.metrics["pnl_usdt"],
            r.metrics["trades"],
        )
    )

    # console output -------------------------------------------------------
    print(
        f"Grid-search express ({len(results)} combinaisons testées, top trié par PF↓ puis MaxDD%↑)"
    )
    header = (
        f"{'timeframe':<8} {'PF':>6} {'MaxDD%':>8} {'PnL':>8} {'Trades':>8}"
    )
    print(header)
    for r in results[:10]:
        m = r.metrics
        print(
            f"{r.params['timeframe']:<8} {m['profit_factor']:>6.2f} {m['max_dd_pct']:>8.2f} {m['pnl_usdt']:>8.2f} {int(m['trades']):>8}"
        )

    # write csv ------------------------------------------------------------
    csv_cols = PARAM_KEY=[REDACTED] + [
        "pnl_usdt",
        "profit_factor",
        "max_dd_pct",
        "winrate_pct",
        "trades",
        "final_equity",
    ]
    with open(os.path.join(out_dir, "grid_results.csv"), "w", newline="") as fh:
        writer = csv.DictWriter(fh, fieldnames=csv_cols)
        writer.writeheader()
        for r in results:
            row = {**r.params, **r.metrics}
            writer.writerow(row)

    best = results[0]
    with open(os.path.join(out_dir, "best_config.json"), "w", encoding="utf8") as fh:
        json.dump({"params": best.params, "metrics": best.metrics}, fh, indent=2)

    # markdown summary -----------------------------------------------------
    md_path = os.path.join(out_dir, "grid_summary.md")
    with open(md_path, "w", encoding="utf8") as fh:
        fh.write(
            "| timeframe | PF | MaxDD% | PnL | trades |\n|---|---|---|---|---|\n"
        )
        for r in results[:10]:
            m = r.metrics
            fh.write(
                f"| {r.params['timeframe']} | {m['profit_factor']:.2f} | {m['max_dd_pct']:.2f} | {m['pnl_usdt']:.2f} | {int(m['trades'])} |\n"
            )

    # optional scatter plot ------------------------------------------------
    try:  # pragma: no cover - optional dependency
        import matplotlib.pyplot as plt

        pf = [r.metrics["profit_factor"] for r in results]
        dd = [r.metrics["max_dd_pct"] for r in results]
        trades = [r.metrics["trades"] for r in results]
        tfs = [r.params["timeframe"] for r in results]
        colors = {tf: i for i, tf in enumerate(sorted(set(tfs)))}
        c = [colors[tf] for tf in tfs]
        plt.figure(figsize=(6, 4))
        plt.scatter(dd, pf, c=c, s=[max(10, t) for t in trades], alpha=0.7)
        plt.xlabel("MaxDD%")
        plt.ylabel("Profit Factor")
        plt.title("PF vs MaxDD")
        plt.savefig(os.path.join(out_dir, "pf_vs_dd.png"))
        plt.close()
    except Exception:  # pragma: no cover
        pass

    return results


__all__ = ["run_grid_search", "build_param_grid", "parse_hours", "GridResult"]
--- [173/191] ./engine/backtest/position_sizing.py ---
# scalper/backtest/position_sizing.py
from __future__ import annotations
from engine.core.signal import Signal

def [REDACTED](equity: float, sig: Signal, risk_pct: float) -> float:
    """
    Taille = (equity * risk_pct) / |entry - sl|
    Retourne la QUANTITÉ (unités de la crypto).
    """
    risk = max(1e-12, abs(sig.entry - sig.sl))
    cash_at_risk = max(0.0, equity) * max(0.0, risk_pct)
    return max(0.0, cash_at_risk / risk)

def fees_cost(notional: float, bps: float) -> float:
    return abs(notional) * (bps / 10000.0)--- [174/191] ./engine/backtest/run_multi.py ---
# annulé--- [175/191] ./engine/backtest/engine.py ---
from __future__ import annotations
import math, statistics as stats
from dataclasses import dataclass
from typing import Any, Dict, List, Tuple
import pandas as pd
from engine.core.signals import compute_signals

@dataclass
class BTParams:
    ema_fast: int = 20
    ema_slow: int = 50
    atr_period: int = 14
    trail_atr_mult: float = 2.0
    risk_pct_equity: float = 0.02
    cash: float = 10_000.0
    slippage_bps: int = 5

def grid_params() -> List[Dict[str, Any]]:
    grid: List[Dict[str, Any]] = []
    for fast in (9, 12, 20):
        for slow in (26, 50, 100):
            if slow <= fast: continue
            for atr_mult in (1.5, 2.0, 2.5):
                grid.append({"ema_fast": fast, "ema_slow": slow, "trail_atr_mult": atr_mult})
    return grid

def run_backtest_once(symbol: str, tf: str, ohlcv: pd.DataFrame, base_cfg_path: str | None = None, params: Dict[str, Any] | None = None):
    p = BTParams(**(params or {}))
    df = compute_signals(ohlcv, {
        "ema_fast": p.ema_fast, "ema_slow": p.ema_slow, "atr_period": p.atr_period
    })
    df = df.dropna().reset_index(drop=True)
    equity = p.cash
    pos_size = 0.0
    entry = 0.0
    trail = None
    trades: List[Dict[str, Any]] = []
    equity_curve: List[float] = [equity]

    for i in range(1, len(df)):
        row_prev = df.iloc[i-1]
        row = df.iloc[i]
        price = float(row["close"])
        atr = float(row["atr"])
        slip = price * (p.slippage_bps / 10_000.0)

        # entrée / sortie par signaux EMA
        sig = int(row_prev["signal"])  # on agit à la bougie suivante
        if pos_size == 0 and sig == 1:
            # calcule une taille simple basée sur le risque % et ATR
            risk_per_unit = max(atr, price * 0.002)  # garde-fou min
            risk_cash = equity * p.risk_pct_equity
            units = max(1.0, risk_cash / risk_per_unit)
            pos_size = units
            entry = price + slip
            trail = entry - p.trail_atr_mult * atr
        elif pos_size > 0:
            # mise à jour du trailing stop
            new_trail = price - p.trail_atr_mult * atr
            if trail is None or new_trail > trail:
                trail = new_trail
            # sortie par signal inverse ou cassure du trail
            exit_signal = (sig == -1) or (price < (trail or 0.0))
            if exit_signal:
                exit_price = price - slip
                pnl = (exit_price - entry) * pos_size
                equity += pnl
                trades.append({"entry": entry, "exit": exit_price, "pnl": pnl})
                pos_size = 0.0
                entry = 0.0
                trail = None

        equity_curve.append(equity if equity > 0 else 0.0)

    metrics = compute_metrics(equity_curve, trades)
    return {"equity_curve": equity_curve, "trades": trades, "metrics": metrics}

def compute_metrics(equity_curve: List[float], trades: List[Dict[str, Any]]) -> Dict[str, float]:
    if not equity_curve:
        return {"net_pnl_pct":0.0,"win_rate":0.0,"max_dd_pct":0.0,"sharpe":0.0,"trades":0,"score":0.0}
    start, end = equity_curve[0], equity_curve[-1]
    net_pnl_pct = (end - start) / start if start else 0.0
    # drawdown
    peak = -1e30; max_dd = 0.0
    for v in equity_curve:
        peak = max(peak, v)
        dd = (peak - v) / peak if peak > 0 else 0.0
        max_dd = max(max_dd, dd)
    # sharpe approx (rendements par pas)
    if len(equity_curve) > 1:
        rets = []
        for i in range(1, len(equity_curve)):
            prev, cur = equity_curve[i-1], equity_curve[i]
            r = (cur - prev) / prev if prev else 0.0
            rets.append(r)
        if rets and (stats.pstdev(rets) or 0) > 0:
            sharpe = (stats.mean(rets) / stats.pstdev(rets)) * math.sqrt(252)
        else:
            sharpe = 0.0
    else:
        sharpe = 0.0
    wins = sum(1 for t in trades if t.get("pnl",0.0) > 0)
    wr = (wins / len(trades)) if trades else 0.0
    score = (sharpe * 2.0) + (net_pnl_pct * 1.0) - (max_dd * 0.5)
    return {"net_pnl_pct": float(net_pnl_pct), "win_rate": float(wr), "max_dd_pct": float(max_dd),
            "sharpe": float(sharpe), "trades": float(len(trades)), "score": float(score)}--- [176/191] ./engine/backtest/cache.py ---
# scalper/backtest/cache.py
from __future__ import annotations

import csv
import json
import os
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple, Iterable, Optional

# ---------------- Timeframe utils ----------------

_TF_SECONDS = {
    "1m": 60, "3m": 180, "5m": 300, "15m": 900, "30m": 1800,
    "1h": 3600, "2h": 7200, "4h": 14400, "6h": 21600, "12h": 43200,
    "1d": 86400, "3d": 259200, "1w": 604800,
}

def tf_to_seconds(tf: str) -> int:
    tf = tf.strip().lower()
    if tf not in _TF_SECONDS:
        raise ValueError(f"Timeframe inconnu: {tf}")
    return _TF_SECONDS[tf]

# ---------------- Fraîcheur cible par TF ----------------

_DEFAULT_MAX_AGE = {
    # règle empirique (peut être surchargée par ENV)
    "1m": 2 * 3600,        # 2h
    "3m": 4 * 3600,        # 4h
    "5m": 12 * 3600,       # 12h
    "15m": 24 * 3600,      # 24h
    "30m": 36 * 3600,      # 36h
    "1h": 3 * 86400,       # 3 jours
    "2h": 5 * 86400,       # 5 jours
    "4h": 10 * 86400,      # 10 jours
    "6h": 15 * 86400,      # 15 jours
    "12h": 20 * 86400,     # 20 jours
    "1d": 3 * 86400,       # 3 jours (ok si 2 jours comme tu voulais)
    "3d": 10 * 86400,
    "1w": 30 * 86400,
}

def max_age_for_tf(tf: str) -> int:
    """Autorise override ENV via BACKTEST_MAX_AGE_<TF> (en secondes)."""
    tf = tf.lower()
    env_key = [REDACTED]"BACKTEST_MAX_AGE_{tf.replace('m','M').replace('h','H').replace('d','D').replace('w','W')}"
    if env_key in os.environ:
        try:
            return int(os.environ[env_key])
        except Exception:
            pass
    return _DEFAULT_MAX_AGE.get(tf, 7 * 86400)

# ---------------- CSV I/O ----------------

def data_dir() -> Path:
    d = Path(os.getenv("DATA_DIR", "data"))
    d.mkdir(parents=True, exist_ok=True)
    return d

def csv_path(symbol: str, tf: str) -> Path:
    return data_dir() / f"{symbol.upper()}-{tf}.csv"

def read_csv_ohlcv(path: Path) -> List[List[float]]:
    out: List[List[float]] = []
    if not path.exists():
        return out
    with path.open("r", newline="") as f:
        r = csv.reader(f)
        header = next(r, None)
        for row in r:
            # columns: timestamp,open,high,low,close,volume
            try:
                ts, o, h, l, c, v = row[:6]
                out.append([int(ts), float(o), float(h), float(l), float(c), float(v)])
            except Exception:
                continue
    return out

def write_csv_ohlcv(path: Path, rows: Iterable[Iterable[float]]) -> None:
    new_file = not path.exists()
    with path.open("w", newline="") as f:
        w = csv.writer(f)
        w.writerow(["timestamp","open","high","low","close","volume"])
        for r in rows:
            w.writerow(r)

# ---------------- Validation / Chargement / Fetch ----------------

@dataclass
class CacheInfo:
    symbol: str
    tf: str
    path: Path
    exists: bool
    fresh: bool
    last_ts: Optional[int] = None
    rows: int = 0

def _is_fresh(last_ts: Optional[int], tf: str) -> bool:
    if not last_ts:
        return False
    age = int(time.time()) - int(last_ts / 1000)
    return age <= max_age_for_tf(tf)

def inspect_csv(symbol: str, tf: str) -> CacheInfo:
    p = csv_path(symbol, tf)
    if not p.exists():
        return CacheInfo(symbol, tf, p, exists=False, fresh=False)
    rows = read_csv_ohlcv(p)
    last_ts = rows[-1][0] if rows else None
    return CacheInfo(symbol, tf, p, exists=True, fresh=_is_fresh(last_ts, tf), last_ts=last_ts, rows=len(rows))

async def [REDACTED](exchange, symbol: str, tf: str, limit: int) -> List[List[float]]:
    # exchange: objet CCXT-like fourni par le live (déjà configuré Bitget)
    return await exchange.fetch_ohlcv(symbol=symbol, timeframe=tf, limit=limit)

async def [REDACTED](exchange, symbol: str, tf: str, limit: int) -> Tuple[CacheInfo, List[List[float]]]:
    info = inspect_csv(symbol, tf)
    if info.exists and info.fresh:
        data = read_csv_ohlcv(info.path)
        return info, data

    # fetch & persist
    data = await [REDACTED](exchange, symbol, tf, limit=limit)
    if data:
        write_csv_ohlcv(info.path, data)
        info = inspect_csv(symbol, tf)  # refresh stats
    return info, data

async def ensure_csv_cache(exchange, symbols: List[str], tf: str, limit: int) -> Dict[str, List[List[float]]]:
    """Vérifie le cache CSV et (re)charge depuis l'exchange si nécessaire."""
    out: Dict[str, List[List[float]]] = {}
    for s in symbols:
        info, rows = await [REDACTED](exchange, s, tf, limit)
        out[s] = rows
    return out

def [REDACTED](symbols: List[str], tf: str, out_path: Path) -> None:
    report = []
    for s in symbols:
        info = inspect_csv(s, tf)
        report.append({
            "symbol": s,
            "tf": tf,
            "path": str(info.path),
            "exists": info.exists,
            "fresh": info.fresh,
            "last_ts": info.last_ts,
            "rows": info.rows,
            "max_age": max_age_for_tf(tf),
        })
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text(json.dumps(report, indent=2))--- [177/191] ./engine/backtest/metrics.py ---
from __future__ import annotations
import math
from dataclasses import dataclass
from typing import List, Dict

@dataclass
class Trade:
    ts: int
    side: str
    entry: float
    exit: float
    pnl_abs: float
    pnl_pct: float
    dur_min: float

def equity_to_drawdown(equity: List[float]) -> float:
    peak = -1e18; maxdd = 0.0
    for v in equity:
        if v > peak: peak = v
        dd = 0.0 if peak == 0 else (peak - v) / peak
        if dd > maxdd: maxdd = dd
    return maxdd

def sharpe(returns: List[float], rf: float = 0.0, period_per_year: int = 365*24*12) -> float:
    # returns: per-bar (ex: par 5m) log or simple; ici simple
    if not returns: return 0.0
    mean = sum(returns)/len(returns)
    var = sum((r-mean)**2 for r in returns)/max(1, len(returns)-1)
    std = math.sqrt(var) if var>0 else 0.0
    if std == 0: return 0.0
    return (mean - rf) / std * math.sqrt(period_per_year)

def summarize(trades: List[Trade], equity: List[float], bar_returns: List[float], start_ts: int, end_ts: int) -> Dict:
    wins = [t for t in trades if t.pnl_abs > 0]
    losses = [t for t in trades if t.pnl_abs < 0]
    wr = len(wins)/len(trades) if trades else 0.0
    gross_win = sum(t.pnl_abs for t in wins)
    gross_loss = abs(sum(t.pnl_abs for t in losses))
    pf = (gross_win / gross_loss) if gross_loss > 0 else float('inf') if gross_win > 0 else 0.0
    mdd = equity_to_drawdown(equity)
    shp = sharpe(bar_returns)
    expectancy = (gross_win - gross_loss) / max(1, len(trades))
    n_years = max(1e-9, (end_ts - start_ts) / (365*24*3600*1000))
    cagr = (equity[-1]/equity[0])**(1/n_years) - 1 if equity and equity[0] > 0 else 0.0
    score = (wr*0.2) + (min(pf,3.0)/3.0*0.3) + (max(0.0,1.0-mdd)*0.3) + (max(0.0, min(shp/3,1.0))*0.2)
    return {
        "trades": len(trades),
        "winrate": wr, "pf": pf, "maxdd": mdd, "sharpe": shp,
        "expectancy": expectancy, "cagr": cagr, "score": score,
        "equity_start": equity[0] if equity else None,
        "equity_end": equity[-1] if equity else None,
    }--- [178/191] ./engine/backtest/__init__.py ---
# engine/backtest/__init__.py
# Paquet backtest : __init__ volontairement léger pour éviter
# les import cycles / symboles manquants lors d'import partiel (ex: loader_csv).

__all__ = [
    # modules utilisables sans side-effects
    "loader_csv",
    "runner",
]--- [179/191] ./engine/positions/state.py ---
from __future__ import annotations
from dataclasses import dataclass, field
from enum import Enum, auto
from typing import List, Optional
import time

class PositionStatus(Enum):
    IDLE = auto()
    PENDING_ENTRY = auto()
    OPEN = auto()
    PENDING_EXIT = auto()
    CLOSED = auto()

class PositionSide(Enum):
    LONG = 1
    SHORT = -1

@dataclass
class Fill:
    order_id: str
    trade_id: str
    price: float
    qty: float
    fee: float
    ts: int

@dataclass
class PositionState:
    symbol: str
    side: PositionSide
    status: PositionStatus = PositionStatus.IDLE
    entry_order_id: Optional[str] = None
    exit_order_id: Optional[str] = None
    req_qty: float = 0.0
    filled_qty: float = 0.0
    avg_entry_price: float = 0.0
    avg_exit_price: float = 0.0
    sl: Optional[float] = None
    tp: Optional[float] = None
    realized_pnl: float = 0.0
    fees: float = 0.0
    opened_ts: Optional[int] = None
    closed_ts: Optional[int] = None
    fills: List[Fill] = field(default_factory=list)
    last_sync_ts: int = field(default_factory=lambda: int(time.time()*1000))

    def apply_fill_entry(self, f: Fill) -> None:
        self.fills.append(f)
        self.filled_qty += f.qty
        # moyenne pondérée
        notional = self.avg_entry_price * (self.filled_qty - f.qty) + f.price * f.qty
        self.avg_entry_price = notional / max(1e-12, self.filled_qty)
        self.fees += abs(f.fee)
        if self.opened_ts is None:
            self.opened_ts = f.ts
        if self.filled_qty > 1e-12:
            self.status = PositionStatus.OPEN

    def apply_fill_exit(self, f: Fill) -> None:
        self.fills.append(f)
        qty = min(self.filled_qty, f.qty)
        # realized pnl sur la quantité fermée
        if self.side == PositionSide.LONG:
            self.realized_pnl += (f.price - self.avg_entry_price) * qty
        else:
            self.realized_pnl += (self.avg_entry_price - f.price) * qty
        self.fees += abs(f.fee)
        self.filled_qty = max(0.0, self.filled_qty - qty)
        # moyenne de sortie indicative
        closed_q = (self.req_qty - self.filled_qty)
        self.avg_exit_price = ((self.avg_exit_price * (closed_q - qty)) + f.price * qty) / max(1e-12, closed_q)
        if self.filled_qty <= 1e-12:
            self.status = PositionStatus.CLOSED
            self.closed_ts = f.ts

--- [180/191] ./engine/positions/__init__.py ---
--- [181/191] ./engine/config/watchlist.py ---
# engine/config/watchlist.py
from __future__ import annotations
import json, os, time
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, List

from engine.config.loader import load_config
from engine.pairs.selector import PairMetrics

def _watchlist_path() -> Path:
    # Fichier versionné ou non ? -> dans DATA_ROOT/reports (hors repo)
    cfg = load_config()
    p = Path(cfg["runtime"]["reports_dir"]) / "watchlist.yml"
    p.parent.mkdir(parents=True, exist_ok=True)
    return p

def save_watchlist(pairs: List[PairMetrics], *, timestamp: int | None = None) -> Path:
    path = _watchlist_path()
    doc = {
        "updated_at": int(timestamp or time.time()),
        "top": [asdict(p) for p in pairs],
    }
    # json lisible (compat .yml reader simple)
    path.write_text(json.dumps(doc, indent=2), encoding="utf-8")
    return path

def load_watchlist() -> Dict[str, Any]:
    p = _watchlist_path()
    if not p.exists():
        return {"updated_at": 0, "top": []}
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return {"updated_at": 0, "top": []}--- [182/191] ./engine/config/strategies.yml ---
{
  "strategies": {
    "BTCUSDT:1m": {
      "name": "BASE_UNTESTED",
      "risk_label": "EXPERIMENTAL",
      "execute": false,
      "ema_fast": 9,
      "ema_slow": 21,
      "atr_period": 14,
      "trail_atr_mult": 1.5,
      "risk_pct_equity": 0.005,
      "last_validated": "2025-08-25T08:30:00Z",
      "ttl_bars": 120
    }
  }
}--- [183/191] ./engine/config/strategies.py ---
# engine/config/strategies.py
from __future__ import annotations
import json, os
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Any, Dict, Tuple

_STRAT_PATH = Path(__file__).resolve().parent / "strategies.yml"

_DEF_TTL_BARS = {  # fallback si meta absent
    "DEFAULT": 300,
    "LOW": 1000,
    "MEDIUM": 500,
    "HIGH": 250,
    "EXPERIMENTAL": 120,
}

def _parse_iso(s: str | None) -> datetime | None:
    if not s: return None
    try:
        if s.endswith("Z"): s = s[:-1] + "+00:00"
        return datetime.fromisoformat(s).astimezone(timezone.utc)
    except Exception:
        return None

def _pair_tf(key: [REDACTED] -> Tuple[str, str]:
    # "BTCUSDT:1m" -> ("BTCUSDT","1m")
    if ":" in key:
        a, b = key.split(":", 1)
        return a.replace("_","").upper(), b
    return key.replace("_","").upper(), "1m"

def _tf_minutes(tf: str) -> float:
    tf = tf.strip().lower()
    if tf.endswith("m"): return float(tf[:-1] or 1)
    if tf.endswith("h"): return float(tf[:-1] or 1) * 60.0
    if tf.endswith("d"): return float(tf[:-1] or 1) * 60.0 * 24.0
    # défaut: 1m
    return 1.0

def _load_doc() -> Dict[str, Any]:
    if not _STRAT_PATH.exists(): return {"meta": {}, "strategies": {}}
    try:
        return json.loads(_STRAT_PATH.read_text(encoding="utf-8"))
    except Exception:
        return {"meta": {}, "strategies": {}}

def _policy_bars(meta: Dict[str, Any], risk_label: str) -> int:
    # 1) meta.ttl_policy_bars
    pol = meta.get("ttl_policy_bars") if isinstance(meta, dict) else None
    if not isinstance(pol, dict): pol = {}
    # 2) env overrides (STRAT_TTL_<RISK>=nb)
    env_key = [REDACTED]"STRAT_TTL_{risk_label.upper()}"
    if os.getenv(env_key):
        try:
            return max(1, int(float(os.getenv(env_key, "0"))))
        except Exception:
            pass
    # 3) table meta/par défaut
    return int(pol.get(risk_label.upper(), pol.get("DEFAULT", _DEF_TTL_BARS.get(risk_label.upper(), _DEF_TTL_BARS["DEFAULT"]))))

def _global_mult(meta: Dict[str, Any]) -> float:
    try:
        m = float(meta.get("[REDACTED]", 1.0))
        env = os.getenv("[REDACTED]")
        if env is not None:
            m *= float(env)
        return max(0.01, m)
    except Exception:
        return 1.0

def load_strategies() -> Dict[str, Dict[str, Any]]:
    """
    Retourne {"PAIR:TF": { params..., 'expired': bool, 'executable': bool, 'ttl_hours': float }}
    TTL = (ttl_bars OU policy[risk]) * durée_barre(tf)
    """
    doc = _load_doc()
    meta = doc.get("meta") or {}
    src = doc.get("strategies") or {}
    mult = _global_mult(meta)

    now = datetime.now(timezone.utc)
    out: Dict[str, Dict[str, Any]] = {}

    for k, v in src.items():
        if not isinstance(v, dict): continue
        pair, tf = _pair_tf(k)
        tf_min = _tf_minutes(str(tf))
        risk = (v.get("risk_label") or "DEFAULT").upper()

        # nombre de barres de validité
        bars = int(v.get("ttl_bars")) if isinstance(v.get("ttl_bars"), (int, float, str)) and str(v.get("ttl_bars")).strip() else _policy_bars(meta, risk)
        bars = max(1, int(bars * mult))

        ttl_hours = (bars * tf_min) / 60.0
        last = _parse_iso(str(v.get("last_validated") or ""))

        expired = True
        if last is not None:
            expired = now > (last + timedelta(hours=ttl_hours))

        execute_flag = bool(v.get("execute", False))
        risk_label = (v.get("risk_label") or "").upper()
        # on ne décide pas ici pour EXPERIMENTAL; l'orchestrateur peut autoriser via env
        executable = execute_flag and not expired and risk_label != "EXPERIMENTAL"

        out[f"{pair}:{tf}"] = {
            **v,
            "pair": pair,
            "tf": tf,
            "risk_label": risk_label,
            "ttl_bars": bars,
            "ttl_hours": ttl_hours,
            "expired": expired,
            "executable": executable,
        }
    return out

def executable_keys(*, allow_experimental: bool = False) -> Dict[str, Dict[str, Any]]:
    all_ = load_strategies()
    out: Dict[str, Dict[str, Any]] = {}
    for k, s in all_.items():
        if s.get("expired"): continue
        if not s.get("execute"): continue
        if s.get("risk_label") == "EXPERIMENTAL" and not allow_experimental:
            continue
        out[k] = s
    return out--- [184/191] ./engine/config/__init__.py ---
--- [185/191] ./engine/config/loader.py ---
# engine/config/loader.py
from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict

# PyYAML est préférable (format YAML "humain"). On garde un fallback JSON.
try:
    import yaml  # type: ignore
except Exception:  # pragma: no cover
    yaml = None  # type: ignore

# ---------------------------------------------------------------------
# Emplacement du fichier de configuration versionné dans le repo
# ---------------------------------------------------------------------
_CFG_PATH = Path(__file__).resolve().parent / "config.yaml"

# Valeurs par défaut (si une clé manque dans config.yaml)
_DEFAULTS: Dict[str, Any] = {
    "runtime": {
        "timeframe": "1m",
        "refresh_secs": 5,
        "data_dir": "/notebooks/scalp_data/data",
        "reports_dir": "/notebooks/scalp_data/reports",
        "logs_dir": "/notebooks/scalp_data/logs",
    },
    "watchlist": {
        "top": 10,
        "score_tf": "5m",
        "backfill_tfs": ["1m", "5m", "15m"],
        "backfill_limit": 1500,
    },
    "maintainer": {
        "enable": True,
        "interval_secs": 43200,          # 12h
        "seed_tfs": ["1m"],
        "[REDACTED]": 120,
    },
}

# Cache en mémoire pour éviter de relire le fichier à chaque appel
_CFG_CACHE: Dict[str, Any] | None = None


# ---------------------------------------------------------------------
# Utilitaires
# ---------------------------------------------------------------------
def _deep_merge(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
    """Fusion récursive (override > base)."""
    out = dict(base)
    for k, v in (override or {}).items():
        if isinstance(v, dict) and isinstance(out.get(k), dict):
            out[k] = _deep_merge(out[k], v)  # type: ignore[index]
        else:
            out[k] = v
    return out


def _read_yaml_or_json(path: Path) -> Dict[str, Any]:
    """Lit config.yaml. Accepte YAML classique ou JSON 'lisible'."""
    if not path.exists():
        return {}
    txt = path.read_text(encoding="utf-8")
    # 1) tenter YAML si dispo
    if yaml is not None:
        try:
            doc = yaml.safe_load(txt) or {}
            if isinstance(doc, dict):
                return doc  # type: ignore[return-value]
        except Exception:
            pass
    # 2) fallback JSON
    try:
        doc = json.loads(txt) or {}
        if isinstance(doc, dict):
            return doc  # type: ignore[return-value]
    except Exception:
        pass
    return {}


# ---------------------------------------------------------------------
# API publique
# ---------------------------------------------------------------------
def cfg_path() -> Path:
    """Retourne le chemin du fichier de config dans le repo."""
    return _CFG_PATH


def load_config(reload: bool = False) -> Dict[str, Any]:
    """
    Charge la configuration fusionnée (defaults + engine/config/config.yaml).
    Utilise un cache en mémoire; passer reload=True pour forcer la relecture.
    """
    global _CFG_CACHE
    if _CFG_CACHE is not None and not reload:
        return _CFG_CACHE

    # Lire le document (vide si absent/illisible)
    doc = _read_yaml_or_json(_CFG_PATH)

    # Fusion récursive avec les defaults
    merged = _deep_merge(_DEFAULTS, doc)

    # Normalisations légères
    # - watchlist.backfill_tfs peut être une chaîne "1m,5m" -> liste
    wl = merged.get("watchlist", {})
    if isinstance(wl, dict):
        tfs = wl.get("backfill_tfs")
        if isinstance(tfs, str):
            wl["backfill_tfs"] = [t.strip() for t in tfs.split(",") if t.strip()]
        merged["watchlist"] = wl

    _CFG_CACHE = merged
    return merged


__all__ = ["load_config", "cfg_path"]--- [186/191] ./engine/config/config.yaml ---
runtime:
  data_dir: /notebooks/scalp_data/data
  reports_dir: /notebooks/scalp_data/reports

  tf_list: [ "1m", "5m", "15m" ]
  age_mult: 5
  topN: 10
  [REDACTED]: 5
  backfill_limit: 1500

  risk_mode: "conservative"      # conservative | normal | aggressive
  exec_enabled: false
  html_port: 8888

  # === parallélisme backtest ===
  [REDACTED]: 4        # nb de workers CPU (ProcessPool)
  backtest_chunk_size: 32        # nb de (pair,tf) par lot envoyé aux workers

  # === orchestrateur asynchrone ===
  auto_refresh: true
  refresh_every_secs: 30         # cadence worker refresh
  min_fresh_ratio: 0.8           # % cellules fraîches (DAT+OK) requis pour lancer backtest
  [REDACTED]: 120    # délai mini entre 2 backtests

  strategy_name: "ema_atr_v1"

  # Paramètres communs (frais/slippage identiques pour tous les modes)
  [REDACTED]:
    maker_fee_rate: 0.0002       # 0.02% (2 bps)
    taker_fee_rate: 0.0008       # 0.08% (8 bps)
    prefer_maker: true
    slippage_bps: 5.0            # 0.05%

  # Overrides selon le risk_mode
  strategy_profiles:
    conservative:
      ema_fast: 9
      ema_slow: 34
      lookahead_bars: 14
    normal:
      ema_fast: 12
      ema_slow: 34
      lookahead_bars: 10
    aggressive:
      ema_fast: 12
      ema_slow: 26
      lookahead_bars: 6--- [187/191] ./engine/bootstrap.py ---
# engine/bootstrap.py
from __future__ import annotations

import importlib
import logging
import subprocess
import sys
from pathlib import Path
from typing import Iterable, Optional

from engine.config.loader import load_config


log = logging.getLogger("bootstrap")


# ------------------------------------------------------------
# Dépendances minimales (légères) – on reste soft (pas de streamlit ici)
# ------------------------------------------------------------

_MIN_PKGS = [
    # pour la visu terminal (jobs/termboard ou health board)
    ("rich", "rich"),
    # utilitaires de base déjà utilisés dans le projet
    ("yaml", "pyyaml"),
    ("requests", "requests"),
]

def _is_installed(mod: str) -> bool:
    try:
        importlib.import_module(mod)
        return True
    except Exception:
        return False

def _pip_install(args: Iterable[str]) -> int:
    cmd = [sys.executable, "-m", "pip", "install", *list(args)]
    return subprocess.call(cmd)

def ensure_min_deps(extra: Optional[Iterable[str]] = None) -> None:
    """
    Installe en douceur les dépendances manquantes (rich, pyyaml, requests).
    Évite de planter le démarrage si pip échoue (on loggue seulement).
    """
    missing = []
    for mod, pip_name in _MIN_PKGS:
        if not _is_installed(mod):
            missing.append(pip_name)
    if extra:
        for name in extra:
            # si l'appelant veut forcer un paquet, on tente
            if not _is_installed(name):
                missing.append(name)

    if not missing:
        return

    try:
        log.info("[deps] installation manquante: %s", ", ".join(missing))
        _pip_install(missing)
    except Exception as e:
        log.warning("[deps] installation partielle: %s", e)


# ------------------------------------------------------------
# Chemins runtime (crée dossiers si absents)
# ------------------------------------------------------------

def ensure_paths() -> None:
    """
    Crée data_dir / reports_dir / logs_dir / tmp_dir si définis dans config.
    """
    cfg = load_config()
    rt = (cfg.get("runtime") or {})
    for key in ("data_dir", "reports_dir", "logs_dir", "tmp_dir"):
        p = Path(rt.get(key) or "").expanduser()
        if not p:
            continue
        try:
            p.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            log.warning("impossible de créer %s=%s: %s", key, p, e)


# ------------------------------------------------------------
# Exchange (Bitget REST) – tolérant aux signatures différentes
# ------------------------------------------------------------

def build_exchange():
    """
    Construit un client BitgetFuturesClient à partir de la conf.
    Supporte plusieurs signatures (paper/base optionnels) sans planter.
    """
    from engine.exchange.bitget_rest import BitgetFuturesClient  # existant dans ton repo

    cfg = load_config()
    ex_cfg = (cfg.get("exchange") or {}).get("bitget", {}) or {}
    trading = (cfg.get("trading") or {}) or {}

    ak = ex_cfg.get("access_key", "")
    sk = ex_cfg.get("secret_key", "")
    pp = ex_cfg.get("passphrase", "")
    base = ex_cfg.get("base", "https://api.bitget.com")
    paper = bool(trading.get("paper", True))

    # Essai 1: avec paper + base
    try:
        return BitgetFuturesClient(
            access_key=[REDACTED] secret_key=[REDACTED] passphrase=[REDACTED]
            paper=paper, base=base
        )
    except TypeError:
        pass
    # Essai 2: sans paper
    try:
        return BitgetFuturesClient(
            access_key=[REDACTED] secret_key=[REDACTED] passphrase=[REDACTED]
            base=base
        )
    except TypeError:
        pass
    # Essai 3: minimal
    try:
        return BitgetFuturesClient(
            access_key=[REDACTED] secret_key=[REDACTED] passphrase=[REDACTED]
        )
    except TypeError as e:
        raise RuntimeError(f"BitgetFuturesClient incompatible avec la conf: {e}")


# ------------------------------------------------------------
# Entrée unique pour préparer l’environnement (appelée par app.run)
# ------------------------------------------------------------

def [REDACTED]() -> None:
    """
    À appeler tôt au démarrage (depuis engine.app) :
      - installe les deps minimales (rich/pyyaml/requests) si besoin
      - prépare les dossiers runtime (data/reports/logs/tmp)
    """
    ensure_min_deps()
    ensure_paths()--- [188/191] ./init.py ---
#!/usr/bin/env python3
"""Install all project dependencies.

Run this script once to install every ``requirements*.txt`` file found in the
repository as well as the packages needed for the test suite.  All subsequent
invocations of the bot or its submodules will then share the same Python
environment with the required dependencies available.
"""

from __future__ import annotations

import subprocess
import sys
from pathlib import Path


def install_packages(*args: str) -> None:
    """Install packages using pip for the current Python interpreter."""
    cmd = [sys.executable, "-m", "pip", "install", *args]
    subprocess.check_call(cmd)


def main() -> None:
    repo_root = Path(__file__).resolve().parent

    # Install from any requirements*.txt file across the repository so that
    # sub-packages with their own dependency lists are also covered.
    for req in sorted(repo_root.rglob("requirements*.txt")):
        install_packages("-r", str(req))

    # Ensure test dependencies are available
    install_packages("pytest")


if __name__ == "__main__":
    main()
--- [189/191] ./backtest_config.json ---
{
  "schema_version": "scalp-backtest/1.0",
  "assets": ["BTCUSDT","ETHUSDT","SOLUSDT","BNBUSDT","XRPUSDT","ADAUSDT"],
  "timeframes": ["1m","5m","15m"],
  "walk_forward": {
    "enabled": false,
    "train_days": 90,
    "test_days": 30,
    "segments": 6
  },
  "costs": {
    "maker_fee_rate": 0.0002,
    "taker_fee_rate": 0.0008,
    "slippage_bps": 5.0
  },
  "optimization": {
    "engine": "optuna",
    "enabled": false,
    "n_trials": 100,
    "timeout_sec": 1800
  },
  "constraints": {
    "min_trades": 25,
    "min_pf": 1.2
  }
}--- [190/191] ./INSTALL.txt ---
# INSTALLATION — Projet scalp

Prérequis système
- Python 3.10+ (recommandé 3.11)
- Accès réseau sortant (Bitget API, Telegram)
- Espace persistant pour DATA_ROOT (ex: /notebooks/scalp_data)

1) Création d’environnement virtuel (recommandé)
   python -m venv .venv
   . .venv/bin/activate
   python -m pip install --upgrade pip

2) Installation des dépendances
   pip install -r requirements.txt

3) Configuration .env (dans /notebooks/.env)
   TELEGRAM_BOT_TOKEN=[REDACTED]
   TELEGRAM_CHAT_ID=123456
   BITGET_ACCESS_KEY=[REDACTED]
   BITGET_SECRET=[REDACTED]
   BITGET_PASS=[REDACTED]
   DATA_ROOT=/notebooks/scalp_data
   LOG_LEVEL=INFO

4) Premier run (collecte market data simple)
   python bot.py
   → écrit /notebooks/scalp_data/data/live/logs/signals.csv

5) Mise à jour pairs + backfill
   python jobs/refresh_pairs.py --timeframe 5m --top 10 --backfill-tfs 1m,5m,15m,1h --limit 2000

6) Backtests + promotion stratégie
   python jobs/backtest.py --from-watchlist --tfs 1m,5m,15m,1h
   python jobs/promote.py --draft /notebooks/scalp_data/reports/strategies.yml.next

7) Lancement live (avec stratégies promues)
   python bot.py
   → écrit /notebooks/scalp_data/data/live/orders.csv (paper mode)

8) Dashboard (optionnel)
   streamlit run dash/app.py --server.port 8501 --server.headless true--- [191/191] ./CHANGELOG.md ---
# Changelog

## Unreleased

- Trigger trade entries via `strategy.generate_signal` with weighted scoring and
  signal levels.
- Dynamic risk management adapting `risk_pct` and leverage based on signal and
  user risk level.
- Notional and margin caps with available balance check to avoid Bitget error
  `40762`.
- Risk notifications with green/yellow/red indicators for terminal and
  Telegram.
